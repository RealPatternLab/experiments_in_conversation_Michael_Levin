{
  "file_metadata": {
    "text_file": "data/extracted_text/pdf_20250803_124125_194397_extracted_text.txt",
    "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
    "pdf_filename": "pdf_20250803_124125_194397.pdf",
    "file_size": 42647,
    "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
    "journal": "Entropy",
    "doi": "10.3390/e24050710",
    "year": "2022",
    "title": "Biology, Buddhism, and AI: Care as the Driver of Intelligence",
    "confidence_score": null,
    "document_type": "research_paper",
    "crossref_data": true,
    "enrichment_method": "crossref_api"
  },
  "chunks": [
    {
      "text": "Intelligence is a central feature of human beings' primary and interpersonal experience. Understanding how intelligence originated and scaled during evolution is a key challenge for modern biology. Some of the most important approaches to understanding intelligence are the ongoing efforts to build new intelligences in computer science (AI) and bioengineering. However, progress has been stymied by a lack of multidisciplinary consensus on what is central about intelligence regardless of the details of its material composition or origin (evolved vs. engineered). We show that Buddhist concepts offer a unique perspective and facilitate a consilience of biology, cognitive science, and computer science toward understanding intelligence in truly diverse embodiments. In coming decades, chimeric and bioengineering technologies will produce a wide variety of novel beings that look nothing like familiar natural life forms; how shall we gauge their moral responsibility and our own moral obligations toward them, without the familiar touchstones of standard evolved forms as comparison? Such decisions cannot be based on what the agent is made of or how much design vs. natural evolution was involved in their origin. We propose that the scope of our potential relationship with, and so also our moral duty toward, any being can be considered in the light of Care—a robust, practical, and dynamic lynchpin that formalizes the concepts of goal-directedness, stress, and the scaling of intelligence; it provides a rubric that, unlike other current concepts, is likely to not only survive but thrive in the coming advances of AI and bioengineering. We review relevant concepts in basal cognition and Buddhist thought, focusing on the size of an agent's goal space (its cognitive light cone) as an invariant that tightly links intelligence and compassion. Implications range across interpersonal psychology, regenerative medicine, and machine learning. The Bodhisattva's vow (\"for the sake of all sentient life, I shall achieve awakening\") is a practical design principle for advancing intelligence in our novel creations and in ourselves.",
      "section": "Abstract",
      "topic": "Intelligence and Care",
      "chunk_summary": "Buddhist concepts, particularly Care, offer a framework for understanding intelligence across diverse embodiments, including AI and bioengineered life forms, by focusing on goal-directedness, stress, and the expansion of an agent's cognitive boundary.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "The fields of basal cognition, Buddhist philosophy, computer science, and cognitive science are all concerned with fundamental questions around intelligence. What is unique about certain configurations of matter that enable them to exhibit intelligent behavior? How do the kinds and degrees of intelligence differ across beings? What processes drive the expansion of intelligence on evolutionary time scales, and what causes changes in the intelligence of a being during its lifespan? How can we understand intelligence in a way that would enable us to create novel instances, as well as improve our own intelligence for life-positive outcomes for all? Traditional approaches to this question have been focused on a set of standard “model systems\" such as human subjects and certain animals (rats, birds, etc.) in the context of a historical evolutionary lineage on Earth. However, recent approaches in artificial intelligence and synthetic bioengineering have begun to produce novel types of agents whose intelligence cannot be readily predicted from the details of their construction or their origins [1-4]. These constructivist efforts to create intelligence in novel implementations (ranging from novel combinations of engineered living tissue to software) reveal key gaps in our understanding of dynamic intelligence [5]. Given the inevitable developments in the biological sciences, and the profound challenges faced by society, it is essential to develop frameworks that help us to detect, understand, and communicate with intelligences in unfamiliar guises. Here, we propose that Buddhist thought, and its emphasis on care and compassion as a catalyst of positive change in intelligent beings, is an empirically fruitful lens with which to understand intelligence.",
      "section": "Introduction",
      "topic": "Defining Intelligence",
      "chunk_summary": "Traditional approaches to understanding intelligence are limited by their focus on familiar biological systems, while the emergence of AI and bioengineered agents necessitates new frameworks that consider care and compassion as key components.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "A few words on methodology are perhaps in order here. This exploration across otherwise typically disparate scientific disciplines and scholarly contexts aims at achieving what can be thought of as \\\"deep integration\\\" [6]. Such an integrational approach does not privilege the discourse of any one particular discipline as the primary carrier of meaning into which statements and insights derived from other, complimentary frameworks and approaches must be translated. Instead, what we here seek to achieve is a form of mutually informed, explorative conversation that engages our customary, discipline-specific frameworks on equal footing, thereby facilitating the recognition of individual blind spots as well as otherwise unacknowledged, shared concerns. As a consequence of taking this approach, certain concepts that are central to this paper—such as stress, care, intelligence, self, or agency—take on a significance that emerges within and is defined by the concrete interdisciplinary encounter. For this reason, we have supplied a Glossary that explains a selection of such broadly applicable concepts as they are understood in the specific context of this paper. Perhaps in line with age-old Buddhist sentiments, our concern is here pragmatic before philosophical. At the same time, we hope that any instances of functional, conceptual integration that this paper may achieve can in turn motivate refinements and constructive research across the sciences of life, cognition, and information, as well as indeed in fields such as philosophy and religion. To illustrate this with an example, one of the main points of our definition of Self (as given in the Glossary and extracted from previous research) is that a Self is an illusory modelling construct created by perceptual systems of Agents. Agents construct models of causal Selves for others, and for ourselves, using the same machinery. The same mechanisms that cause an agent to act toward stress reduction in itself (even though the beneficiary of those actions is in an important sense impermanent) can be expanded to extend to other Selves. In this way, while our focus is on understanding and formulating Self in a way that is applicable to a broad range of scientific contexts, we also see ourselves as here contributing to the treatment of perennial issues in contemporary Buddhist philosophy—such as the feasibility of genuine care in a world without real individuals [7-13]. Similarly, with respect to the paper's main thesis regarding care as a driver for intelligence: we hope that apart from addressing contemporary scientific or social aims and practices, our discussion may as well contribute to an understanding of classic Buddhist doctrine in its own right.",
      "section": "Introduction",
      "topic": "Interdisciplinary Methodology",
      "chunk_summary": "The paper adopts a \\\"deep integration\\\" methodology, engaging diverse disciplines on equal footing to explore the concept of Self as an illusory construct and Care as a driver of intelligence, aiming to contribute to both scientific and philosophical understanding.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "The field of basal cognition [14-18] emphasizes a continuum of intelligence, which originated in the control loops of microbes but was scaled up throughout multicellular forms to the obvious kinds of intelligent behavior observed in advanced animals. The emphasis on functional problem-solving, learning, and creative responses to challenges enables a focus on the central invariant of intelligence, not contingent facts and frozen accidents of the evolutionary journey of life on Earth. Given that intelligent behavior does not require traditional brains [16,18], and can take place in many spaces besides the familiar 3D space of motile behavior (e.g., physiological, metabolic, anatomical, and other kinds of problem spaces), how can we develop rigorous formalisms for recognizing, designing, and relating to truly diverse intelligences?",
      "section": "Introduction",
      "topic": "Basal Cognition and Diverse Intelligences",
      "chunk_summary": "Basal cognition highlights a continuum of intelligence across life forms, emphasizing functional problem-solving and the need for formalisms to understand intelligence beyond traditional brain-centric views.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "One way to think about a general, substrate-independent definition of \\\"Intelligence\\\" is centered on goal-directed activity [19,20]: what is common to all intelligent systems, regardless of their composition or origin, is the ability to display a degree of competency in reaching a goal (in some problem space, [21]) despite changing circumstances and novel perturbations. These ideas extend classical discussions by Spinoza, Kant, Jonas, and Heidegger. All intelligences, no matter how embodied, can be compared directly with respect to the maximum spatiotemporal scale of the goals towards which they can represent and work. A corollary to this view is that the driver of this kind of homeostatic dynamic is that such systems exhibit \\\"stress\\\" (the delta between current state and optimal state, or the difference between the goals at different subsystems' levels): reduction of this stress parameter is a driver that keeps the system exerting energy in action to move and navigate within the problem space. It should be noted that stress can be seen as the inverse of \\\"satisfaction\\\" [22], and is relative to a contextual and non-stationary target. Evolution enables the scaling of intelligence by exploiting biophysical mechanisms that enable progressively larger goal states (and thus progressively more complex causes of stress) to be represented and pursued [23]. More complex and advanced cognitive agents are capable of being stressed by larger and more complex states of affairs [24], regardless of their specific composition or provenance. These ideas are novel and somewhat disruptive for many traditional approaches that have been largely focused on brains and do not comfortably stretch to encompass advances in bioengineering, chimeric technologies, and machine learning. In complement to the Western traditions that have driven now-dissolving boundaries between brain, body, and environment [25], we propose that Buddhism offers an approach that is uniquely suited to the new field developing at the intersection of computer science, bioengineering, and cognitive science (Figure 1).",
      "section": "Introduction",
      "topic": "Goal-Directed Intelligence and Stress",
      "chunk_summary": "Intelligence can be defined by goal-directed activity and the ability to manage \\\"stress\\\" (the difference between current and optimal states), with evolution driving the scaling of intelligence by enabling the pursuit of progressively larger goal states.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "We propose a central concept as a key invariant across these fields: Care (a metric focused on motivation, stress, and goal-directedness of agents). If stress is the manifest discrepancy between current and optimal conditions, \\\"Care\\\" can in turn be defined as concern for stress relief, and \\\"intelligence\\\" as the degree of capacity for identifying and seeking such relief. By analyzing the role of Care in diverse contexts, informed by a Buddhist approach, we propose a new path towards improving both natural and artificial intelligence via a commitment to radical expansion of a being's or an agent's cognitive boundary: the scale of the things it can possibly care about (defined by the range of states that cause it stress and cause it to exert effort to change). In this framework, what an agent can possibly care about is a central determinant of its degree of intelligence. Importantly, this view not only helps us understand the origins and implementation of diverse types of intelligence within an agent, but also helps clarify the changes of an agent's intelligence in its outward-facing relationships to other agents. Whereas the drive to reduce one's own stress is a primitive and universal ingredient in cognition and intelligence, the inclusion of others' stress as a primary goal necessarily increases the cognitive boundary of an individual and scales its intelligence. Given the modular nature of homeostatic loops, this only requires that sensors that normally gauge the agent's own states (face inwards) expand to include information about others' states (start to face outwards). In this framework, the recognition of agency outside oneself and the progressive inclusion of their states in one's own homeostatic stress-reduction loops is a bidirectional feedback loop that leads to the scaling of intelligence and increases in practical compassion. This loop operates on both the evolutionary and individual lifespan time scales, and in more advanced forms, comes under rational control of systems whose primary goals may start to include the meta-cognitive goal of increasing intelligence and compassion. Advanced intelligence includes the ability to notice agency, and thus stress, and to seek its reduction. We employ this perspective on intelligence in an analysis of the Bodhisattva principle of agency and cognition, focusing on the traditional concept of \\\"taking the Bodhisattva vow\\\" and so committing to the pursuit of cognitive perfection (\\\"awakening,\\\" Skt. bodhi) for the benefit of all sentient beings throughout time and space [26,27]. In addition to better ways to understand biology, this framework suggests a number of conclusions with respect to stress transfer and goal identification that can serve as design principles for improved general artificial intelligence systems.",
      "section": "Introduction",
      "topic": "Care as a Driver of Intelligence",
      "chunk_summary": "Care, defined as concern for stress relief, is proposed as a central concept in understanding intelligence, with the expansion of an agent's cognitive boundary (the scale of things it cares about) driving the scaling of intelligence and compassion.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "Many definitions of intelligence and cognitive capacity have been debated over the centuries [28]. The problem with most existing formalisms is that they are closely tied to a specific type of subject—such as humans, rats, birds, etc.—a traditional animal at a single \\\"privileged\\\" size and temporal scale [29], or even type of anatomy. Comparing intelligences among different animals, such as octopuses and dogs, is very challenging because their diverse environments and behaviors underline the fact that intelligence can be hard to ascertain in unfamiliar guises. An even bigger limitation in this field is the impending explosion in the prevalence of truly unusual living creatures and distributed systems [30]. Novel living beings produced by engineering and hybrid approaches (such as evolutionary design) include ex vivo constructs such as embryoids, organoids, and assembloids [31,32], cyborgs of animals and plants [3,4,33-38] resulting from living tissue tightly integrated with designed inorganic interfaces [39,40] and with closed-loop control systems [41], biological robots such as computer-controlled invertebrates [42-44], and hybrots consisting of living brains instrumentized to control artificial new bodies [1,43,45-49]. Without any familiar phylogenetic guideposts (e.g., \\\"it's a kind of fish so we expect a fish-like range of behaviors\\\"), it may be extremely difficult to place the intelligence of novel, synthetic creatures with respect to the familiar examples. Recent efforts in \\\"Diverse Intelligences\\\" and Artificial Life initiatives seek to acknowledge the wide range of \\\"life as it can be\\\" [50,51], and produce frameworks for understanding of intelligence that not only subsume possible living beings (designed and evolved) but also include potential exobiological intelligences and purely software (AI) creations. The quest to be able to directly compare truly diverse intelligences, regardless of their origin story or composition, requires us to be able to specify the most general invariant underlying intelligence and cognition: what do all cognitive agents, no matter how advanced or humble, have in common?",
      "section": "The Cognitive Light Cone Framework: Cognitive Boundaries, Goal-Directedness, and Domains of Concern",
      "topic": "Diverse Intelligences",
      "chunk_summary": "Existing definitions of intelligence are often tied to specific biological systems, making it difficult to assess the intelligence of novel bioengineered and AI agents; a more general framework is needed to compare diverse intelligences.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "One such framework has been suggested, and is focused on a candidate for an invariant that enables direct comparison of truly diverse agents (regardless of their composition or origin) [23,24,52]: goal-directedness. We suggest that an essential nature of cognition, in any embodiment, is the capacity for goal-directed activity in some problem space. In this sense, goal-directedness does not require a high-level self-awareness, but only a cybernetic kind of teleonomic functionality [53-60] (of course, other frameworks could be developed around different invariants such as information processing [61]). On this view, any possible agent can be represented by drawing the spatiotemporal boundaries of the biggest goals which it is capable of pursuing (Figure 2). Tell me what you care about—what you actively spend energy on trying to achieve despite perturbations and novel situations—and I can immediately gauge your degree of sophistication. A bacterium can try to manage local sugar concentrations, with a bit of memory and a bit of predictive power. A dog has a larger area of concern, significant memory and predictive capacity in the short term, but it is probably impossible for it to care about something that will happen 100 miles away, 2 months from now. Humans have a huge cognitive envelope, perhaps uniquely one that is larger than our own lifespan; our state of being a creature capable of goals that appear fundamentally unachievable is very characteristic of Buddhist practice. Every creature thus has a “cognitive boundary”—which can be represented in the form of a light cone within space and time that demarcates the edge of what it can care about (not the first-order boundary of what it can sense and affect, but the second-order boundary demarcating the scale of its possible goals). Analyzing systems with respect to this aspect has several advantages. The first is that it is completely agnostic about the composition of agents, enabling the most basal, primitive forms to be compared on the same scale as humans (whose cognitive boundary might extend to planetary scales) and novel life forms (synthetic, exobiological, etc.). Another advantage is that the continuous space underscores the futility of binary categories (“is it intelligent?\", \\\"is it cognitive?”, “is it a real decision or just physics?”): modern bioscience offers no support for some sort of clean bright line separating cognitive beings from non-cognitive ones. All of the interesting capacities of mind, like the ones of the body, evolved gradually. Taking evolution seriously means asking “what kind” and “how much\", with respect to intelligence and cognition broadly conceived [62]. This is consistent not only with the facts of bioengineering (that any purportedly “non-cognitive” system can be mixed and hybridized with a cognitive one), but also with the evolutionary history of cognition. All of the main components of neurons (ion channels, electric synapses, neurotransmitter machinery, etc.) existed long before brains appeared—they were present in our unicellular ancestors. Indeed, evolution long ago (about the time of bacterial biofilms [63]) discovered that bioelectric networks are an ideal medium for scaling computation, coordinating and synthesizing information across distance, and implementing memory and reprogrammability. Developmental bioelectricity [64] is the medium by which non-neural cells form networks to manage morphogenesis in development and regeneration. Pre-neural bioelectric networks in the body underlie large-scale anatomical decision-making and possess instructive pattern memories that guide growth and form [64,65]. It is very likely that this system served as a precursor to neurobiology: prior to electrical networks controlling muscles to move the body through 3D space, these same networks generated signals to control cell behaviors in the body to move the body configuration through anatomical morphospace. Thus, anatomical homeostasis is a goal-seeking capacity of the collective intelligence of cellular swarms comprising living bodies [24].",
      "section": "The Cognitive Light Cone Framework: Cognitive Boundaries, Goal-Directedness, and Domains of Concern",
      "topic": "Cognitive Light Cone",
      "chunk_summary": "A framework based on goal-directedness is proposed, where an agent's \\\"cognitive boundary\\\" (represented by a light cone) defines the spatiotemporal extent of its goals, allowing comparison of diverse intelligences regardless of composition or origin.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "According to the goal-directed model of intelligent agents, any individual agent is then delimited by the spatiotemporal boundary of events that it can seek to measure, model, and affect [23]. This surface sets a functional boundary, or \\\"light cone\\\" of its cognitive ability. We have considered the way systems may exchange stress between each other by exchanging signals, which has direct translations into the world of machine learning agents as well as natural agents. Agents may progressively come to reduce their levels of stress and transfer them between each other in more efficient ways, by means of communicating their goals. It may be helpful to clarify the difference between the goal-defined light cone (Figure 2) and a mere behavioral space light cone. While the latter merely defines the space of possible states in which an agent can find itself (defined by its position, speed, temperature, etc.), the light cone we defined above rather characterizes the maximum extent of the goals and aspirations of an agent, or in other words, its capacity for Care. An agent's Care light cone (CLC) and its corresponding physical light cone (PLC) of behavioral space can be brought together in the same representation (Figure 3). In our representation of light cones, the two diagonal lines represent the two extrema in terms of physical change of the system state, while the horizontal line indicates the present state space. Anything outside of the cones that is located in the future cannot be reached from the present state in the future, nor can anything in the past that lies outside the cones be influencing the present state. Light cones may be represented in two dimensions as in Figure 3. In our depiction of light cones, both physical states and cared-for states belong to their respective light cones. The diagram symbolically shows a phase space where each point corresponds to a state. Points within the Care light cone (CLC, represented in blue) represent states for which the agent cares at a given point in time—typically the present time in our depiction, but one may picture a time series of such light cones changing in time—rather than the states the agents are physically in. A state may be situated within a Care light cone even if it is too distant in space or time to have any interaction with the agent. Conversely, some states in the physical light cone (PLC, represented in yellow) may be beyond the light cone of Care of the agent, yet remain physically achievable through a certain trajectory. Goals cease to exist as soon as the self (the Care light cone, in blue) is reduced to a point, or, on the contrary, extends infinitely over the whole phase space of possible states (in yellow). The light cones we consider are clouds of possibilities for agents, meaning that they represent distributions of probabilities of Care and physical states achievable in time and space. We note that the two types of light cones we describe naturally take two different shapes. On the one hand, the physical light cone determines the limited subspace corresponding to an initial state of the world undergoing change in accordance with a set of physical laws, similar to the light cones in the theory of relativity or light cones of information in evolutionary theory [66]. On the other hand, Care light cones need not bear such a limitation since an agent may care about entities that are not within reach in space and time. A light cone does not exist in a vacuum. On the contrary, it corresponds to a given substrate, and is surrounded by other light cones so that it may even overlap or contain other ones. The act of extending a light cone bears some connection with the act of several agents cooperating or acting as a cognitive whole as their Care light cones may start overlapping.",
      "section": "Two Distinct Light Cones: One for Physical States, One for Care",
      "topic": "Care Light Cone vs. Physical Light Cone",
      "chunk_summary": "Two distinct light cones are introduced: the Care Light Cone (CLC) representing the spatiotemporal extent of an agent's care, and the Physical Light Cone (PLC) representing the space of physically achievable states, with the CLC not limited by physical constraints.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "In this framework, intelligence is the degree of sophistication an agent can muster in navigating some specific problem space. Defined very broadly, problem spaces can be seen as fields that emerge in the gap between current and optimal conditions—or in other words, as fields of stress. The generalization of problem spaces beyond the traditional 3D space of \\\"behavior\\\" into other, virtual problem spaces is essential for understanding evolution of basal cognition. Living things first solved problems in metabolic space, and evolution then pivoted the same kinds of strategies to solve problems in physiological, transcriptional, and anatomical space, before speed-optimizing these dynamics to enable rapid behavior in 3D space. Since every cognitive agent is made of parts, it is essential to have a theory about how numerous goal-seeking agents link together into a new, larger cognitive system that is novel and not present in any of the subunits. The multiscale competency architecture of life [24] is such a hypothesis about the scaling of cognition, seeing complex system-level behaviors in any space as the within- and across-level competition and cooperation among the various subunits and partitions of composite agents (i.e., all agents).",
      "section": "Problem Space, Fields of Stress, and Continuity of Cognitive Forms of Life",
      "topic": "Problem Spaces and Stress Fields",
      "chunk_summary": "Intelligence is defined as the ability to navigate problem spaces, which are viewed as fields of stress emerging from the gap between current and optimal conditions, with the multiscale competency architecture of life explaining the scaling of cognition through interactions of goal-seeking subunits.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "This emphasis on the fundamental continuity, kinship, and infinite variety of life and cognition resonates with Buddhist descriptions of continuous cycles of life and death that emerge through infinite causal dependencies and with potential for radical bodily and cognitive transformation [26,67]. Another feature of this vision that aligns well with Buddhist ideas is the lack of a permanent, unique, unitary Self [68]. The picture given by the evolutionary cell-biological perspective is one where a cognitive agent is seen as a self-reinforcing process (the homeostatic loop), not a thing [69–71]. Of course, on long time scales, all objects are, consistent with Mahāyāna Buddhist perspectives, just temporary configurations—the distinction between permanent objects and temporary bundles of coherent processes (cf. the Ship of Theseus paradox and Nāgasena's chariot parable as a description of biological beings [72,73]) fades away. In this society of processes, overlapping goal-seeking partitions are all interacting with each other. Importantly, the boundaries of these Selves not only interpenetrate, but can also change during the agent's lifetime. Defections from large-scale anatomical goals, such as those that occur due to an inappropriate reduction of gap junctional connectivity [74], present as cancer, cause reversions of cell behavior to ancient unicellular concerns which lead to metastasis and over-proliferation as the cells treat the rest of the body as external environment. Another key fundamental commonality is the focus on striving. The central treadmill (loop) of life is a homeostatic effort to attain a specific setpoint, despite the buffeting influence of the cruel, dissipative environment. The driver of this loop is stress—the delta between current state and desired state, and all of the system's efforts are guided by the effort to minimize stress (essentially, unhappiness). Defined in this way, \\\"stress\\\" turns out to be a compelling translation of the Sanskrit term duhkha (otherwise often rendered as \\\"suffering\\\"), which describes a treacherous world inhabited by restlessly craving beings. In this world of stress, existence equals dissatisfaction, and so duhkha is a continuous state that compels beings to act [67]. This stress-focused perspective can be seen as suggesting that the expansion of cognition across eons was basically a process of scaling goals, from humble metabolic needs to single cells to the grandiose goals of \\\"make and maintain a whole limb\\\" of tissue- and organ-level cellular collectives. It is fascinating to think about how this expansion of concern scales basic self-preservation goals into outward-facing preferences about complex, large states of the environment and even care for the states of other beings.",
      "section": "Problem Space, Fields of Stress, and Continuity of Cognitive Forms of Life",
      "topic": "No-Self and Striving",
      "chunk_summary": "The Buddhist concept of no-self aligns with the view of cognitive agents as self-reinforcing processes, with \\\"stress\\\" (duhkha) as the driver of striving and the expansion of cognition involving the scaling of goals from basic needs to complex concerns for others.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "One instructive example is what happens in bioelectric networks during multicellularity. Cells join into networks with electrical synapses known as gap junctions. What is special about these is that unlike traditional signaling (by diffusible secreted chemical signals and receptors), information molecules pass through gap junctions directly into the internal milieu of the recipient cell. Once the signal is inside a cell, that cell cannot tell whether this molecule is a memory trace of something that happened to that cell (a true memory engram) or a false memory incepted into its informational structure by a kind of memory transfer from its neighbor. Imagine for example a calcium spike due to an injurious stimulus: calcium has no metadata on it to describe whose signal it is, and once it spreads across a few cells, they become a collective that has information about injury that is distributed across the individuals. In effect, it performs a kind of \\\"mind meld\\\", binding subunits into a collective because it becomes very difficult to keep individualization regarding which cell has which information. It is hard to maintain the I-you distinction, and cooperation is massively favored. This is not because the agents have become less selfish, but because the size of the self (to which they are committed) has grown. For properly coupled cells, it is impossible to hide information from each other (from yourself) and it is impossible to do anything injurious to your neighbor because the same effects (consequences) will affect you within seconds. Gap junctions provide an efficient life-transforming dynamic—cause and effect which massively augments cooperative interactions. The eventual result is the scaling up of the cognitive boundary, the processing capacity, the information content and structure, and the goals. An individual cell strives to become two cells. A gap-junction-coupled collective strives to make an organ, being able to represent goal states such as number of fingers which are unfathomable to individual cells. In connecting with others in a strong informational sense [75], the functional non-indifference to one's own states begins to expand and face outwards, enabling responses to progressively more distant others' states. Much work remains to identify policies for informational coupling of subunits that optimize the potentiation of collective intelligence and care. These policies will be as relevant to establishing thriving social structures as to the design of novel general intelligences.",
      "section": "Problem Space, Fields of Stress, and Continuity of Cognitive Forms of Life",
      "topic": "Gap Junctions and Collective Intelligence",
      "chunk_summary": "Gap junctions, by enabling direct transfer of information between cells, facilitate a \\\"mind meld\\\" that expands the self and promotes cooperation, leading to the scaling of cognitive boundaries and the emergence of collective intelligence.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    },
    {
      "text": "Biology offers many examples of Selves which change on-the-fly—not just during evolutionary time scales, but during the lifetime of the agent. All animals were once a single fertilized egg cell, then became a collection of cells solving problems in anatomical space, and only later developed an emergent centralized Self focused around navigating 3D space of behaviors [76]. Butterflies (and their brains) result from the rapid remodeling of a caterpillar and its very different brain. In addition to these natural examples, recent advances in chimeric and bioengineering technology are enabling an inevitable explosion of diverse forms of life. Hybrots, cyborgs, chimeras, and other combinations of living material, bioengineered components, and software form an astronomically huge option space for possible forms with diverse kinds of bodies and behaviors [1,3,4,33,42,43,45-47,77-81]. This requires us to move from the picture of Adam naming a finite set of static animals in the Garden of Eden to frameworks that can handle the full range of life and mind as they can be—all possible sentient beings. For AI workers, it is important to step back from a neurocentric view of intelligence—life was solving problems long before neurons evolved; thus, a focus on neuromorphic architectures (such as specifically neural-network models) is unnecessarily restrictive. All of the main components of neural systems—ion channels, electrical synapses, neurotransmitter machinery, etc., were present long prior to the appearance of brains [82,83]. Indeed, all cells form bioelectrical networks that process information in morphospace in ways familiar to neurosciences (but on a slower time scale) [64]. The emphasis on natural intelligence as fundamentally arising from goal-directed (homeostatic) loops dovetails with key open problems in AI research, with respect to intrinsic motivation [84,85] and goals: how do goals arise in complex systems? How do we predict and manage the goals of collective intelligences (such as robot swarms), ensuring life-positive engineered systems? Evolution is only part of the story, since synthetic living organisms, such as Xenobots—protoorganisms made of frog skin cells [86-88], exhibit coherent anatomical, physiological, and behavioral outcomes that have no backstory of selection forces shaping them. The central concept in this new frontier is Care: what do these systems spend energy to try to achieve—what do they care about? What sets the scope and content of their goals?",
      "section": "Problem Space, Fields of Stress, and Continuity of Cognitive Forms of Life",
      "topic": "Changing Selves and AI",
      "chunk_summary": "Biological examples of changing selves highlight the need to move beyond neurocentric views of intelligence, with a focus on Care as a central concept in understanding the goals and motivations of both natural and synthetic living systems, including AI.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_194397.pdf",
      "original_filename": "pdf_20250803_124125_194397_extracted_text.txt",
      "authors": "[\"Thomas Doctor\", \"Olaf Witkowski\", \"Elizaveta Solomonova\", \"Bill Duane\", \"Michael Levin\"]",
      "year": "2022",
      "journal": "Entropy",
      "doi": "10.3390/e24050710"
    }
  ],
  "gemini_response": "",
  "processed_at": "2025-08-06T12:55:25.358490",
  "enriched_at": "2025-08-11T14:50:03.228070",
  "enrichment_methods": [
    "crossref"
  ]
}