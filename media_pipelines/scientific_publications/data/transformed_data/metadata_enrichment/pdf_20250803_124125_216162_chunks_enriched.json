{
  "file_metadata": {
    "text_file": "data/extracted_text/pdf_20250803_124125_216162_extracted_text.txt",
    "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
    "pdf_filename": "pdf_20250803_124125_216162.pdf",
    "file_size": 26070,
    "authors": [
      "Tingting Amy Gibson",
      "James A. Henderson",
      "Janet Wiles"
    ],
    "journal": "2014 International Joint Conference on Neural Networks (IJCNN)",
    "doi": "10.1109/ijcnn.2014.6889850",
    "year": null,
    "title": "Predicting temporal sequences using an event-based spiking neural network incorporating learnable delays",
    "confidence_score": null,
    "document_type": "proceedings-article",
    "publication_date": "2014",
    "page_range": "3213-3220",
    "crossref_data": true,
    "enrichment_method": "crossref_api"
  },
  "chunks": [
    {
      "text": "A common view in neuroscience is that memory is encoded in connection strength between neurons, leading artificial neural network models to focus on connection weights. This paper presents a weightless spiking neural network prototype for classification tasks, where memory is stored in timing between neurons, trained using Hebbian Spike Timing Dependent Plasticity (STDP) modulating connection delays.",
      "section": "Abstract",
      "topic": "Weightless Spiking Neural Networks",
      "chunk_summary": "This paper introduces a weightless spiking neural network model that encodes memory in timing between neurons, trained using STDP.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Synaptic plasticity's role in learning and memory is an open question. Hebb's postulate suggests that persistent excitation of cell B by cell A increases B's excitability by A. This principle of past activity remodeling synapses underlies modern neural computation research and may have influenced the use of numerically weighted connections in computer science.",
      "section": "Introduction",
      "topic": "Synaptic Plasticity and Learning",
      "chunk_summary": "Hebb's postulate of synaptic modification forms the basis of modern neural computation research and the use of weighted connections in computer science.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "An alternative to weight manipulation for information processing and memory storage is regulating spike propagation speed. Myelin, or axon thickness in invertebrates, affects spike propagation speed. Myelin insulates axons, enabling faster, more efficient signal travel. Delay times vary between organisms and myelin amounts can change through myelination and demyelination.",
      "section": "Introduction",
      "topic": "Spike Propagation and Myelin",
      "chunk_summary": "Regulating spike propagation speed through myelin or axon thickness offers an alternative mechanism for information processing and memory storage.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "This paper explores biologically plausible plasticity architectures in computational learning models. It presents a weightless spiking neural network (WSNN) where learning occurs through spike propagation times, not weight adaptation. The WSNN is trained on MNIST in an unsupervised manner, achieving comparable performance to weight-based models. The paper proposes delays as a means to modulate synchrony and spike intensity.",
      "section": "Introduction",
      "topic": "Weightless Spiking Neural Networks",
      "chunk_summary": "This paper proposes a biologically plausible WSNN model where learning is based on spike propagation times, demonstrating its effectiveness on MNIST.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Biological neurons operate asynchronously, suggesting spike timing's role in information encoding, memory, and decision-making. Time To First Spike (TTFS) is proposed for faster classification. Other approaches use spike frequency and oscillations for computation, like polychronization.  A supervised learning rule updating weights and delays showed improved performance on TIDIGITS. This paper presents an unsupervised learning rule for weightless connections at the individual neuron level.",
      "section": "Previous work",
      "topic": "Prior Work on Spike Timing",
      "chunk_summary": "Previous research highlights the importance of spike timing in neural computation, with various approaches using TTFS, oscillations, and combined weight/delay updates.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The networks are built with BindsNET, a framework for spiking neuron architectures. A biologically inspired myelination process modifies axonal delays in a 2-layered, feed-forward, weightless SNN. The network uses delay-encoded input and a TTFS output layer with competition enforced by a sliding threshold and a Winner-Take-All (WTA) mechanism. A linear decoder analyzes outputs and assigns neurons to MNIST digits.",
      "section": "Method",
      "topic": "Network Architecture and Implementation",
      "chunk_summary": "The WSNN uses BindsNET, a myelination process, delay-encoded input, TTFS output, and a linear decoder for MNIST classification.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Output neurons are Leaky Integrate and Fire (LIF) with adaptive firing thresholds. LIF neurons process spatio-temporal signals and transmit them as spikes, encoding information in spike times, sequences, and frequency. The LIF neuron equation is provided. Signal oversaturation is addressed by threshold adaptation, adjusting neuron sensitivity to incoming spikes using θo and θi to modulate threshold Ti. θo is constant, while θi adapts based on spiking activity, with decay over time. Competition is increased by forcing at least one neuron's theta to zero. Learning involves homeostasis between sliding threshold and Hebbian STDP.",
      "section": "Method",
      "topic": "Leaky Integrate and Fire Neurons",
      "chunk_summary": "The WSNN uses LIF neurons with adaptive thresholds to process spatio-temporal signals and prevent signal oversaturation.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The WSNN has synaptic delays and firing thresholds as primary parameters. Synaptic delays represent time steps for signal delay, initialized uniformly between 0 and 32. Firing thresholds determine the membrane potential for spike generation. The constant threshold θo is initialized to -52, and θi is randomly sampled between 0 and 0.1.",
      "section": "Network Parameters & Initialization",
      "topic": "Network Parameters",
      "chunk_summary": "The WSNN's main parameters are synaptic delays (0-32) and firing thresholds (θo = -52, θi = 0-0.1).",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "STDP is a biologically inspired learning rule for SNNs. Unlike ANNs, SNN signals are spikes.  Myelination dynamically adjusts delay times. STDP with myelination can tune neurons for synchronous firing, crucial for learning and decision-making.  Weights are replaced with time delays for biological plausibility. STDP modifies afferent neuron properties via long-term potentiation and depression.  Delay decrease increases firing probability. A Particle Swarm Optimizer (PSO) optimizes hyperparameters.",
      "section": "Inter-neurons delay plasticity",
      "topic": "STDP and Myelination",
      "chunk_summary": "STDP, combined with dynamic delay adjustments via myelination, is used to train the WSNN, replacing weights with time delays for biological plausibility.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The modified Hebbian STDP rule adjusts axonal transmission delay based on the delay between afferent and efferent spikes (Δt).  Δt is calculated at the neuron level, regardless of incoming spike transmission time.  A delay buffer simulates variable transmission delay.  Normalization and a constant delay increase (demyelination) are applied, promoting focus on common patterns and forgetting less common ones.",
      "section": "Inter-neurons delay plasticity",
      "topic": "Modified Hebbian STDP Rule",
      "chunk_summary": "A modified Hebbian STDP rule adjusts axonal transmission delays based on Δt, with normalization and demyelination mechanisms.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The MNIST dataset is used, with normalization preprocessing. Input neurons fire a single spike using TTFS, timed proportionally to pixel brightness. Brighter pixels spike earlier, black pixels don't spike. The encoding time window is 20-32ms. TTFS reduces spikes and simulation time, with performance comparable to Poisson encoding.",
      "section": "Input encoding",
      "topic": "Input Encoding with TTFS",
      "chunk_summary": "MNIST images are encoded using TTFS, where brighter pixels spike earlier within a 20-32ms window.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Networks were evaluated with 1000-4000 neurons. Accuracies were reported after one epoch.  Simultaneous output spikes decreased during training, with fewer spikes for correctly predicted digits. Time to first output spike also decreased for correct predictions.  The model uses TTFS and delay-learning, requiring fewer iterations and spikes than weight-based models with Poisson encoding. Final delays are quantizable with 5 bits, resulting in a small memory footprint.",
      "section": "RESULTS",
      "topic": "Network Performance",
      "chunk_summary": "The WSNN showed good performance with decreasing simultaneous spikes and faster output spike times for correct predictions, using less computational resources than weight-based models.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Misclassified \"7\" digits highlight model limitations. Misplaced bright pixels can cause incorrect output neuron firing. Penalizing excess white pixels didn't help. Adding another WTA layer or TTFS encoding with inhibition might mitigate this.  Time discretization in SNNs reduces spike time accuracy. Delays, unlike weights, are affected by this.  TTFS might be more beneficial for datasets with more evenly distributed pixel values.",
      "section": "Limitations",
      "topic": "Model Limitations",
      "chunk_summary": "Limitations include misclassification due to misplaced bright pixels, time discretization affecting delay accuracy, and potential benefits of TTFS for datasets with more evenly distributed pixel values.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The model's sensitivity to hyperparameters is another issue. High firing thresholds can prevent output spikes, limiting the valid parameter space. The current model relies solely on the first spike's timing, potentially missing information in static data. The winner-take-all architecture reduces excessive spikes. Using Integrate and Fire (IF) neurons instead of LIF could further alleviate this.",
      "section": "Limitations",
      "topic": "Hyperparameter Sensitivity and Data Representation",
      "chunk_summary": "The model is sensitive to hyperparameters, particularly firing thresholds, and might benefit from using IF neurons instead of LIF.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "This paper presents a time-based, weightless spiking neural network (WSNN) for classification, using spike timing and firing thresholds. The model's success suggests that manipulating spike transmission speed and conductance contributes to learning and event encoding.  The WSNN uses Hebbian STDP to adjust communication timing, encoding information in spike rate and timing. It utilizes both temporal and spatial dimensions, adjusting spike quantity and timing, unlike prior models focusing only on spatial aspects.",
      "section": "DISCUSSION",
      "topic": "WSNN and Spike Timing",
      "chunk_summary": "The WSNN demonstrates the viability of using spike timing and firing thresholds for classification, suggesting the importance of spike transmission speed and conductance in learning.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The WSNN replaces weighted connections with communication delays, compatible with Hebbian learning rules like STDP. Delay-based connections offer faster decisions, fewer computational resources, and reduced energy consumption. The network's binary signals require fewer operations and less storage.  Addressing time delays opens new possibilities for learning algorithms and benefits time-driven data processing. Traditional neural networks struggle with time dimensions, and Hebbian learning in SNNs hasn't reached desired performance. Adding time to connections may provide an advantage.",
      "section": "DISCUSSION",
      "topic": "Advantages of Delay-Based Connections",
      "chunk_summary": "Delay-based connections in the WSNN offer computational advantages, including faster decisions and reduced resource consumption, and address the time dimension crucial for time-driven data.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Future work will explore multi-layered models with higher neuron complexity, leveraging delay connections' timing aspect. Time-driven data like video or sound will be used to exploit the delay network's potential.  Acknowledgements are given to Peter Marathas, Tufts University High Performance Compute Cluster, XSEDE Bridges GPU AI, The Elisabeth Giauque Trust, and the Barton Family Foundation.",
      "section": "DISCUSSION",
      "topic": "Future Work and Acknowledgements",
      "chunk_summary": "Future research will focus on more complex WSNN architectures and time-driven data, building on the current model's success.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_216162.pdf",
      "original_filename": "pdf_20250803_124125_216162_extracted_text.txt",
      "authors": "[\"Hanel Hazan\", \"Simon Caby\", \"Christopher Earl\", \"Hava Siegelmann\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    }
  ],
  "gemini_response": "",
  "processed_at": "2025-08-06T12:39:49.275778",
  "enriched_at": "2025-08-11T14:27:42.605191",
  "enrichment_methods": [
    "crossref_title"
  ]
}