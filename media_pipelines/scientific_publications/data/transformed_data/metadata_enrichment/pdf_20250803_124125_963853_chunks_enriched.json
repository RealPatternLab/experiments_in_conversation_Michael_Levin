{
  "file_metadata": {
    "text_file": "data/extracted_text/pdf_20250803_124125_963853_extracted_text.txt",
    "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
    "pdf_filename": "pdf_20250803_124125_963853.pdf",
    "file_size": 33805,
    "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
    "journal": "Distill",
    "doi": "10.23915/distill.00023",
    "year": "2020",
    "title": "Growing Neural Cellular Automata",
    "confidence_score": null,
    "document_type": "research_paper",
    "crossref_data": true,
    "enrichment_method": "crossref_api"
  },
  "chunks": [
    {
      "text": "Most multicellular organisms develop from a single cell into complex anatomies through self-organization, where cells communicate to determine organ shape, placement, and growth. Understanding how simple rules and feedback loops lead to complex outcomes is a key research area. Evolution has leveraged physics and computation to create robust morphogenetic processes. This process is highly robust, even allowing for regeneration in some species. A central question is how cell collectives know what to build and when to stop. Genomics and stem cell biology explain cell differentiation, but the algorithm for large-scale anatomical specification remains unknown. Discovering this algorithm is crucial for biomedicine, enabling control over growth and form. Biological \"subroutines\" like \"build an eye here\" can be triggered by signals. Formulating computational models of these processes is important for both biology and technology, potentially leading to self-repairing systems.",
      "section": "Introduction",
      "topic": "Morphogenesis and Self-Organization",
      "chunk_summary": "Multicellular organisms develop through self-organization, but the algorithm for large-scale anatomical specification remains a key question for biology and future biomedicine.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "Cellular Automata (CA) models are used to simulate local interactions, offering a roadmap for understanding cell-level rules that lead to complex behavior. In our CA model, cell states are represented by continuous values, enabling a differentiable update rule. Differentiable rules allow the use of loss functions and gradient-based optimization, similar to deep learning.  Our goal is to design a CA that grows a predefined pattern from a single cell, analogous to organism development. The rules guiding individual cell behavior are analogous to the genome, and running the model reveals the resulting patterning behavior.",
      "section": "Model",
      "topic": "Cellular Automata Model",
      "chunk_summary": "A Cellular Automata model with continuous cell states and a differentiable update rule is used to simulate pattern formation, analogous to organism development.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "Each cell state is a 16-dimensional vector. The first three values represent RGB color, and the alpha channel (α) indicates living cells (α > 0.1).  \"Mature\" cells have α > 0.1, while \"growing\" neighbors have α ≤ 0.1. Hidden channels represent internal cell states, analogous to chemical concentrations or electric potentials. All cells share the same update rule (genome), and their differentiation arises from these internal states.",
      "section": "Model",
      "topic": "Cell State Representation",
      "chunk_summary": "Cell states are 16-dimensional vectors, with RGB for color, alpha for living status, and hidden channels for internal states.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "The cell update rule consists of perception, update, stochastic update, and living cell masking. Perception uses fixed 3x3 Sobel filters to estimate gradients of state channels, mimicking cells relying on chemical gradients. The update rule applies differentiable operations (convolutions, ReLUs) to the perception vector, outputting an incremental update to the cell state.  Stochastic update introduces randomness by applying a per-cell mask to update vectors, avoiding global synchronization. Living cell masking sets empty cells (no mature cells in the neighborhood) to zero.",
      "section": "Model",
      "topic": "Cell Update Rule",
      "chunk_summary": "The cell update rule involves perception via Sobel filters, a differentiable update function, stochastic updates, and masking of empty cells.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "In Experiment 1, the CA is trained to achieve a target image after a random number of updates, starting from a single seed cell.  Pixel-wise L2 loss is applied between the grid and the target pattern. Training uses backpropagation-through-time.  Results showed varying long-term behaviors: some patterns died out, some grew uncontrollably, and some remained stable. This highlighted the need for mechanisms to promote persistent patterns.",
      "section": "Experiments",
      "topic": "Learning to Grow",
      "chunk_summary": "Training the CA to grow a target pattern from a seed cell resulted in varied long-term behaviors, highlighting the need for stability.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "Experiment 2 aimed to create stable patterns by making the target pattern an attractor.  A \"sample pool\" strategy was used to address the computational cost of long training sequences.  The pool stores various cell configurations, and training iteratively refines the dynamics to recover the target pattern from these states.  Replacing the highest-loss sample with the seed state improved training stability. This approach encourages the system to learn how to persist and improve existing patterns, not just grow them from scratch.",
      "section": "Experiments",
      "topic": "Persistent Patterns",
      "chunk_summary": "A sample pool strategy was used to train the CA to create stable patterns by making the target pattern an attractor.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "Experiment 3 explored the regenerative capabilities of the trained models.  Models were damaged in various ways (removing halves, cutting out sections), revealing different regeneration behaviors. Some models showed strong regeneration without explicit training, likely due to the training for growth and stability. To improve regeneration, the training process was modified to include damaging pool-sampled states, forcing the system to learn recovery from various types of damage.",
      "section": "Experiments",
      "topic": "Regeneration",
      "chunk_summary": "Models trained for growth and stability showed varying regenerative abilities, and training was modified to explicitly encourage regeneration by damaging pool-sampled states.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "Experiment 4 investigated the effect of rotating the perceptive field by rotating the Sobel kernels used for gradient estimation.  Rotating the kernels resulted in rotated versions of the target pattern, demonstrating robustness to changes in the perceptive field. This suggests that the model learns a more general representation of the target shape, not just a specific sequence of actions based on the initial orientation of the perceptive field.",
      "section": "Experiments",
      "topic": "Rotating Perceptive Field",
      "chunk_summary": "Rotating the perceptive field by rotating the Sobel kernels resulted in rotated patterns, demonstrating robustness and a more general shape representation.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "This work builds upon a rich history of CA and PDE models in simulating biological systems, including Turing patterns, the Gray-Scott model, and Conway's Game of Life.  More recent work like SmoothLife and Lenia explored continuous CAs.  Evolutionary algorithms have been used to find CA rules for specific patterns, including regeneration of simple shapes like the French flag.  The close relationship between Convolutional Neural Networks and CA has been noted, and our Neural CA could be considered a type of recurrent residual convolutional network.  Self-organization is becoming more prevalent in machine learning with Graph Neural Networks and dynamic graph networks.",
      "section": "Related Work",
      "topic": "CA and Neural Networks",
      "chunk_summary": "This work relates to previous research on CA, PDEs, continuous CAs, evolutionary algorithms for CA rule discovery, and the connection between CNNs and CA.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "This model has implications for embryogenesis, regeneration, bioengineering, and synthetic morphology.  Understanding how organisms achieve and control regeneration is crucial for biomedical repair.  As bioengineering moves towards creating living machines, programming system-level capabilities like anatomical homeostasis becomes essential.  Recent research shows that target morphology is maintained by a physiological circuit, not hard-coded in DNA, and this setpoint can be rewritten.  Developing computational models of these processes is key for rationally editing this information and achieving desired large-scale outcomes in regenerative medicine.",
      "section": "Discussion",
      "topic": "Embryogenetic Modeling",
      "chunk_summary": "The model has implications for understanding and controlling regeneration, crucial for biomedical repair and bioengineering of living machines.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    },
    {
      "text": "A physical implementation of this system could be a grid of tiny computers representing cells, each with minimal ROM and RAM, communicating state vectors to neighbors and displaying color.  This decentralized system could be programmed to achieve and recover predefined global states, potentially leading to reliable, self-organizing agents.  This decentralized approach contrasts with traditional global modeling in deep learning and may inspire further exploration of decentralized learning models.",
      "section": "Discussion",
      "topic": "Engineering and Machine Learning",
      "chunk_summary": "A physical implementation of the model could be a grid of interconnected microcomputers, offering a decentralized approach to achieving complex tasks.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124125_963853.pdf",
      "original_filename": "pdf_20250803_124125_963853_extracted_text.txt",
      "authors": "[\"Alexander Mordvintsev\", \"Ettore Randazzo\", \"Eyvind Niklasson\", \"Michael Levin\"]",
      "year": "2020",
      "journal": "Distill",
      "doi": "10.23915/distill.00023"
    }
  ],
  "gemini_response": "",
  "processed_at": "2025-08-06T11:46:44.153512",
  "enriched_at": "2025-08-11T14:37:49.543011",
  "enrichment_methods": [
    "crossref"
  ]
}