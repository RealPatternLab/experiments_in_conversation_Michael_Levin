{
  "file_metadata": {
    "text_file": "data/extracted_text/pdf_20250803_124126_292642_extracted_text.txt",
    "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
    "pdf_filename": "pdf_20250803_124126_292642.pdf",
    "file_size": 36605,
    "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
    "journal": "Unknown",
    "doi": null,
    "year": null,
    "title": "Morphogenesis as Bayesian Inference: A Variational Approach to Pattern Formation and Control in Complex Biological Systems",
    "confidence_score": null,
    "document_type": "research_paper"
  },
  "chunks": [
    {
      "text": "Recent advances in molecular biology such as gene editing, bioelectric recording and manipulation, and live cell microscopy using fluorescent reporters - especially with the advent of light-controlled protein activation through optogenetics - have provided the tools to measure and manipulate molecular signaling pathways with unprecedented spatiotemporal precision. This has produced ever increasing detail about the molecular mechanisms underlying development and regeneration in biological organisms. However, an overarching concept – that can predict the emergence of form and the robust maintenance of complex anatomy - is largely missing in the field.",
      "section": "Abstract",
      "topic": "Need for a predictive model of morphogenesis",
      "chunk_summary": "Despite advances in molecular biology, a comprehensive theory predicting morphogenesis is lacking.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Classic (i.e., dynamic systems and analytical mechanics) approaches such as least action principles are difficult to use when characterizing open, far-from equilibrium systems that predominate in Biology. Similar issues arise in neuroscience when trying to understand neuronal dynamics from first principles. In this (neurobiology) setting, a variational free energy principle has emerged based upon a formulation of self-organization in terms of (active) Bayesian inference.",
      "section": "Abstract",
      "topic": "Limitations of classical approaches and introduction of free energy principle",
      "chunk_summary": "Classical approaches struggle with biological systems, prompting the application of the variational free energy principle from neuroscience.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The free energy principle has recently been applied to biological self-organization beyond the neurosciences. For biological processes that underwrite development or regeneration, the Bayesian inference framework treats cells as information processing agents, where the driving force behind morphogenesis is the maximization of a cell's model evidence. This is realized by the appropriate expression of receptors and other signals that correspond to the cell's internal (i.e., generative) model of what type of receptors and other signals it should express.",
      "section": "Abstract",
      "topic": "Free energy principle in morphogenesis",
      "chunk_summary": "The free energy principle models cells as information processors maximizing model evidence through receptor expression.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The emerging field of the free energy principle in pattern formation provides an essential quantitative formalism for understanding cellular decision-making in the context of embryogenesis, regeneration, and cancer suppression. In this paper, we derive the mathematics behind Bayesian inference as understood in this framework and use simulations to show that the formalism can reproduce experimental, top-down manipulations of complex morphogenesis. First, we illustrate this 'first principle' approach to morphogenesis through simulated alterations of anterior-posterior axial polarity (i.e., the induction of two heads or two tails) as in planarian regeneration. Then, we consider aberrant signaling and functional behavior of a single cell within a cellular ensemble as a first step in carcinogenesis as false 'beliefs' about what a cell should 'sense' and 'do'. We further show that simple modifications of the inference process can cause and rescue mis-patterning of developmental and regenerative events without changing the implicit generative model of a cell as specified, for example, by its DNA. This formalism offers a new road map for understanding developmental change in evolution and for designing new interventions in regenerative medicine settings.",
      "section": "Abstract",
      "topic": "Applying free energy principle to morphogenesis and cancer",
      "chunk_summary": "This paper applies the free energy principle to model morphogenesis, cancer, and regeneration, demonstrating its potential for understanding developmental change and designing regenerative interventions.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Evolutionary change results from mutations in DNA and selection acting on functional bodies. Thus, it is essential to understand how the hardware encoded by the genome enables the behavioral plasticity of cells that can cooperate to build and repair complex anatomies. Indeed, most problems of biomedicine - repair of birth defects, regeneration of traumatic injury, tumor reprogramming, etc. could be addressed if prediction and control could be gained over the processes by which cells implement dynamic pattern homeostasis. The fundamental knowledge gap and opportunity of the next decades in the biosciences is to complement bottom-up molecular understanding of mechanisms with a top-down computational theory of cellular decision-making and infotaxis.",
      "section": "Introduction",
      "topic": "Bridging the gap between molecular mechanisms and computational models of cell behavior",
      "chunk_summary": "Understanding how genomic information translates into cellular behavior is crucial for addressing biomedical challenges, requiring a computational theory of cellular decision-making.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Relevant concepts have been developed in neuroscience and physics, but are generally not familiar to developmental or regenerative biologists. Here, we lay out the mathematical foundation of the type of Bayesian modeling employed by new approaches to understand metazoan cell cooperation to characterize - and simulate - pattern formation. We start by identifying a Lyapunov function that can be used to analyze and solve any dynamic system, using the fundamental theorem of vector calculus (i.e., the Helmholtz Decomposition). We use it to characterize the generalized flow of systemic states, in terms of convergence to a non-equilibrium steady-state. We then introduce the notion of a Markov blanket that separates the external and internal states of the system, where the Markov blanket is comprised of active and sensory states. Using this partition, we can then replace the Lyapunov function with a variational free energy to solve for the evolution of internal and active states and thereby characterize self-organization in far from equilibrium systems that can be partitioned into a cell (i.e., internal states and their Markov blanket) and the external milieu.",
      "section": "Introduction",
      "topic": "Mathematical foundation of Bayesian modeling for pattern formation",
      "chunk_summary": "This paper presents the mathematical basis of Bayesian modeling for understanding pattern formation, using concepts like Lyapunov functions, Helmholtz decomposition, and Markov blankets.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Subsequent sections apply this formalism to illustrate morphogenesis and neoplasia using simulations. Bayesian inference is a statistical process, wherein Bayes theorem is used to update the probability of a hypothesis with respect to evidence obtained by measurement of the sensorium or environment. In essence, any kind of information processing system infers unobservable (i.e., hidden) states of its environment by comparing sensory samples with predictions of sensory input and updating its expectations about the causes of that input. Bayes theorem rests on the three basic axioms of probability theory and is used to relate the conditional probability of an unobservable event A, given an observable quantity B, to the likelihood of B, given that A is true. This is written as: P(A | B) = P(B|A)P(A) / P(B) , where conditional probability P(A|B) is also called the posterior; namely, the inferred probability of an event A, given an event B. Conversely, P(B|A) is the probability of B, given A, called the likelihood. The probability P(A) is called a prior belief and the probability of P(B), is called marginal likelihood or evidence. In Bayesian inference, the above relationship is used to accumulate information about an unobservable or hidden state by sampling measurable events. This is known as Bayesian belief updating, because it converts prior beliefs into posterior beliefs - based on a generative model. In short, the likelihood assigned to the observation and prior beliefs are combined to form posterior beliefs.",
      "section": "Introduction",
      "topic": "Bayesian inference and belief updating",
      "chunk_summary": "Bayesian inference uses Bayes' theorem to update beliefs about hidden states based on observed evidence, combining prior beliefs and likelihood to form posterior beliefs.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "To describe the dynamics of an ensemble of information processing agents (as in cells, for example) as a process of Bayesian belief updating, we need to relate the stochastic differential equations governing Newtonian motion and biochemical activity to the probabilistic quantities above. This is fairly straightforward to do, if we associate biophysical states with the parameters of a probability density - and ensure their dynamics perform a gradient flow on a quantity called variational free energy. Variational free energy is a quantity in Bayesian statistics that, when minimized, ensures the parameterized density converges to the posterior belief, as we will see below. In neuroscience, the minimization of variational free energy is referred to as active inference. This approach to neuronal dynamics has been successfully used to reproduce a variety of neuronal phenomena.",
      "section": "Introduction",
      "topic": "Relating biophysical dynamics to Bayesian belief updating",
      "chunk_summary": "Biophysical dynamics can be related to Bayesian belief updating by associating states with parameters of a probability density undergoing gradient flow on variational free energy.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Crucially, exactly the same scheme has been shown recently - through computational proof-of-principle simulations - to produce and maintain the somatic patterning of self-organization. We will see that when the basic condition for an inference type description of a system namely, the existence of a Markov blanket separating external and internal states - is satisfied, agents such as biological cells form into organized conglomerations based on their generative models of how of their blanket states influence – and are influenced by external states in the external milieu (i.e., the states of other cells).",
      "section": "Introduction",
      "topic": "Markov blankets and self-organization",
      "chunk_summary": "The presence of a Markov blanket allows agents like cells to self-organize based on their generative models and interactions with the external environment.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "In classical thermodynamic descriptions, this would be accompanied by an increase of thermodynamic entropy over the entire system, through localized increases in organization (i.e., decrease in entropy) of the states associated with each cell (i.e., internal states and their Markov blanket). However, as biological systems, especially cells, are invariably open, far-from-equilibrium or non-equilibrium steady state systems, the dynamics of this process are almost impossible to compute. Instead, by focusing on a probabilistic account of self-organization, in terms of Bayesian belief updating, we can place an upper bound on the entropy of the system's blanket states that is computationally tractable. In brief, we will see that the dynamics of system with a Markov blanket that self-organizes to non-equilibrium steady-state can be described as a gradient flow on this computable (variational) free energy bound. This approach has been shown to have a high predictive validity in neurobiology; both in terms of behavior and the neuronal correlates of action and perception. However, its application in the broader biosciences has not been explored, even though the basic assumptions behind it apply broadly.",
      "section": "Introduction",
      "topic": "Probabilistic account of self-organization and variational free energy",
      "chunk_summary": "A probabilistic approach using Bayesian belief updating and variational free energy provides a computationally tractable way to describe self-organization in open, far-from-equilibrium biological systems.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "In what follows, we introduce the mathematics that underwrites the Bayesian interpretation of non-equilibrium steady-state dynamics. We will start with a brief overview of the Helmholtz decomposition and Lyapunov functions in dynamical systems. We will see that one can formulate any dynamics in terms of a potential function that plays the role of a Lyapunov function. This is illustrated from the point of view of classical mechanics with dissipative aspects. We then derive the same result in terms of density dynamics using the Fokker Planck equation, in generalized coordinates of motion. This formulation shows that the potential or Lyapunov function is simply the negative log probability of a state being occupied at non-equilibrium steady-state. Crucially, this quantity is bounded from above by variational free energy. This means the flow of particular states at non-equilibrium steady-state can be cast as a gradient flow on the same quantity that is minimized by Bayesian belief updating.",
      "section": "Mathematical Foundations",
      "topic": "Mathematical framework for Bayesian interpretation of dynamics",
      "chunk_summary": "The mathematical framework presented uses Helmholtz decomposition, Lyapunov functions, and the Fokker-Planck equation to link non-equilibrium steady-state dynamics to Bayesian belief updating through variational free energy.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "The Helmholtz decomposition states that any sufficiently smooth (i.e., possessing continuous derivatives) vector field F can be decomposed into an irrotational (curl-free) and a solenoidal (divergence-free) vector field. Because an irrotational vector field has only a scalar potential and a solenoidal vector field has only a vector potential, we can express the vector field as F = -∇I + ∇ × A, where I and ∇ × A are the irrotational and solenoidal vector fields respectively.",
      "section": "Stability and Convergence in Coupled Dynamical Systems",
      "topic": "Helmholtz decomposition",
      "chunk_summary": "The Helmholtz decomposition separates a vector field into irrotational and solenoidal components, represented by scalar and vector potentials.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Lyapunov functions have been used extensively in dynamical systems theory and engineering to characterize the stability of fixed points of a dynamical system. Lyapunov functions are generally defined for smooth systems through the following conditions: (a) L(x*) = 0, and L(x) > 0 if x ≠ x* (b) dL(x)/dt ≤ 0, for all x ∈ O, where O ⊂ R is an open set containing all states x. (a) requires the Lyapunov function L to be minimal for fixed points x* representing local minima, and (b) denotes convergence to these fixed points over time. Following Yuan et al., 2014, we can generalize this local Lyapunov function of stability to a global Lyapunov function that plays the role of a potential function of any dynamical system. This follows by generalizing condition (a) to allow for saddle points: ∇L(x*) = 0",
      "section": "Stability and Convergence in Coupled Dynamical Systems",
      "topic": "Lyapunov functions",
      "chunk_summary": "Lyapunov functions characterize the stability of dynamical systems, with a generalized version allowing for saddle points.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Following Yuan et al., 2014 we show how a Lyapunov function is equivalent to a potential function, when characterizing the stability of a dynamical system. In physics, a potential function ψ can be constructed to describe the flow of or forces acting on a particle through a potential energy gradient: Fpot = ∇ψ. These forces are conservative, where the total work done on the particle is independent of its trajectory (e.g., Gravitational force). However, there are also dissipative, or non-conservative forces, for which the total work done depends on the particle's trajectory and is hence irreversible (e.g., frictional force). At steady-state, these components balance each other, so that the total Force Ftot is zero: Ftot = Fcon + Fdis = 0, where Fcon and Fdis are the conservative and dissipative forces respectively. For example, in electromagnetics, the Lorentz force describes the forces acting on a moving charged particle: FLorentz = qE + ev × B, where q is its charge, v the velocity of the particle, and E and B are the electric and magnetic forces, respectively. We can therefore write Fcon as a combination of Lorentz force and potential energy induced force: Fcon = -∇ψ(x) + ev × B, while the dissipative force can be expressed as a frictional force (due to dissipative random fluctuations): Fdis = -Sv. Here, S is a symmetric and semi-positive definite friction tensor. Combining these definitions, we can express the total force as a balance of the forces as defined above, resulting in: Sv + ev × B = -∇ψ(x). One can generalize this equation for arbitrary n-dimensional systems by replacing the vector-valued cross product v × B = Tv, where T is an antisymmetric matrix to give the canonical form of: (S + T)v = -∇ψ(x). Finally, following Yuan et al., 2014 we can transform this expression into a standard form using a diffusion tensor Γ (defined as half the covariance of the dissipative random fluctuations) and a tensor Q (describing friction) satisfying ∇ • Q∇ψ(x) = 0, by setting ψ(x) as the Lyapunov function L(x) as defined above so that we get: f(x) = v = (Q – Γ)∇L(x), where f(x) describes the flow of states. This equation describes the evolution or flow of states resulting from (conservative and dissipative) forces at non-equilibrium steady-state. In summary, for any dynamical system at non-equilibrium steady-state, we can express the flow in terms of a scalar potential or Lyapunov function f(x) = L(x), where the flow can always be decomposed into a gradient flow, which minimizes the potential, and a solenoidal component, that flows on the iso-contours of the potential. The final move is to associate the Lyapunov function or potential with variational free energy as follows.",
      "section": "Stability and Convergence in Coupled Dynamical Systems",
      "topic": "Lyapunov functions as potential functions",
      "chunk_summary": "Lyapunov functions can be treated as potential functions describing the flow of states in dynamical systems, incorporating both conservative and dissipative forces.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Variational free energy is a function of internal states that allows one to associate the Lyapunov function from (17) with Bayesian model evidence and hence characterize systemic dynamics in terms of Bayesian inference and the implicit generative models. This device works by unpacking the non-equilibrium steady-state flow of external, internal and blanket states. Under this partition, instead of minimizing the Lyapunov function or (thermodynamic) potential, the internal and active states come to minimize variational free energy. Crucially, the variational free energy is defined in terms of a generative model and implicit posterior beliefs encoded by internal states. This minimization licenses an interpretation of self-organization in terms of belief updating according to Bayes rules above. In turn, this allows us to specify the resulting non-equilibrium steady-state in terms of a generative model - and ensuing inference - as we will see below. First, we will revisit the standard form for dynamics above, in the setting of generalized coordinates of motion and density dynamics as described by the Fokker Planck equation.",
      "section": "Variational Free Energy",
      "topic": "Variational free energy and generative models",
      "chunk_summary": "Variational free energy links Lyapunov functions to Bayesian model evidence, allowing the interpretation of self-organization as belief updating based on a generative model.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "We can describe dynamics in generalized coordinates of motion, denoted with a tilde, where x̃ is defined as: x̃ = (x, ẋ, ẍ, ...). This augments a state with its velocity, acceleration and so on. Later, we will use generalized coordinates of motion to parameterize a posterior density over (the generalized motion of) external states (that are hidden behind the Markov blanket). Among other advantages, generalized coordinates of motion allow one to accommodate temporal correlations in random fluctuations. Assuming a smooth dynamical system, subject to random fluctuations, we can describe the motion of states with the Langevin equation: ẋ = f(x) + ῶ, where f(x) is the generalized flow (or time evolution) of states due to forces acting on the states and ῶ are random fluctuations, under the usual Wiener assumptions (the flow of states is made up of a process of independent, Gaussian increments that follow a continuous path). In statistical physics the ensuing dynamics is commonly described in terms of density or ensemble dynamics; namely, the evolution of the probability density p(x), through the Fokker-Planck equation. The Fokker Planck equation can be obtained for any Langevin equation, using the conservation of probability mass: ṗ(x) = ∇ • [ẋp(x)] = 0, where ẋp(x) describes the probability current. This turns the Fokker-Planck equation into a continuity equation, which reads: ṗ(x) = ∇ • Γ∇p – ∇ • (f(x)p). This is a partial differential equation that describes the time evolution of the probability density p(x) under dissipative (first term) and conservative (second term) forces. At non-equilibrium steady-state, the density dynamics is just the solution to the Fokker Planck equation: L(x) = -ln p(x), such that ṗ = -p∇L and ṗ = 0. Using the Helmholtz decomposition from (2), we can now express steady-state flow in terms of a divergence-free component and a curl-free descent on a scalar Lyapunov function L(x) to obtain f(x) = (Q – Γ)∇L(x). This is the solution at non-equilibrium steady-state and is exactly the same solution for the flow of particles in the classical treatment above. Crucially, we can now see that the Lyapunov function is the negative log probability of finding the system in any (generalized) state L(x) = -ln p(x). This is also known as the self-information of a state in information theory (also known as surprisal, or more simply surprise). In Bayesian statistics it is known as the negative log evidence.",
      "section": "Generalized Flow",
      "topic": "Generalized flow and Fokker-Planck equation",
      "chunk_summary": "Generalized coordinates of motion and the Fokker-Planck equation describe the evolution of probability density in dynamical systems, with the Lyapunov function at non-equilibrium steady-state representing the negative log probability of a state.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "In summary, any weakly mixing dynamical system that at non-equilibrium steady-state will evince a flow that can be decomposed into a gradient flow on surprise and an accompanying solenoidal flow. Because we can associate the Lyapunov function in (18) with a free energy, the system is effectively minimizing a free energy in its convergence to a set of attracting states (known as a random dynamical attractor), which have a high probability of being occupied; namely a high marginal likelihood or evidence. This construction is used extensively in biophysical research fields, such as protein folding to solve for steady-state solutions.",
      "section": "Generalized Flow",
      "topic": "Minimizing free energy and convergence to attractor states",
      "chunk_summary": "Dynamical systems at non-equilibrium steady-state minimize free energy, converging to attractor states with high probability.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "Physics offers a useful formalism to understand, at a quantitative level, the ability of biological systems (as evidenced by regulative development and regeneration) to work towards an invariant outcome, despite various perturbations. Understanding this 'goal-directed' activity is an important open problem in biological control. The least action principle can predict the emergence of form, in terms of the flow or paths of least action in biological systems. For example, in colonies, ants find the paths of least action to harvest food and bring it to the colony. This example considers their paths as flow channels, or trajectories, finding the least average action for each instance of foraging, given available resources. More generally, minimization of action in an open system leads to structure formation. The 'flows' in such (dissipative) systems are of energy, matter and constituent elements along the paths of least action. An open dynamical system tends towards its state of least action, or the 'most action efficient state'. A canonical example of the emergence of such dissipative structures is when a moving fluid (e.g., a river) erodes obstructions to its flow to form a network of flow channels.",
      "section": "Least Action Principles",
      "topic": "Least action principle and emergence of form",
      "chunk_summary": "The least action principle explains the emergence of form in biological systems, where systems tend towards states of least action, as exemplified by ants finding efficient foraging paths and rivers eroding obstructions.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "In (dissipative) random dynamical systems, action is not minimized for each element of the system, but, on average over an ensemble of elements (or repeated trajectories of the same element). Obstructive-constraint minimization therefore reduces action for each event within the system and self-organizes it, forming a flow structure that could be construed as a dissipative structure. Crucially, since self-organizing open systems are not conservative, their structured flow is quintessentially dissipative. While the Lyapunov function of a physical system is readily used to establish the stability of a fixed point in dynamical systems, physicists commonly use the Lagrangian to solve the trajectory of a systems states. Classically, for a conservative system, the Lagrangian is defined as: L = T - V, where V is the potential energy of the system, defined through the constraints of the system, and T is the kinetic energy of the particles that constitute the system at hand. For any Lagrangian, the trajectory of states in generalized coordinates (t, x(t), ẋ(t)) are given by the solutions to the the Euler-Lagrange equation, which are bound by the principle of variations to be functions for which the following functional has extrema (i.e., is stationary): S(x) = ∫ L(t, x(t), ẋ(t)) dt. S integrates the Lagrangian of generalized states for boundary conditions defined for initial and final time points t₁ and t₂. The most likely path between these points is obtained when the functional derivative is zero; i.e., δS = 0. This is the Hamilton's principle. In this case, the equations of motion are derived from the Euler-Lagrange equations which are the solutions of the principle of least action: d/dt (∂L/∂ẋᵢ) - ∂L/∂xᵢ = 0 for i = 1,2,..., n. Where xᵢ are the generalized coordinates and ẋᵢ the generalized velocities.",
      "section": "Least Action Principles",
      "topic": "Lagrangian mechanics and least action principle",
      "chunk_summary": "In dissipative systems, action is minimized on average, leading to self-organization. The Lagrangian, defined as the difference between kinetic and potential energy, is used to determine the trajectory of states through the Euler-Lagrange equation and the principle of least action.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "For dissipative systems, this equation has additional dissipative terms. For example, if the dissipative function depends on the square of the velocity: F = ½kx², Then the Euler-Lagrange equations become: d/dt (∂L/∂ẋᵢ) - ∂L/∂xᵢ + ∂F/∂ẋᵢ = 0 The constraints to motion of the agents in a system are given additionally by the Lagrange multipliers. δ∫ [L(t, x(t), ẋ(t)) + Σₖ λₖ(t)gₖ(t, x(t))]dt = 0 with the added dissipative terms are as follows. d/dt (∂L/∂ẋᵢ) - ∂L/∂xᵢ + ∂F/∂ẋᵢ + Σₖ λₖ∂gₖ/∂xᵢ = 0 Where λₖ are the Lagrange multipliers, and gₖ are the constraints. The solutions are the constrained Lagrangian equations of motion, which Terms with random noise can also be added to this equation, which are pertinent for biological systems. Because the Lagrangian describes the trajectories of particles under forces, the functional S is the action of the system. Hence, when the variational principle is applied to the action of a system in this manner, it is referred to as a least action principle. To apply least action principles to the kind of systems of interest in biology, it is necessary to consider the action of an ensemble of systems of particles. Minimizing the average action allows individual trajectories to deviate from their paths of least paths, so that they can reduce the action of other particles. The most likely solution for an ensemble minimizes the ensemble average of action, compared to other arrangements of particles and implicit constraints on their flow. As the system evolves, it searches forever lower minima of this average action. This means that the principle of least action does not apply in isolation to each member of the ensemble but is contextualized by coupling between particles that depend upon many characteristics. These characteristics include: the number of particles, the number of interactions, the total action of the system within certain interval of time, etc. Furthermore, these interdependent functions (interfunctions) are bound by power law relations. From our perspective, the key observation here is that any (dissipative) random dynamical system can be formulated as a gradient flow on the log likelihood of its states. This is reflected in our solution L(x) = − ln p(x) to the Fokker-Planck equation in (17), which means the action is the time or path integral of the marginal likelihood or self-information: S = ∫ f(x(t)) dt = ∫ ln p(m) dt, for any system or model m. This means, the least action integral over the Lagrangian turns into an integration over the self-information of states, which is known as entropy in information theory. In short, the principle of least action manifests as a principle of least entropy for systems that possess a random dynamical attractor - and thereby obtain non-equilibrium steady-state. We now consider the specific structure of the system or model m that underwrites Bayesian inference; namely, the Markov blanket.",
      "section": "Least Action Principles",
      "topic": "Least action principle in dissipative systems and relation to entropy",
      "chunk_summary": "In dissipative systems, minimizing average action leads to self-organization and can be formulated as a gradient flow on log likelihood.  The principle of least action becomes a principle of least entropy for systems with a random dynamical attractor.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    },
    {
      "text": "A robust literature is developing around the ability of cells and many other aneural systems measuring aspects of their environment via specific sensors. All biological systems can be analyzed in terms of sensory and internal states and the relationships between them. A Markov partition separates all states x ∈ X into external e ∈ E, sensory s ∈ S, active a ∈ A, and internal states i ∈ I (with their generalized versions x̃, ē, š, ã, and ĩ), so that x ∈ X = E×S×A×I, where × denotes the Cartesian product that returns a product set of sets. The ensuing partition is defined in Table 1. The Markov blanket separating external and internal states is hence given by S × A, as depicted in Figure 1. The partition into external, internal and blanket states rests upon conditional independencies implicit in the system's equations of motion or dynamics. In brief, external and internal states depend only upon blanket states, subject to the constraint that sensory states are not influenced by internal states and active states are not influenced by external states. With the Markov partition (and associated influences) in hand, the flow f(x) can then be decomposed into 4 parts: fₑ(ē, š, ã) fₛ(ē, š, ã) fₐ(š, ã, ĩ) fᵢ(š, ã, ĩ) The response of active and internal states, to sensory stimuli, therefore, becomes (a) fₐ(š, ã, ĩ) = (Qₐ – Γₐ)∇ₐL(š, ã, ĩ) (b) fᵢ(š, ã, ĩ) = (Qᵢ – Γᵢ)∇ᵢL(š, ã, ĩ) (c) L(š, ã, ĩ) = − ln p(š, ã, ĩ|m), where m describes the Markov partition that defines the underlying random dynamical system (e.g., a cell). Inserting (c) into (a) and (b), gives: (a') fₐ(š,ã, ĩ) = (Γₐ – Qₐ)∇ₐ ln p(š, ã, ĩ|m) (b') fᵢ(š,ã, ĩ) = (Γᵢ - Qᵢ)∇ĩ ln p(š, ã, ĩ|m) The key aspect of this dynamics is that the autonomous (i.e., active and internal) states of an agent depend upon same quantity, which reduces to the log probability of finding the agent in a particular state; where the agent's states comprise the internal states and their Markov blanket. In this partition, autonomous states are those states that do not depend upon external states; namely, internal and active states. Solving equation (30) for the evolution f of active and internal states thus corresponds to evaluating the gradients of the log probabilities above that correspond to the Lagrangian of an open system. In general, this would be a very difficult problem to solve; however, we can now replace the Lagrangian with a variational free energy functional of a probabilistic model of how a system thinks it should behave, as follows.",
      "section": "Markov Blanket",
      "topic": "Markov blanket and state partitioning",
      "chunk_summary": "A Markov blanket separates external and internal states, with sensory and active states forming the blanket. The dynamics of active and internal states depend on the log probability of the agent's state, which can be replaced by a variational free energy functional.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250803_124126_292642.pdf",
      "original_filename": "pdf_20250803_124126_292642_extracted_text.txt",
      "authors": "[\"Franz Kuchling\", \"Karl Friston\", \"Georgi Georgiev\", \"Michael Levin\"]",
      "year": null,
      "journal": "Unknown",
      "doi": null
    }
  ],
  "gemini_response": "",
  "processed_at": "2025-08-06T12:44:12.607431"
}