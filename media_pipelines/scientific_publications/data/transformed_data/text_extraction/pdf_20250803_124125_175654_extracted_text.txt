Gene regulatory networks exhibit several kinds of
memory: Quantification of memory in biological
and random transcriptional networks

Surama Biswas,
Santosh Manicka,
Erik Hoel, Michael
Levin

HIGHLIGHTS

Gene regulatory
networks' dynamics are
modified by transient
stimuli

GRNs have several
different types of memory,
including associative
conditioning

Evolution favored GRN
memory, and
differentiated cells have
the most memory capacity

Training GRNs offers a
novel biomedical strategy
not dependent on genetic
rewiring


[PAGE 2] Gene regulatory networks exhibit several
kinds of memory: Quantification of memory
in biological and random transcriptional networks

Surama Biswas,¹ Santosh Manicka,¹ Erik Hoel,¹ and Michael Levin 1,2,3,*

SUMMARY
Gene regulatory networks (GRNs) process important information in developmental biology and biomedicine. A key knowledge gap concerns how their responses change over time. Hypothesizing long-term changes of dynamics induced by transient prior events, we created a computational framework for defining and identifying diverse types of memory in candidate GRNs. We show that GRNs from a wide range of model systems are predicted to possess several types of memory, including Pavlovian conditioning. Associative memory offers an alternative strategy for the biomedical use of powerful drugs with undesirable side effects, and a novel approach to understanding the variability and time-dependent changes of drug action. We find evidence of natural selection favoring GRN memory. Vertebrate GRNs overall exhibit more memory than invertebrate GRNs, and memory is most prevalent in differentiated metazoan cell networks compared with undifferentiated cells. Timed stimuli are a powerful alternative for biomedical control of complex in vivo dynamics without genomic editing or transgenes.

INTRODUCTION
Gene regulatory networks (GRNs) are key drivers of embryogenesis, and their importance for guiding cell behavior and physiology persists through all stages of life (Alvarez-Buylla et al., 2008; Huang et al., 2005). Understanding the dynamics of GRNs is of high priority not only for the study of developmental biology (Davidson, 2010; Peter and Davidson, 2011) but also for the prediction and management of numerous disease states (Fazilaty et al., 2019; Qin et al., 2019; Singh et al., 2018). Much work has gone into the computational inference of GRN models (De Jong, 2002; Delgado and Gómez-Vela, 2019), and the development of algorithms for predicting their dynamics over time (Schlitt and Brazma, 2007). However, the field has been largely focused on rewiring—modifying the inductive and repressive relationships between genes—to control outcome. This can be difficult to control in biomedical contexts, and even in amenable model systems, it is often unclear what aspects of the network should be altered to result in desired system-level behavior of the network. Dynamical systems approaches have made great strides in understanding how GRNs settle on specific stable states (Herrera-Delgado et al., 2018; Zagorski et al., 2017). However, significant knowledge gaps remain concerning temporal changes in GRN dynamics, their plasticity, and the ways in which their behavior could be controlled for specific outcomes via inputs not requiring rewiring.

Thus, an important challenge in developmental biology, synthetic biology, and biomedicine is the identification of novel methods to control GRN dynamics without transgenes or genomic editing, and without having to solve the difficult inverse problem (Lobo et al., 2014) of inferring how to reach desired system-level states by manipulating individual node relationships. A view of GRNs as a computational system, which converts activation levels of certain genes (inputs) to those of effector genes (outputs), with layers of other nodes between them, suggests an alternative strategy: to control network behavior via inputs—spatiotemporally regulated patterns of stimuli that could remodel the landscape of attractors corresponding to a system's "memory." A broad class of systems, from molecular networks (Szabó et al., 2012) to physiological networks in somatic organs (Goel and Mehta, 2013; Turner et al., 2002), exhibit plasticity and history-based remodeling of stable dynamical states. Could GRNs likewise exhibit history dependence that could help us explain the variability of cellular responses, and that could be exploited to control their function by modulating the temporal sequence of inputs? This is a different approach from existing conceptions of memory as changes at the epigenetic and protein levels (Corre et al., 2014; Nashun et al., 2015; Quintin et al., 2014; Zediak et al., 2011).

Several prior studies have tested specific memory phenomena in GRN models (Kandel et al., 2014; Levine et al., 2013; Macia et al., 2017; Ryan et al., 2015; Science, 2003; Sible, 2003; Urrios et al., 2016; Watson et al., 2010, 2011; Xiong and Ferrell, 2003). However, there has been no systematization of the kinds of memories that such networks could possibly exhibit. We sought to rigorously define several types of memory (loosely analogous to those found in the behavioral science of neural networks), provide an algorithm with which any future network model can be evaluated for interesting memory dynamics (to make predictions for experiment), and compare existing models of important biological networks with those of random networks.

[PAGE 3] One especially intriguing possibility concerns associative learning (Kohonen, 2012; Palm, 1980). The textbook experiment by Pavlov illustrates associative learning in a specific form known as "classical conditioning" (Lee and Young, 2013; Rescorla, 1967) (Figures 1A-1C). Here, initially, the dog naturally salivates when it smells food, termed the unconditioned stimulus (UCS), and does not salivate when it hears a bell ring (Figure 1A), making the bell the neutral stimulus (NS). The smell of food and the sound of a bell are unrelated stimuli, and only one, the UCS, induces the dog's salivation (the response R). In this experiment, the dog is exposed to the UCS and NS at the same time repeatedly (Figure 1B). Gradually, the dog learns to associate the NS with the UCS, to the point where it responds to the bell alone as if food is present, functionally

[PAGE 4] transforming the NS to a conditioned stimulus (CS), which can now produce the response R (Figure 1C). Although associative learning is traditionally studied as a neural phenomenon, many different types of dynamical systems can instantiate it (Baluška and Levin, 2016; Fernando et al., 2009; Manicka and Levin, 2019a, 2019b; McGregor et al., 2012) (Figure 1D). Indeed, the original experiments of Pavlov showed associative and other kinds of learning within his dogs' organ systems (Gantt, 1974, 1981), in addition to the well-known learning of the animal via its brain.

In biomedical contexts, some drugs targeting specific network nodes are highly effective in laboratory studies but too toxic to use long-term in patients (Frey et al., 2019). If associative memory existed in GRNs, predictive algorithms could be developed to reveal which stimuli can be used to trigger desired responses via a paired "training" paradigm. In this case, the network would associate the effects (R) of a powerful but toxic drug (UCS) with a harmless one (NS, which would become the CS). It might then be possible to treat the patient with the neutral drug (NS) to obtain the desired therapeutic response of the UCS without the side effects (Figure 1D). This is just one example of a number of strategies that can be developed for rational control of GRN function, once the memory properties of GRNs of interest were characterized.

To achieve this, we rigorously systematized the notion of memory in dynamical models of GRNs and similar types of networks and developed algorithms to analyze the plasticity of response to specific node activations over time. Here, we focus on a well-known class of dynamical models known as Boolean networks (BN) that was pioneered by Stuart Kauffman (Kauffman, 1969) and Rene Thomas (Thomas, 1973) as simple coarse-grained models of GRNs. The nodes (variables) in a BN are binary: a state of 0 (OFF) represents repression, whereas 1 (ON) represents activation. Gene states are updated over time due to interactions with other genes and their transcripts, as described by the Boolean functions associated with each node. The Boolean operators defining the relations among the genes are AND, OR, NOT, and XOR (see Transparent methods for more details). Boolean models have proved to be useful in gaining dynamical insight into numerous phenomena, such as criticality (Kauffman and Strohman, 1994), cell signaling (Saez-Rodriguez et al., 2009), pattern formation and control (Marques-Pita and Rocha, 2013), cancer reprogramming (Zanudo and Albert, 2015), drug resistance (Eduati et al., 2017), and even memory in plants (Demongeot et al., 2019); the Cell Collective model database (Helikar et al., 2012) that we utilize in this work contains many more such published examples. For comprehensive reviews of BNs, including aspects of how they are inferred, analyzed, and used to make predictions, see Albert et al. (2008), Albert and Thakar (2014), Albert (2004), and Wang et al. (2012).

We hypothesized that GRNs in general may be capable of diverse new kinds of memory, in that their response to future node activation events would change to implement the desired network behavior, and that an algorithm could discover the necessary sequence of stimuli to make this occur predictably. Such long-term change in behavior due to experience (memory) could occur via changes at the level of the dynamical system state space, not requiring changes in inductive/repressive relationships between genes (rewiring the connectivity). We specifically hypothesized that such historicity would be an inherent property of networks but would be significantly enriched in real biological GRNs. It is important to note that the memory being tested here takes place within the lifetime of a single, constant GRN, rather than during a process of evolutionary selection or population learning.

[PAGE 5] If found, long-term changes in GRNs' dynamical system states would be analogous to intrinsic plasticity in neuroscience, which functions alongside synaptic plasticity (rewiring that changes the connection weights between nodes). There is increasing biological evidence that learning and memory happen at the level of single neurons and that memory could be stored in their dynamic activities as intrinsic plasticity due to the dynamics of bioelectric circuits (Banerjee, 2015; Daoudal and Debanne, 2003; Debanne et al., 2003; Gallaher et al., 2010; Geukes Foppen et al., 2002; Izquierdo et al., 2008; Law and Levin, 2015; Snipas et al., 2016). The theoretical foundations of such plasticity-free learning have been explored (Stockwell et al., 2015; Yamauchi and Beer, 1994). Thus, the existence of plasticity-free memory in GRNs would have major implications along several lines. First, it would suggest novel developmental programs where dynamic gene expression could result from GRNs whose functional behavior was shaped by prior biochemical interactions and not genomically hardwired. Second, it would suggest a new approach to biomedical interventions complementing gene therapy: drug strategies with temporally controlled delivery regimes could be designed to train GRNs to produce specific outcomes, shape their responses to drug and other interventions in the future, disrupt cancer cells' adaptation to therapeutics, or prevent disease states from arising in specific circumstances. Moreover, an understanding of GRNs' long-term modification by prior physiological experiences could help explain the wide divergence of drug efficacy and side effects across patients and even across clonal model systems (Durant et al., 2017).

The presence of a kind of learning in GRNs has been suggested in specific cases (Deritei et al., 2019; Fernando et al., 2009; Herrera-Delgado et al., 2018; Sherrington and Wong, 1989, 1990; Stockwell et al., 2015; Tagkopoulos et al., 2008; Zañudo et al., 2017), but there has been no systematic study of memory across diverse GRNs or analysis of possible different kinds of memories that may exist and the relationships between them. Moreover, plasticity in the form of changes to the weights of connections, or mechanisms, is generally thought to be required for memory in GRNs. It is also unknown whether memory is a property of all networks (e.g., random ones) or whether biological GRNs exhibit unique memory types or increased propensity for memory. Here, we comparatively analyze the definitions of memory in the context of animal behavior, mapping them onto possible GRN dynamics, providing a taxonomy of learning types appropriate for GRNs and other networks like protein pathways, all without any changes to weights or mechanisms. We rigorously define the kinds of memory that could be present in GRNs (Figures 2 and S1) and then produce an algorithm (Figure S2) to systematically test any given GRN for the presence of different types of memory with different choices of network nodes as stimuli targets.

[PAGE 5] RESULTS

Transcriptional networks can exhibit multiple kinds of memory

A GRN is a model of transcriptional control consisting of genes and their mutual regulations (Blais and Dynlacht, 2005; De Jong, 2002). Each gene has a basal expression level that applies when the gene is neither regulated by any external stimuli nor influenced by other genes (through their encoded proteins). Basal expression levels change when a gene is activated via regulation, which then in turn may modulate others (Macneil and Walhout, 2011; Samal and Jain, 2008). We designate the activation of some nodes as "inputs" (corresponding to specific sensory experiences that an animal may receive from its environment) and the activation of another node R as a "response" (corresponding to a discrete behavior that can be produced under specific circumstances). We define "training" in this context as the stimulation of some of the network nodes in a specific pattern to induce long-lasting changes in how the network responds to node activation events in the future.

We formally define "memory" (Figure 2) in this context as a phenomenon describing the relationship between two sets of genes, namely, "stimulus" and "response" that satisfies the following conditions: (1) the stimulus activates the response and (2) the response retains its activation state even after deactivation of the stimulus (the existence of history). The fundamental signature of memory is its temporality—a long-lasting and stimulus-specific change induced by a transient experience (Bacchus et al., 2013; Chechile, 2018; Durso and Nickerson, 1999; Weitz and Simmel, 2012). We consider individual nodes of a Boolean GRN as the potential targets of external stimuli and able to produce a response (output or effector nodes). For example, a specific transcript can be upregulated by some exogenous factor triggering its expression, and the appearance of a given gene product (e.g., secretion of an important hormone or growth factor) can be considered the circuit's response. For applications, we are especially interested in nodes that can be readily stimulated with small-molecule drugs, and for response, we are interested in nodes that control key drivers of health and disease (e.g., the levels of calcium, pH, immune activation, cell differentiation, etc.). The challenge then, for any given network and response of interest, is to computationally identify the correct nodes that may serve as inputs, as well as a temporal stimulation regime for those stimulus node(s) that will result in desired changes in response activity over time.

Specifically, we consider two types of stimulus nodes, namely, unconditional stimulus (UCS) and neutral stimulus (NS), and a single response node (R). The first type of stimulus, UCS, is capable of triggering R, and the second type, NS, is initially neutral to R but may be conditioned such that it becomes a driver to activate R. In classical conditioning of a GRN, we pair the NS with the UCS and apply both repeatedly so that the system can learn the association between the two stimuli and functionally link the NS with the state of R. Later, we test to see if R is now driven by the NS alone (if true, NS can now be called a conditioned stimulus [CS]). The taxonomy of possible memory types in such systems, and their relationships, are schematized in Figure 2, including UCS-based memory (UM, long-term response induction by a specific stimulus), pairing memory (PM, one-shot stabilization of response to compound cues), transfer memory (TM, resembling discrimination training that results in a more generalized response), associative memory (AM, including two of its sub-categories: long recall associative memory [LRAM] and short recall associative memory [SRAM]), and consolidation memory (CM).

We tested (using the algorithm shown in Figure S2) many models of a diverse range of biological systems (networks with fewer than 25 nodes) obtained from the publicly available dataset Cell Collective (Helikar et al., 2012) for each of the kinds of memory using the criteria in Figures S1 and 2. A key aspect of our algorithm is that for any given GRN to be analyzed, the algorithm tests all the combinations of different nodes for their ability to serve as NS, CS, UCS, and R in the various assays. Thus, for any given network (or set of networks), one can compute the prevalence of memories—what percentage of all possible combinations of choices of nodes as NS, UCS, and R give rise to different kinds of memories. Figure 3 describes the structure function of a Boolean GRN, the network simulation for UCS-based memory evaluation, and overall memory estimation of a GRN with an example of a small GRN named Cortical Area Development GRN.

[PAGE 7] The raw data in each of the training and testing phases for AM are shown in Figure 4 (representative data for the other memory types are shown in Figures S3-S6), using the Mammalian Cell Cycle 2006 network and suitable choices of nodes for NS, CS, UCS, and R as an example (Figure 4A). The Boolean equations corresponding to this GRN and each of other GRNs tested in this article (as given in the GRN repository) can be found in Data S1. The pre-requisite conditions (Figure 4B), for an appropriate choice of nodes to serve as UCS, NS, and R (see Figure 3B) out of all possible choices of nodes for these roles, are that UCS alone should be sufficient to trigger R and that NS alone does not trigger R before training and memory evaluation. The training phase (Figure 4C) shows pairing of UCS and NS activations. After successful training, the NS alone becomes sufficient to trigger R (Figure 4D); it is seen that the NS has in fact become a conditioned stimulus because when it stops, the response stops, and when it is presented again, the response begins again (Figure 4E). This fulfills the basic criteria of Pavlovian conditioning (Figure 1) and shows that the functional roles of the input nodes with respect to GRN behavior have been stably altered by experience of stimuli, the pairing of two node activations during training (Figure 4C). It should be noted that in identifying specific nodes as effective CS, UCS, and R nodes for a given instance of memory, it is not the case that the memory somehow resides in those three nodes: memories are a function of the entire network, distributed therein and revealed as experience-dependent changes of network-wide activity by stimulation and readout at specific nodes chosen as inputs and outputs.

[PAGE 5] Figure 2. Definition of different memory types

UCS and NS input stimuli are schematized as switches, whereas response R is schematized as ON/OFF electric bulb. To represent the ON, OFF, Repeated, and Relaxed states of UCS and NS, blue and green switches with different positions are used, respectively. We define the pre-requisites for memory testing in the block headed as Before training. It requires that initially (at time t_0) UCS, NS, and R should be OFF, and such that UCS triggers R and NS does not trigger R. The memory evaluation table describes each memory type as a row. The five broad memory types are described here as UCS Based, Pairing, Transfer, Associative, and Consolidation memory. For each memory evaluation, there are training and testing phases, showing the overall dynamic and what is learned (the stable change in system behavior) over time.


[PAGE 5] Figure 4. Time series data of a GRN's evaluation for associative memory

This trace describes the run time state changes in evaluating associative memory of a mammalian cell cycle network.
(A) In the mammalian cell cycle network 2006, the genes used as UCS, NS/CS, and R are highlighted with blue, red, and
cyan respectively. With these respective colors the states of UCS, NS/CS, and R in different plots are defined. A downward
arrow in each plot shows the start of the activation of the corresponding stimuli. In each panel, we show the 10 past states
of a stimulus to depict its state change upon the activation at time 0.
(B) The resultant states of R, observed from activation of UCS and NS, respectively, before training: R gets activated with
onset of UCS but NS cannot trigger R.
(C) Pairing (training) experiment shows the successful activation of R.
(D) After training, activation of the previously neutral stimulus causes R to be activated, confirming that the experience of
paired stimuli has converted the NS node to a CS.
(E) As further confirmation of stable causality established between CS and R by training, we first deactivate CS, to see if R
gets deactivated, and then reactivate the CS to ensure that it can activate R again.


[PAGE 7] We sought to discover minimal networks showing each kind of memory, to serve as prototypical examples clarifying the logic of each type of memory, and to guide the design of novel GRNs for synthetic biology applications that could exploit transcriptional dynamics for memory functions. At minimum a network needs two nodes (UCS and R) to form UM and three nodes (UCS, R, and NS) to form any other type of memories. To test the topographies and motifs associated with each type of memory, we created 10,000 random Boolean networks (RBNs) for each case and evaluated each memory using our toolkit (see Transparent methods). The smallest networks discovered to be sufficient for each type of memory are shown in Figure 5. We conclude that even fairly simple networks, readily accessible to synthetic biology construction, can give rise to memory functionality.

[PAGE 7] Biological GRNs possess various memory types: an analysis across taxa

We tested 35 biological GRNs (those <25 nodes in size, from Cell Collective (Helikar et al., 2012); Data S1 provides Boolean equations for each GRN obtained from the same website) for each kind of memory (Figure 6A). These included GRNs at different strata of the tree of life (prokaryotes and eukaryotes), cancer, diverse metabolic processes in adult and embryonic stages of mammals, cellular signaling pathways in invertebrate and plants, etc. For each network, the prevalence of each type of memory was analyzed by assessing the number of different combinations of nodes that can serve as UCS-R-NS.

Three of 35 (8.57%) GRNs exhibited no feasible stimuli-response (UCS-R-NS) combinations exhibiting memory. For those GRNs with memories (32 out of 35), UM was the most prevalent type of memory, followed by TM. AM and PM memory types were somewhat rarer (only 5 of 35 GRNs). AM appeared in "Aurora Kinase A in Neuroblastoma," "B cell differentiation," "CD4+ T Cell Differentiation and Plasticity", "Cell Cycle Transcription by Coupled CDK and Network Oscillators," "Mammalian Cell Cycle 2006," and "Neurotransmitter Signaling Pathway" GRNs, among which the first and the third GRNs (highlighting one combination of stimuli-response for each) are shown in Figures 6B and 6C respectively. In each of the first three and the last GRNS, AM and PM occurred together. For each GRN, the percentage of combinations where a certain memory appeared out of all feasible combinations is listed in Table S2.

We then asked whether there is any grouping of the different GRNs that reveals a pattern—is predictive for the presence of memory capacity. Although it is difficult to define categories that objectively and unambiguously sort the available GRNs into sharp classes, we considered two simple, rough categorizations of GRNs: one based on whether they belong to vertebrates or invertebrates and the other based on whether they belong to generic or unicellular cell activities versus specific cell types in metazoan bodies. We found that both the vertebrate/invertebrate and the cell specificity distinctions are excellent predictors (with an accuracy of 74% and 86%, respectively) of the existence of memory, as evidenced by their performance as classifiers of prevalence of memory (Figure 7). Thus, we conclude that a diverse set of biological GRN structures exhibit various types of memory, which are especially highly represented within differentiated cells of vertebrate organisms.

[PAGE 9] Memory types and their relative prevalence among possible GRNs

Do larger networks in general have more memory capacity than smaller ones? To better understand the properties that underlie memory in networks, we generated RBNs to test different aspects of network structure. To determine how memory in RBNs changes with increasing network size, we created RBNs ranging in size from 5 to 25 nodes, with 100 RBNs generated for each size range (see Table S7). We evaluated the pool of RBNs of each size separately to observe the change of average memory distribution with the increase in size. We found that memory is less common in smaller RBNs (under 15 nodes in size, Figures 8A and 8D) and restricted to UM- and TM-type memory. Other types of memory start appearing in RBNs with 15 or more nodes. Although UM dominates, all memory types were observed (Figures 8B-8D), with increasing amounts of the non-UM memory types at network sizes of 20 and 25 (Figures 8C and 8D). Interestingly, in 15 and 20 node networks, LRAM is more common than SRAM, but in 25 node RBNs, SRAM dominates (Figures 8B-8D). We then asked whether the same relationship between network size and likelihood of memory holds in biological networks. We grouped the 35 biological GRNs into 5 categories with network size 5-9 (2 GRNs), 10-14 (6 GRNs), 15-19 (14 GRNs), 20-24 (10 GRNs), and 25-25 (3 GRNs). We evaluated memory and presented average memory distribution in the same manner as RBNs. We observed that GRNs have large amount of memories across networks, but, like RBNs, the percentage of networks with memory increases with network size. Availability and proportion of different types of memories in GRNs (Figures 8E-8H) are not entirely size dependent, although this relationship will become better quantified for biological networks when larger numbers of GRNs become available at different size ranges.

We next asked whether the likelihood of finding memories in networks of different sizes was similar in biological GRNs as in randomly constructed networks: is there anything special about how memories are distributed in biological networks of various sizes? For each type of memory in a GRN, we observed how its prevalence fits into the probability distribution of the corresponding values of 100 RBNs of similar size. To determine whether the size/memory relationship in biological GRNs is in any way unique (distinct from that observed in random networks), we calculated p values (Table S3) and performed an outlier test (Table S4) comparing the distributions in Figure 8. The null hypothesis is that the distributions of GRN memories across size categories are not different from the memories of similar-sized RBNs. If the outlier test is passed for a given memory type, this would indicate rejection of null hypothesis: GRN memory distributions are different from those found in similar RBNs. In each analysis, we obtained a matrix (35 GRNs each having 8 types of memory, including no memory). In the first case, each element of the matrix is a p value [0, 1]. We considered significance when p < 0.05. In the second case, the value is binary (1 if the value is an outlier in the distribution of similar-sized 100 RBNs; 0 otherwise). Using either test, the percentage of significant deviations from random distributions was relatively higher for UM and TM when compared with other memory types.

To confirm the differences between the class of biological GRNs and random counterparts, we also conducted Fisher's exact test to determine whether GRNs and RBNs are statistically different. For 3 categories of GRNs we tested network sizes of 5-9 (small), 15–19 (intermediate), and 25–25 (large). Using contingency tables of memory versus no memory in GRN and similar-sized RBNs, for all the 3 cases, the null hypothesis that occurrences of memory in GRNs and RBNs are not different was rejected with p values 2.0E-05, 7.4E-323, and 4.4E-323, respectively. Taken together, our statistical analyses show that biological GRNs have unique distributions of memory types with respect to network size.

[PAGE 12] The memory profile of biological GRNs is unique

Do real biological networks' topologies offer more opportunities for memory dynamics than would be expected by chance in arbitrary networks of similar size and type? We generated 3500 "configuration models"—100 randomized versions for each biological GRN—and analyzed them for the presence and prevalence of each memory type. We then used statistical tests to compare these aggregate statistics to the memory profiles of the 35 actual biological networks, to determine whether GRNs of biological origin are in any way special with respect to memory capacity over what is provided by the generic properties of BNs. We note that comparisons across different types of GRNs are limited by the set of available specific GRNs; thus, future analyses of a broader set of GRNs emerging from this field are likely to refine and expand our results.

Given a certain type of memory in a GRN, we checked to see how the value fits into the probability distribution of the corresponding values of its ensemble. We calculated p values (Table S5) and conducted an outlier test (Table S6). In each type of analysis, we obtained a matrix (35 GRNs each having 8 types of memory, including no memory). In the p value test each matrix element was a p value [0, 1]. We considered significance when p<0.05. In the outlier test, the value was binary (1: if the value is an outlier in its random ensemble; 0: otherwise). In either test, the percentage of success was relatively high for UM and TM compared with others.

Furthermore, we examined how each type of memory in a GRN fits into its random ensemble, visualizing the distribution of memories via violin plots (Hoffmann, 2015). We found (Figure 9A) that the incidence of memory-containing biological GRNs is generally unique with respect to possible GRNs, lying outside the [5 95] percentile bars. Thus, we found that the data are not compatible with memory profiles in biological networks occurring solely as a consequence of the generic mathematical dynamics of BNs (Kauffman, 1993). The fact that distribution of memories across real biological networks differs from that of randomized networks suggests that biological evolution has given rise to GRNs with specific memory properties. Our data do not distinguish between direct selection for memory in GRNs and indirect selection in which

[PAGE 12] Figure 7. Distribution of different memory types across diverse biological systems
(A and B) The memory capacity of GRNs can be systematically classified according to their features. (A) A classification of GRNs based on whether they correspond to vertebrate or invertebrate species. This panel shows that vertebrate GRNs tend to contain more memory than the invertebrates, as quantified by the classification performance metrics: Accuracy = 0.74, Sensitivity = 0.88, Specificity = 0.63, Positive predictive value = 0.67, Negative predictive value = 0.86, and AUC = 0.75. Red borders indicate data from invertebrate GRNs, whereas green borders indicate data from vertebrate GRNs. (B) A classification of GRNs based on whether they derived from a unicellular or generic process or from a specific somatic cell type. This panel shows that the GRNs corresponding to the non-generic cell types tend to contain more memory than the generic ones, as quantified by the classification performance metrics: Accuracy = 0.86, Sensitivity = 0.88, Specificity = 0.84, Positive predictive value = 0.82, Negative predictive value = 0.89, and AUC = 0.86. Classification was performed as follows. First, the memory capacity of each GRN was computed as the proportion of memory within the total that included the "no-memory" type. Then, if the memory capacity of a GRN exceeded 50% it was categorized under the "memory" class or in the "no memory" class otherwise. The standard binary classification metrics reported above were computed based on the associated confusion matrix containing the number of True-Positives (TP), False-Positives (FP), True-Negatives (TN), and False-Negatives (FN) where the "memory" class is the "positive" class and the "no-memory" class is the "negative" class. As per standard definitions, Accuracy is the proportion of TP and TN among the total number of instances, Sensitivity is the proportion of TP among the actual positive instances, Specificity is the proportion of TN among the actual negative instances, Positive predictive value is the proportion of TP among the predicted positive instances, Negative predictive value is the proportion of TN among the predicted negative instances, and AUC is the area under the receiver operating characteristic curve, which can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.


[PAGE 13] memory is favored because it enables some other feature with selective advantage (e.g., plasticity of physiological response).

Memories in biological GRNs do not occur independently
As different kinds of memories have not before been rigorously defined for GRNs, or examined across the broad range of possible networks, it was not known whether memories tend to occur in the space of GRN topologies independently or whether certain GRN structures simultaneously predispose the network to multiple types of memories (perhaps distributed across different sets of CS/UCS nodes). Thus, we next sought to characterize possible relationships between the incidence of distinct memories in a wide range of possible networks. Having generated a large number of configuration models, we asked whether the presence of one type of memory is statistically related to the likelihood of finding any other memories.

We found that conditional entropy (quantifying ordered correlation) between two types of memories in biological GRNs (Figure 9B) is much higher than that of their randomized configuration models (Figure 9C). Correlation between AM (especially LRAM) and any other memory type (not including SRAM and CM) is especially significant. Biological GRNs show tight correlations between UM and TM. Moreover, in biological GRNs, PM predicts the existence of both UM and TM, but the correlation does not hold for the reverse direction, whereas CM implies you will find UM. In the case of configuration models, the sub-categories of AM (LRAM and SRAM) showed correlation to AM. We conclude that the potentials for forming different kinds of memories are not independent, that specific GRN architectures tend to simultaneously support more than one kind of memory, and that the existence of some types of memory can be predicted solely based on the finding of other types.


[PAGE 14] DISCUSSION
Numerous problems in biomedicine and fundamental life sciences face the inverse problem that affects all complex emergent systems: how do we control system-level behaviors by manipulating individual components? This problem is as salient for bioengineers and clinicians seeking to regulate gene expression cascades as for evolutionary developmental biologists seeking to understand how living systems efficiently regulate themselves (Crommelinck et al., 2006; Karsenti, 2008). An important direction in this field is the discovery of strategies that exploit patterns of input (experiences), rather than hardware rewiring, to achieve desired changes in network behavior or explain the modification of pathway properties faster than occurs during evolution. This strategy requires the development of algorithms to identify specific patterns of stimuli that exert stable, long-term changes in behavior, thus characterizing endogenous memory properties of the system.

We wondered if it were possible to train gene regulatory networks, providing targeted patterns of stimuli to stably change their behavior at the dynamical system level, rather than rewiring network topology at the genetic or chromatin epigenetic levels. This would take advantage of existing computational capabilities of the system and effectively offload much of the computational complexity inherent in trying to manage

[PAGE 15] GRN function from the bottom up. Such approaches (Pezzulo and Levin, 2016), if the GRN structures were amenable to them, would enable the experimenter