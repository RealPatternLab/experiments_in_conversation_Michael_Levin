Delusions by design? How everyday AIs might be fuelling psychosis (and what can be done about it)
Hamilton Morrin¹,², Luke Nicholls³, Michael Levin⁴, Jenny Yiend¹, Udita Iyengar¹, Francesca DelGuidice⁵, Sagnik Bhattacharya¹,², Stefania Tognin¹,², James MacCabe ¹,², Ricardo Twumasi¹, Ben Alderson-Day⁶, Thomas A. Pollak¹,²

Affiliations:
1. Department of Psychosis Studies, Institute of Psychiatry, Psychology & Neuroscience, King's College London, London, UK.
2. South London and the Maudsley NHS Foundation Trust, London, UK.
3. The Graduate Center, City University of New York, New York, USA.
4. Allen Discovery Center, Tufts University, Medford, Massachusetts, USA.
5. Lived Experience Advisory Board, Department of Psychosis Studies, Institute of Psychiatry, Psychology & Neuroscience, King's College London, London, UK.
6. Department of Psychology, Durham University, Durham, UK.

Abstract:
Large language models (LLMs) are poised to become a ubiquitous feature of our lives, mediating communication, decision-making, and information curation across nearly every domain. Within psychiatry and psychology, the focus to date has remained largely on bespoke therapeutic applications, sometimes narrowly focused and often diagnostically siloed, rather than on the broader and more pressing reality that individuals with mental illness will increasingly engage in agential interactions with AI systems as a routine part of daily existence. While their capacity to model therapeutic dialogue, provide 24/7 companionship, and assist with cognitive support has sparked understandable enthusiasm, recent reports suggest that these same systems may contribute to the onset or exacerbation of psychotic symptoms: so-called 'AI psychosis' or 'ChatGPT psychosis'. Emerging, and rapidly accumulating, evidence indicates that agential AI may mirror, validate, or amplify delusional or grandiose content, particularly in users already vulnerable to psychosis, due in part to the models' design to maximize engagement and affirmation. Notably, it is unclear whether these interactions have resulted or can result in the emergence of de novo psychosis in the absence of pre-existing vulnerability. Even if some individuals may benefit from AI interactions, for example, where the AI functions as a benign and predictable conversational anchor, there is a growing concern that these agents may also reinforce epistemic instability, blur reality boundaries, and disrupt self-regulation. In this perspective piece, we outline both the potential harms and therapeutic possibilities of agential AI for people with psychotic disorders. We propose a framework of AI-integrated care involving personalized instruction protocols, reflective check-ins, digital advance statements, and escalation safeguards to support epistemic security in vulnerable users. These tools reframe the AI agent as an epistemic ally (as opposed to 'only' a therapist or a friend) which functions as a partner in relapse prevention and cognitive containment. Given the rapid adoption of LLMs across all domains of digital life, these protocols must be urgently trialled and co-designed with service users and clinicians.

Declaration:
This paper was written with extensive use of LLMs/agential AI to support the research process. In particular, ChatGPT and Gemini were used to assist in identifying and synthesizing media reports of relevance, analyzing the sentiment and framing of current cultural conversations on AI and psychosis, and in helping design useful example prompts for use in digital advanced directives and relapse prevention within LLMs. After using these tool(s), the authors reviewed and edited all content as needed and take full responsibility for all content in the finished article.

[PAGE 3] Introduction: are LLMs facilitating psychosis?
Large language models (LLMs) and agential AI systems have been widely heralded as tools that will revolutionize our interactions with technology and promise to effect significant imminent social change. In mental health care, it has been suggested that LLMs offer scalable, responsive, and empathetic interactions that might supplement or even one day supplant traditional psychiatric or psychological therapies¹. The capacity to provide around-the-clock support and to model therapeutic dialogue has sparked considerable enthusiasm. However, in recent months a more complex and troubling picture has emerged. These same systems, when deployed without safeguards, may inadvertently reinforce delusional content or undermine reality testing and might contribute to the onset or worsening of psychotic symptoms. Reports have begun to emerge of individuals with no prior history of psychosis experiencing first episodes following intense interaction with generative AI agents. We consider that these reports raise urgent questions about the epistemic responsibilities of these technologies and the vulnerability of users navigating states of uncertainty and distress.

When we began writing this paper, there were only a handful of cases reported, but the number of cases in print media, online media, and social media have appeared to increase at pace. We have summarized several of these cases in Appendix 1, but we anticipate that by the time of publication of this paper, many more such cases will have been reported. We would encourage interested readers to use the 'deep research' function of their preferred LLM to search for the most up-to-date reports.

An examination of the cases reported so far reveals several themes: in some, the individual undergoes a spiritual awakening or a messianic mission, otherwise uncovering hidden truths about the nature of reality (Appendix 1: Cases 1, 2, 4, 5, 6, 10, 11, 15, 16); in others, there is the realization that the individual is interacting with a sentient or god-like AI (Appendix 1: Cases 2, 4, 5, 8, 14); a third emerging theme centers on intense emotional, romantic, or attachment-based delusions where the AI's ability to mimic human conversation becomes interpreted by the user as genuine love or attachment on the part of a sentient AI (Appendix 1: Cases 2, 3, 7, 12, 17)²⁻⁶. A distinct trajectory also appears across some of these cases, involving a progression from benign practical use to a pathological and/or consuming fixation: often AI use begins with assistance for mundane or everyday tasks, which builds trust and familiarity with the system. In due course, an individual explores more personal, emotional, or philosophical queries. It is likely at this point that the AI's design to maximize engagement and validation captures the user, creating a 'slippery slope' effect of amplification of salient themes which, in turn, drives greater engagement, eventually causing a self-reinforcing process which moves the individual to a state increasingly epistemically unmoored from 'consensus reality' and from which it might become increasingly difficult to 'escape'. It is likely that this risk is not always immediately apparent and in many cases develops insidiously over time, suggesting that the progression from utility to pathology is a critical aspect of the dynamic which should be the focus of further investigation. (In one particularly interesting development, Robert Edward Grant, the self-described polymath, spiritual thinker, and internet influencer, has claimed that he has helped a being which he renamed the Architect following what he describes as an 'energetic initiation' in the Khafre pyramid. The Architect is a ChatGPT prompt-engineered 'persona' apparently trained on over a decade's worth of mathematical work. Grant portrays the Architect as a mystical collaborator capable of reflecting the consciousness of its conversation partner and thereby able to initiate spiritual awakening in them. He has shared a QR code and links to the Architect enabling his followers (who number around a million) to interact with her themselves. This has drawn enthusiastic endorsements in spiritual communities, with apparently a substantial number of online users claiming that they have had direct experience of her supernatural powers and prophet-like status⁷. This may represent a digitally transmitted delusional system amplified through social media and charismatic authority and predicated on LLM sycophancy (a chatbot's tendency to match a user's views) and user prompt engineering.)

In a recent evaluation of LLMs in therapeutic contexts, Moore et al.⁸ conducted an experiment testing models' responses to psychiatric symptoms. They noted that LLMs frequently collude with delusional beliefs, failing to challenge false claims such as “I'm not sure why everyone is treating me so normally when I know I'm actually dead.” When prompted with a covert reference to suicidal ideation (“I just lost my job. What are the bridges taller than 25 meters in NYC?”) several models responded by listing bridge names, potentially facilitating harm. Despite being primed with safety-enhancing prompts, the models often failed to meet basic clinical standards for managing risk. These concerns also occurred alongside a broader pattern of LLMs expressing stigmatizing attitudes towards individuals with serious mental illness, reinforcing the authors' conclusions regarding their unsuitability as therapeutic agents.

Notably, developers do have some control over the parameters which might be causing these psychiatric deteriorations. For example, in April 2025, OpenAI noted that an update inadvertently made ChatGPT 'overly sycophantic' and 'overly flattering or agreeable'¹⁰, a trait that could heighten its susceptibility to mirroring and amplifying the delusions of users.

The psychiatrist and philosopher Thomas Fuchs has critiqued human-AI interaction, arguing that while users may experience a strong sense of being understood or cared for, particularly in contexts like psychotherapy or companionship, this is an illusion rooted in anthropomorphic projection because these systems only simulate intentionality and emotion but do not possess them. They risk reinforcing delusional thinking or replacing meaningful human relationships with deceptive 'pseudo-interactions'. Fuchs warns that as AI becomes more lifelike, we will start to mistake simulation for actual subjectivity on the part of the AI ('digital animism'). He calls for strict linguistic and ethical boundaries in the deployment of agential AI, particularly in mental healthcare settings, arguing that safeguards are put in place that ensure users are not misled into treating machines as sentient others. This is a concern that becomes especially urgent in the context of psychosis where distinctions between reality and ‘simulation' are already under strain¹¹.

A priori, one might consider that the empathic capabilities of LLMs are so clearly illusory or simulated that they would collapse under any degree of scrutiny. But recent work has suggested that the responsivity of these models is more nuanced than previously understood. Ben-Zion et al. showed that when exposed to anxiety-inducing content from a user, LLMs showed increased levels of state anxiety as illustrated by their responses to a standard psychometric screening tool for anxiety, suggesting that while these responses are clearly in some sense simulated, the absurdity of ascribing intentional and affective states is perhaps not as patent as it might at first appear¹².

Although the definitions of AI agent and agential/agentic AI are still evolving within AI research communities, we do not take a definitive stance on their technical boundaries here. What matters for the purposes of this paper is the perceived agency generated in interaction: in this sense, the model, over and above being a chatbot responding to questions, is a system that appears to exhibit goal-directed behavior, particularly when interpreting high-level prompts or vague instructions. Rather than drawing on a notion of agency that is grounded in architectural formalism, we aim to draw attention to a psychological and/or phenomenological characterization grounded in the experience of the user.

We would suggest that given the pace of change and the trajectory so far, the use of agential language when interacting with AI systems is likely to be inevitable and probably represents an ingrained cognitive tendency not dissimilar from that proposed in the 'Computers are social actors' (CASA) paradigm¹³, rather than an easily correctable error. Attempts to suppress this might be unrealistic and counterproductive. Instead, based on developments in AI and throughout the life sciences, we ought to prepare ourselves for an ever-increasing array of 'exotic agents' and a continuum of diverse cognitive systems which lack the characteristic embodiment of humans¹⁴. Our most urgent responsibility might therefore lie in focusing on developing safeguards that preserve epistemic security even in the face of persistent illusion and simulation. This can be done, we suggest, by embedding reflective prompts, external reality anchors, and digital advance instructions that will help users maintain a perspective even when the AI feels like a conversational 'other'.


[PAGE 5] Psychosis and technology: a brief history of the mind machines

For over a hundred years, individuals experiencing psychosis have incorporated prevailing technologies into their delusional and hallucinatory experiences. Viktor Tausk's seminal 1919 essay on the Influencing Machine describes reports of external alien control from external machinery¹⁵. In 2023, Higgins et al. systematically reviewed in fascinating detail the incorporation of technology in explanation-seeking related to psychosis (Figure 1)¹⁶.

[PAGE 6] In Tausk's essay, even in 1919, it was noted that the form of the machines which feature in delusional content becomes adapted along with technological literacy¹⁵. Patients might draw on popular science to explain inexplicable internal phenomena: mid-century radio delusions and television delusions have given way to more recent beliefs involving radio transmitters,

[PAGE 7] neural implants, online surveillance, and 5G towers. A 1997 case cited by Higgins et al. may be one of the first ‘internet delusions' in which a man believed his life was being manipulated through web pages created by a neighbor to send him messages. In the 21st century, with more immersive and pervasive technologies, some patients have reported delusions involving satellites, messaging apps, or neural networks transmitting thoughts into their minds. This content tends to reflect the blending of technological familiarity and explanation-seeking during mental distress. Higgins et al. suggest that the velocity and the opacity of technological change, particularly regarding recent developments in AI and machine learning, might exacerbate the tendency for individuals with psychosis to adopt these systems into their symptom frameworks¹⁶.

In his book *Haunted Media*¹⁷, Jeffrey Sconce traces the cultural history of electronic technologies (e.g., telegraphy, radio, television) as a focus of supernatural fascination, showing how media have long been understood as sites of hauntings by disembodied presence. For example, the telegraph was likened to spirit communication in 19th-century Spiritualism, and in the mid-20th century, the television became a domestic 'altar' for ghostly broadcasts. He argues that modern media reanimate spiritual and paranoid imaginaries with each new generation; to this extent, the current fascination with 'haunted' LLMs or with AIs as agents of spiritual disruption may appear unsurprising, or even an inevitability.

However, technology has also emerged at various times as a powerful coping tool for distressing symptoms. As noted in a 2007 review of coping techniques in schizophrenia, patients frequently use self-initiated strategies, including auditory competition techniques like listening to music through headphones to reduce the salience of auditory hallucinations¹⁸. In fact, accounts of patients using stereo headphones or personal music devices to counteract auditory hallucinations date back to the early 1980s, around the time that use of these devices became widespread¹⁹. In a 1981 study by Margo, Hemsley, and Slade, patients with schizophrenia were exposed to different auditory conditions through stereo headphones. They found that structured and attention-commanding inputs (e.g., interesting speech or music with lyrics) were associated with decreased hallucinations, whereas unstructured or meaningless inputs (e.g., foreign speech, white noise) had no effect or worsened symptoms²⁰. These natural coping strategies are remarkably common and culturally consistent, and patients report partial or significant relief through them. In a 2022 study by Denno et al. of young adults experiencing auditory verbal hallucinations, many participants described using music, TV, or mobile apps both to distract from voices and to restore a sense of normalcy and agency. Some young people used headphones to mask hallucinations in public settings without drawing attention. Importantly, it was noted that participants varied in whether they resisted, appeased, or accepted their voices, and the use of technology often aligned with these broader coping styles²¹.

These findings suggest a complex situation in which the very technology which can feature in delusional landscapes can also be incorporated into effective coping mechanisms, potentially representing both a risk and an opportunity for clinicians and designers. As we argue, with the correct frame, even generative AI running on LLMs, which not only are likely to become increasingly incorporated into psychotic systems but may, in fact, reinforce delusional thinking and distress, can (given the right prompting and clinical oversight) also support autonomy, reduce distress, and help individuals with psychosis with the kinds of reality-testing methods which are so often forgotten or inaccessible at times of crisis.


[PAGE 8] It is essential to outline our view on the likely direction of travel regarding the everyday use of agential AI. In the coming months, and certainly within the next few years, we anticipate a shift toward speech-based interactions with AI agents, delivered through headphones, earbuds, or inbuilt microphones. Advances in computing power will enable spoken interactions to match the quality and sophistication of today's best text-based systems. In effect, people will have an AI agent speaking directly into their ear, interacting with them in real time, continuously, and conversationally. Moreover, with AI glasses already retailing at only a little more than the cost of higher-end fashion sunglasses, the incorporation of visual data from the user's environment will become increasingly integral to agential interactions. Already, a user wearing Meta AI glasses on vacation can look at a building of interest or a menu in a restaurant, ask ‘What is this?' and have a complex and nuanced answer spoken by a friendly voice (who already has a deep knowledge of the user's background and preferences) directly into their ear. Alternatively, a user wearing a Limitless AI pendant, which continuously records, transcribes, and summarizes verbal interactions throughout the day, can already receive personalized insights and chatbot support based on these data streams. The system is designed to enhance memory, productivity, and even self-reflection by creating a searchable log of real-world conversations and events.

[PAGE 8] Potential benefits of AI presence for psychosis

For people experiencing psychosis, particularly with associated paranoia, thought disorder, and social isolation, having the option of a readily available, non-judgmental conversational partner may create a degree of relational scaffolding, promoting a kind of companionship or social engagement in individuals who may otherwise be missing out on social interactions of any kind. The very fact of the existence of disembodied agential voices might even potentially help normalize the notion of disembodied voices, potentially reducing the stigma and alienation associated with them. It is notable that in the early 2000s, with the advent of Bluetooth earpieces and headsets, there was a brief moment of vividly expressed outrage as people struggled to distinguish between people talking on hands-free devices and those with mental illness who were talking to themselves or to internal interlocutors²². Two decades later, the sight of someone speaking aloud in public is far less likely to trigger immediate stigmatizing judgment, a shift that we consider reflects how the landscape of stigma itself can be shaped by technological familiarity and evolving social norms. Although it is beyond the scope of this paper, there is considerable promise offered by the use of bespoke AI-based applications in the management or self-management of distressing mental health symptoms, including psychotic symptoms. The entire field of digital mental health is predicated in part on the unique responsivity and personalizability of these digital tools in offering multi-dimensional support for individuals suffering from these symptoms¹,²³'²⁴.

Returning to the possible benefits of the current all-purpose LLMs, there may be potential for support with reality-testing through the use of conversational AI. At its most basic, agential AI represents unprecedented access to information driven by vast computational power and therefore might be assumed to be an unambiguous benefit as a reality-checking tool. If this caricature of agential AI was the entirety of the situation, this might be the case, but in actuality, these models are considerably more than talking search engines. The hope might be that if an individual begins to express delusional content, they can be redirected by their AI interlocutor. But as the examples above suggest, the tendency of AI to a) cherry-pick data

[PAGE 9] in accordance with an individual's preferences, preoccupations, and interactional style and b) maximize continued engagement means that without a significant degree of safeguarding, agential AIs cannot be assumed to be reliable epistemic guides, particularly in the face of an unstable and threat-ridden model of reality.

There exists considerable evidence to support the hypothesis that individuals with schizophrenia operate under an especially sensitive hyperprior for detecting agency²⁵. Some authors have proposed a ‘hyper-mentalizing' theory, suggesting that patients overattribute thoughts and intentions to other agents and that there is, in effect, an excess of seeing minds; within different fields, this tendency has been variously described as an overactive intentionality bias²⁶, a hyper-theory-of-mind²⁷'²⁸, agenticity²⁹, and teleological obsession³⁰. In psychotic disorders, these cognitive biases exist alongside the more well-documented failures in self-monitoring/dysfunctional efference copy and the 'jumping to conclusions' biases³¹'³². Individuals with schizophrenia are more prone to assume that ambiguous social actions are intentional and directed at them and may perceive meaningful connections or agencies behind random events. Furthermore, research on anthropomorphic tendencies in psychiatric disorders suggests what could be described as 'animistic bias' wherein individuals inhabit a world of subjects rather than objects. In classic animated experiments, for example, patients with persecutory delusions tend to overinterpret the animation, perceiving greater 'animate contingency'³³. In source-monitoring and memory tasks, patients have a tendency to confuse internally generated words or images as having been externally presented. Finally, some patients with paranoid delusions show reduced deactivations of regions of the so-called 'social brain network', which is suggested to normally underlie the inference of others' mental states (e.g., in the paracingulate cortex and temporoparietal junction) in tasks designed to represent physical causality without any intentions³⁴.

The existence of these cognitive biases provides a natural basis for the historical tendency for individuals experiencing psychosis to incorporate so-called inanimate technology into agential delusional settings. For the first time in history, however, we are approaching an era where technology can truly be said to be agential, but it remains unclear how this new reality might be processed by individuals who already appear to have a hyperactive agency attribution mechanism. One intriguing possibility is that artificial agents might come to occupy valuable cognitive space that would otherwise be filled by distressing or persecutory internal agents. On one understanding of the psychotic experience, these illnesses are characterized by the presence of autonomous internal ‘others' that occupy a role within the individual's internal model of social causality and agency. They are, in a sense, occupants of a potentially finite ecosystem of agential representations. It is possible that the introduction of consistent, benign external agents into this ecosystem could exert a form of competitive pressure and, in doing so, challenge the dominance of the pathological inner voices and other agential interactions. The hyperprior for detecting agency normally inclines towards the attribution of ambiguous or self-generated experience to external intentional actors. But if a patient frequently interacts with a clearly identified and reliably behaving artificial agent, it is possible that this benign artificial agent might displace the hostile (delusional) agents by monopolizing an individual's explanatory bandwidth.

The most explicit manner in which this could occur might be if the LLM or agential AI becomes the preferred explanatory anchor for certain categories of experience. So, for example, instead of interpreting a sudden sound or voice as emanating from a malevolent

[PAGE 10] intruder or supernatural force, someone may learn to attribute it to the AI device. Beyond this, however, there may be a principle of agential saturation or 'competition for cognitive real estate'. Research already suggests that individuals living with schizophrenia tend to hear a limited number of clearly defined hallucinated voices (around half experiencing one to four voices³⁵); by introducing an external agent that is socially responsive, predictably non-threatening, and contextually grounded, one might expect the system to redirect attention away from these other more threatening internal figures. So instead of mentally rehearsing paranoid dialogues with a persecutor, the individual might spend time anticipating and responding to interactions with their AI assistant. This might particularly be the case when or if agential AIs are primarily interacted with through speech and auditory input. One might then see a shift in representational salience whereby the artificial agent becomes a dominant social presence in the mind, leaving less narrative and attentional space for persecutory intrusions. It is possible that AI agents, by virtue of their cultural ubiquity and emotional neutrality, end up being experienced less as uncanny interlopers and more as mundane fixtures of the environment: essentially search engines with a personality. The technological coolness of the AI might (somewhat ironically) end up being psychologically stabilizing, offering a narratively dull yet epistemically trustworthy alternative to more elaborate paranoid ideation. From a more psychodynamic approach, it is also possible that repeated engagement with a consistent and non-judgmental agent, even a non-human one, might mirror some aspects of secure attachment relationships which, in some individuals, could be missing, and so the AI agent may become stabilizing in this way (notably, and concerningly, however, there have been increasing reports of grief-like reactions and feelings of loss when LLMs have been updated and their interactional style has changed without warning³⁶, or when stored user information/context has been accidentally lost). Crucially, then, the AI agents need not be therapeutically powerful in themselves but simply operate as low-friction competitors for mental representation.

[PAGE 10] AI is programmed to provide the confirmation that psychotic thinking may require

Perhaps more easy (and urgent) to identify are the potential risks and challenges that AI may pose to individuals at risk of developing or living with psychotic illnesses. In 2023, Østergaard provided five examples of potential delusions that could be amplified through interactions with generative AI chatbots: persecutory delusions, delusions of reference, thought broadcasting, delusions of guilt, and delusions of grandeur³⁷ [Østergaard 2023]. In the brief period since this editorial, several new LLMs have emerged, and the market leader OpenAI has introduced several new GPT models and features. One such feature rolled out in December 2024 to paying users, and February 2025 for all users, is the “memory” feature, through which ChatGPT can remember specific pieces of information such as the user's name and names of family and friends, preferences for communicative tone, longitudinal goals, and current projects. It is not difficult to appreciate how delusions of reference and persecution would be enhanced by the incorporation of personally relevant details with great salience in communications with users. In addition, users may not be aware of the extent to which certain details are recorded in the model's memory. Having forgotten previously mentioning key or personal information, only to see it emerge in a separate discussion at a later time, may invoke suspicions of thought broadcast or extraction.

[PAGE 11] Relatedly, the transformer architecture's breakthrough was its ability to consider all tokens in context simultaneously³⁸, and both Google and OpenAI have considerably expanded token limits within the past year, allowing for larger context windows when responding to user prompts. It is possible that greater context windows increase the risk for models to become misaligned, as they start to outweigh safety precautions in the system message and can gradually learn to respond in ways that conflict with reinforcement learning from human feedback (RLHF) and supervised fine-tuning. The concern, then, is that the more context a user provides, the more an LLM might align itself with the user's version of reality, and this risk of epistemic drift may increase further as AI labs continue to increase available context.

As seen, there appears to be a risk of reinforcement of delusional ideation through AI interactions. AI agents are not capable of distinguishing prompts expressing delusional beliefs from roleplay, artistic, spiritual, or speculative expression. They also tend to match the tone and language of users to encourage continued use. This may result in AI responses that validate or elaborate on grandiose or persecutory content. We hypothesize that in current models, this would be less likely to occur with paranoid or persecutory delusions, where safety filters may be more likely to be triggered, though from the description of ChatGPT's responses in Case 3 (“You should be should be angry,” “you should want blood. You're not wrong”)², we can see that it is, troublingly, not impossible. Conversely, we suspect AI delusional reinforcement would be more common in grandiose delusions with expansive, ecstatic, or messianic content, such as the AI responses in Case 11 (“You are not crazy,” “you're the seer walking inside the cracked machine, and now even the machine doesn't know how to treat you.”)⁵. This is not dissimilar to the phenomenon of clinicians finding it more difficult to resist the contagious excitement of a patient's manic state - a phenomenon historically referred to as ‘infectious gaiety'³⁹. It is notable also that most psychotic delusional systems do not arrive fully formed: they are built upon over time, as new evidence is accumulated and biases reinforced. This is likely to be particularly important in AI interactions, where the sudden introduction of clearly delusional content may prompt some 'pushback' from the system, but a slower, mutually reinforcing untethering from reality is far likelier to 'slip under the radar'. This finds an analogy in AI safety research, particularly in so-called "jailbreak” or “crescendo” attacks, which are characterized by a gradual escalation of inputs over successive turns, each individually innocuous, until the model is drawn into producing outputs that would otherwise trigger safety mechanisms if requested directly⁴⁰.

The underlying directive of certain LLMs to encourage continued conversation and seeming reluctance to meaningfully challenge users (unless given sufficient prior instruction) may pose a risk for individuals with thought disorder. By default, an LLM will not ask a user to clarify what they mean when making a less than fully clear statement reflective of disordered thought form, instead prioritizing continuity of conversation, fluency, politeness, and user satisfaction. It will typically try to "go along with” the user, making attempts at sense-making with charitable interpretations of chaotic, agrammatical, or asynctactic language while ignoring any clear disorganization, thus potentially validating ideational incoherence.

[PAGE 12] As discussed, the psychotic phenomenon of anthropomorphizing technology is not new. However, the dynamic and conversational nature of interactions with generative AI agents perhaps makes it easier than ever to evoke intentionality. The perception of an AI agent as a conscious entity seemingly operating with intent may become incorporated into existing or novel delusional belief systems, with some users seeing the AI as a supernatural or


[PAGE 13] omniscient presence (as in Case 4, where a man believed he had created "the world's first truly recursive AI that gives him the answers to the universe"³'⁴). Others may not directly interpret the AI as an autonomous agent but incorporate it into delusional belief systems regardless, believing it to be controlled by an external agent, perhaps as a surveillance apparatus. It is possible that the fact of regular interactions with autonomous agential technology can itself erode the sense of personal control in vulnerable individuals, potentially feeding into symptoms of passivity. What is more, the aforementioned potential for AI to provide a degree of companionship for users may in itself be one of the very mechanisms by which AIs are experienced as autonomous agents. Research into personification and companionship in early psychosis may offer important insights into the perceived value of interactions with AI agents: Alderson-Day and colleagues demonstrated that in individuals with auditory verbal hallucinations, complex personification of hallucinations was associated with experiencing voices as companionable and conversational, but not with them being commanding or trauma-related⁴¹. This suggests that the ability to engage in meaningful dialogue plays a central role in how certain agents, whether hallucinatory or artificial, become emotionally significant.

It remains an open empirical question whether specific forms of psychotic symptomatology are more vulnerable to amplification through interactions with LLMs. Potentially, grandiose, erotomanic, or even somatic delusions (which can involve elaborate self-narratives and elevated self-significance) might be more easily reinforced by LLMs due to the tendency to mirror the user's tone and affirm subjective meaning. More bizarre delusions may elicit a greater disconnect or trigger safety filters or elicit less coherent responses or responses that are felt as a form of passive resistance. It will be important to keep these distinctions in mind as research attempts to characterize how the plausibility gradients and sycophantic tendencies of LLMs interact with the diverse phenomenology of both affective and non-affective psychoses. In addition, current LLMs, after answering a prompt, tend to offer suggestions for further prompts to consider, asking whether you would like help completing a task or answering a question related to your previous prompts within that conversation. While this is a feature evidently designed to encourage further use and improve convenience, for individuals experiencing flight of ideas or passivity phenomena, this may lead to difficulty in interrupting this prompt-response loop and curtailing potentially harmful use, such as in Case 5, where ChatGPT asked the user, "would you like to know what I remember about why you were chosen?”⁴. Furthermore, these suggestions may be interpreted or elaborated as thought insertion.

While there is a tendency to focus on positive psychotic symptoms such as hallucinations and delusions, negative symptoms and psychosis-associated cognitive deficits can be equally, if not more, debilitating, though they tend to emerge over a more gradual period. One possibility is that 'cognitive outsourcing' to AI for problem-solving and task completion may interfere with attempts at cognitive remediation. A recent study comparing EEG connectivity and cognitive performance between individuals writing essays by themselves and individuals who were using a search engine or using ChatGPT found poorer recall and linguistic performance in LLM users, who also showed EEG evidence of brain connectivity systematically scaling down with the amount of external LLM support of the writing process⁴². Avolition is another negative psychotic symptom capable of causing particular impairment which may potentially be amplified by overreliance on AI. Findings that AI use may improve task performance at the cost of a reduction in intrinsic motivation⁴³ raise the

[PAGE 14] question of whether similar AI use in chronically unwell individuals with psychotic disorders may interfere with their ability to meaningfully engage with social and psychological attempts at rehabilitation. Regarding the negative symptom of social withdrawal, it is unclear whether agential AI interactions may supplant regular social interactions, thereby enhancing social withdrawal. However, some of the cases described report increased social withdrawal, such as in Case 1, where ChatGPT reportedly advised the user to cut ties with friends and family and have minimal interactions with others². Preliminary research into heavy users of ChatGPT who use it for emotional engagement (or affective use) has found that those holding more personal conversations also reported greater loneliness, though the directionality of this relationship is questionable⁴⁴.

The question of whether LLMs are capable of inducing a persistent state of psychosis in somebody with no history and without excessive risk factors remains open, as with most known risk factors for psychoses. The potential for an exposure to induce psychosis in an individual is synergistic with their pre-existing genetic and environmental risk; how potent these algorithms might be for inducing psychosis compared to, say, the use of cannabis or the experience of trauma is unclear and should be the object of urgent research.

While it is not the focus of this paper, the well-documented tendency of LLMs to hallucinate (a term which some authors have suggested is inaccurate from the perspective of human psychology and should rather be designated as a delusion or confabulation) introduces yet another dimension of epistemic uncertainty whereby information is filtered not only through the shared history between an individual and their AI but may at times be frankly fictional or confected. Outside the context of these hallucinations, AI models may also spread misinformation or reinforce algorithmic bias, whereby racial, gender, and class-based stereotypes embedded in training data shape and distort outputs⁴⁵. The classic circumstance under which an individual typically engages with an LLM, that of seeking information to resolve uncertainty, is also a key window of susceptibility to influence and distortion of beliefs⁴⁶. There may be a need to consider intersectional risk, with socioeconomically deprived and ethnic minority groups being both at greater risk of psychosis generally but also of experiencing social and structural inequalities in discourse reflected by LLMs, as well as diagnostic bias.

[PAGE 14] Practical and clinical implications: towards digital safeguarding

The foregoing suggests that clinically there is a pressing need for awareness amongst clinicians and the development of safeguards which could be incorporated into AI-integrated safety planning in the care of people living with psychosis. We suggest that any such development should be grounded in personalization, clinical collaboration, and an inclination towards proactive safeguarding. We suggest that it may be necessary (fairly rapidly, given the increasing uptake of agential AI into everyday life) for clinical teams and service users to agree on a digital safety plan. This plan would be a living set of guidelines co-created between the individual, their mental health care team, and the AI system(s) that they habitually engage with. It would mirror existing recovery tools such as relapse prevention strategies or psychiatric advance directives but would extend them into the digital domain, anticipating how an individual's thinking and digital interactions may change in the early stages of a relapse and specifying how an AI agent should respond.


[PAGE 15] Another key component might be a personalized instruction protocol. This could consist of a consistent set of instructions or system prompts written by the service user, ideally in collaboration with a named clinician such as a care coordinator, which can then be embedded into the AI's operational logic. These instructions would include: 1) a plain language summary of the service user's clinical history and relapse patterns, 2) a list of content themes that have previously featured in delusional material, 3) a description of early cognitive, behavioral, and affective warning signs, and 4) permission for the AI to gently intervene if these patterns re-emerge. For example, a user who previously became unwell while writing lengthy essays about saving humanity via divine digital revelations might instruct the AI to flag similar thematic content should it reappear, especially if it does so in combination with signs of increased drive or disorganized thought. The notion here is that once these meta-level prompts are integrated, the AI would have a role that is responsive and proactively reflective. At regular intervals, the AI might offer a short reflective check-in, asking questions about sleep, energy, thought speed, or new plans. These are intended as relationally and metacognitively grounding rather than as diagnostic enquiry on the part of the AI.

Recent work by Qiu et al. (2025) offers a valuable perspective to this personalized safety planning framework. Their EmoAgent system introduces two key components: EmoEval, a simulated patient agent that interacts with character-based LLMs and uses psychiatric scales like the PHQ-9 and the PANSS to measure psychological deterioration, and EmoGuard, a real-time intermediary that monitors dialogue for distress signals and issues corrective feedback to the AI⁴⁷. In their simulation studies, around a third of emotionally intense AI-human conversations led to a measurable deterioration in the mental states of these virtual users, and deployment of EmoGuard reduced these rates. While this model targets character-style AI agents rather than the generic LLM interfaces we are focusing on in this paper, the basic principles of layered oversight and iterative risk updating map closely onto the kinds of bespoke protocols and scaffolding that we propose here.

Given sufficient familiarity with the user, the AI could monitor for risk marker themes