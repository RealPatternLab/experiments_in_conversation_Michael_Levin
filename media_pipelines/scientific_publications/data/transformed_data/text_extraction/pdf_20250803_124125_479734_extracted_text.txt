[PAGE 1] Metacognition as a Consequence of Competing Evolutionary Time Scales
Franz Kuchling, Chris Fields and Michael Levin

Abstract: Evolution is full of coevolving systems characterized by complex spatio-temporal interactions that lead to intertwined processes of adaptation. Yet, how adaptation across multiple levels of temporal scales and biological complexity is achieved remains unclear. Here, we formalize how evolutionary multi-scale processing underlying adaptation constitutes a form of metacognition flowing from definitions of metaprocessing in machine learning. We show (1) how the evolution of metacognitive systems can be expected when fitness landscapes vary on multiple time scales, and (2) how multiple time scales emerge during coevolutionary processes of sufficiently complex interactions. After defining a metaprocessor as a regulator with local memory, we prove that metacognition is more energetically efficient than purely object-level cognition when selection operates at multiple timescales in evolution. Furthermore, we show that existing modeling approaches to coadaptation and coevolution—here active inference networks, predator-prey interactions, coupled genetic algorithms, and generative adversarial networks-lead to multiple emergent timescales underlying forms of metacognition. Lastly, we show how coarse-grained structures emerge naturally in any resource-limited system, providing sufficient evidence for metacognitive systems to be a prevalent and vital component of (co-)evolution. Therefore, multi-scale processing is a necessary requirement for many evolutionary scenarios, leading to de facto metacognitive evolutionary outcomes.

Keywords: metacognition; metaprocessor; coevolution; coadaptation; temporal scales; active inference; predator-prey models; coupled genetic algorithms; generative adversarial networks

[PAGE 1/2] 1. Introduction
The idea of metacognition—“thinking about thinking”—is as old as introspective philosophy and is central to both cognitive neuroscience [1-6] and artificial intelligence [7-11]. Metacognition is sometimes defined to include exclusively human capabilities, e.g., theoretical self-knowledge [12]. In its simplest form, however, any regulator that is an integral part of some larger system can be viewed as a “metacognitive” model of the lower-level system components that it regulates [13]; systems that include such internal regulators display self-monitoring and self-regulation, the most basic attributes of metacognition noted in [12].

In humans, metacognition is broadly associated with executive control [14,15] and "deliberate" Process-1 problem solving [16-18], though whether there is a clean, architectural difference between this and “automated” Process-2 problem solving remains subject to considerable debate [19-21]. It is clear, in particular, that metacognitive processes are not always consciously introspectable. How such “high-level" metacognition relates to more prosaic forms of regulation, either in humans or across phylogeny, remains poorly understood and indeed relatively uninvestigated.

Here, we adopt a broad notion of metacognition, employing this term to indicate the function of regulating some “lower-level” computational process. An architectural component that implements metacognition will be referred to as a metaprocessor for the component being regulated. This purely functional definition of metacognition explicitly avoids the seemingly intractable philosophical problem of determining whether a given system is "cognitive"; our approach to this latter problem is detailed elsewhere [22]. It also explicitly depends on how the larger system of which the metaprocessor is a component is identified. In particular, this larger system must be identified as comprising both the metaprocessor and the system that it regulates. A thermostat, for example, can be considered a metaprocessor for the on/off switch of a furnace, provided that the thermostat, the switch, and the rest of furnace are considered to compose a single system, a "regulated furnace"; if the thermostat is considered independently of this larger system, it is simply a thermally regulated switch. The question of whether something is a metaprocessor is, therefore, not an ontological question, but rather an architectural question that depends on how the larger system that contains it is functionally decomposed.

[PAGE 2] With this understanding of metacognition as a function and metaprocessors as components, we address the question of why natural selection has produced systems capable of metacognition, i.e., systems regulated by metaprocessors. We suggest that metacognition can be expected whenever (evolutionary) fitness functions—as we will discuss in the context of active inference in Section 2.1 and in genetic algorithms in Section 3.2.3—vary on multiple timescales. As we will see, such fitness functions are crucially multi-scaled. Metacognition, in this view, provides a context-dependent switch between depth-first and breadth-first searches that allows the avoidance of local minima. As such, it can be expected to be both ancient and ubiquitous. We can, indeed, identify slowly acting regulators of faster control systems even in prokaryotes; e.g., acetylation that changes the sensitivity of the chemotaxis “switch” molecule CheY in E. coli [23]. This example reminds us that biological systems implement memories at multiple scales using various molecular, cellular, and bioelectric substrates [24].

In what follows, we first briefly review comparative studies that take human metacognition as a starting point in Section 2.1. We outline the computational resources required to implement metacognition in arbitrary systems, focusing on requirements for memory and an ability to measure duration, in Section 2.2. We review the conditions under which interactions between physical systems are mediated by Markov blankets (MBs) [25,26] in Section 2.3, and then provide a general overview of the framework of active inference to minimize variational free energy (VFE) as it applies to living systems from the cellular scale upwards [27-31] in Section 2.4. As this framework is provably general for systems that maintain states conditionally independent from those of their environments (or in quantum-theoretic terms: separable states) over macroscopic times [32,33], it provides a general formalism for discussing the evolution of computational architectures enabling metaprocessing. We turn in Section 3 to our main hypothesis: that metaprocessing, and hence metacognition as a function, can be expected to arise in any systems faced with selective pressures—effectively, sources of VFE—that vary on multiple timescales. We begin in Section 3.1 by proving, under generic assumptions, that metacognition is more energetically efficient, and hence confers higher fitness, than purely object-level cognition when selection operates at multiple timescales. We then turn in Section 3.2 to illustrations of this result, reviewing simulation studies of a variety of architectures, including multi-agent active inference networks, Lotka–Volterra-based systems of predator-prey interactions, coupled genetic algorithms, and coupled generative adversarial networks (GANs), showing how multiple timescales intrinsic to the relevant problem domains induce metacognition as a high-level function. We then discuss in Section 3.3 the emergence of coarse-graining representations of both time and space as a general consequence of metacognition. We conclude that metacognition is far from being human- or even mammal-specific and is not a special case; rather it can be expected at every level of biological organization and always addresses fundamentally the same problem, that of dealing effectively with uncertainties having different spatio-temporal scales.

[PAGE 3] 2. Background
Before we investigate metacognition as an intrinsic function in generic terms, as well as in a variety of well-studied model paradigms, we first consider metacognition from an evolutionary perspective, outline the key computational resources necessary for metaprocessing, and briefly review the active inference framework as a provably general model of systems capable of metacognition.

[PAGE 3] 2.1. Metacognition from an Evolutionary Perspective
The idea of metacognition as a function has two distinct theoretical antecedents: the philosophy and, later, psychology of deliberate, conscious control in humans, and the practical, engineering development of regulators and control systems. The definition of “conscious control” is of course problematic, as there are well-known and substantive disagreements about what consciousness is and how it is manifested (see for example [34,35] for recent reviews). Global workspace [36,37] and higher-order thought [38,39] approaches to consciousness naturally involve metacognition; others, for example Integrated Information Theory [40], do not. Even the notion of content of consciousness is controversial [41].

It is, moreover, as noted earlier, not clear what distinguishes “deliberate” from “automatic" processes [19-21]. These definitional controversies motivate our current purely functional approach to metacognition. This approach is broadly consonant with the systems engineering tradition, particularly as elaborated within artificial intelligence and robotics, where canonical metacognitive functions such as intrinsic motivation [42], curiosity-driven allocation of learning resources [43], and ethics-driven decision making [44] have become increasingly prominent components of autonomous or semi-autonomous systems.

Broadening the definition of metacognition away from human-specific capabilities and avoiding debates about consciousness allows explicit consideration of metacognition as an evolutionary development with potentially deep phylogenetic roots (but also see [22] for an approach to consciousness fully consistent with this evolutionary perspective). As expected, given the self-monitoring and self-regulating components of metacognition, canonical metacognitive tasks such as introspection, deliberate choice, and voluntary recall engage components of both executive [15] and default-mode [45] networks [46-50]. Such tasks can be translated to nonhumans by removing requirements for verbal communication and employing species-appropriate measures of choice, risk, confidence, and self-representation [51]. The involvement of homologous cortical structures in metacognition tasks in humans and other mammals provides additional support for human-like metacognition in other primates [52-56] and rodents [57]; claims of metacognitive ability based on behavioral tests and observation in the wild remain controversial [51,58,59]. Whether metacognitive abilities have been developed, presumably by convergent evolution, in invertebrates, e.g., cephalopods [60,61], also remains controversial.

Here, we offer an alternative approach to metacognition that focuses not on behavioral tests of function, but rather on the presence of architectural structures supporting metaprocessing. This architecture-driven approach avoids a priori assumptions derived from human uses of metacognition; hence, it leaves open the possibility of metacognitive functions in phylogenetically more diverse organisms that might not be noticed or recognized using human cognition as an implicit definition. This leads us to ask, from an abstract, architectural perspective, what computational resources are required to support metaprocessing.

[PAGE 3/4] 2.2. Computational Resources for Metaprocessing
A generic metaprocessing architecture is shown in Figure 1: the metaprocessor samples both the input to and internal representations within an “object-level” processor and provides outputs that either regulate the object-level process or modify its output. Note that in this representation, meta- and object-level processors are regarded as components of a single containing system that receives inputs and produces outputs. Communication between object- and meta-level processors occurs via a defined channel that can, from a design perspective, be considered an application programming interface (API) and is restricted to the data structures supported by this channel. The Good Regulator Theorem requires that the metaprocessor be (i.e., encode or implement) a "model" of the object processor [13]. The regulatory capability of a metaprocessor is subject to the usual tradeoff between the accuracy, complexity, and computational efficiency of its model; overparameterized models or models that require too much time to execute are clearly nonoptimal. Barring these, regulatory capability increases as access to the internal state of the object processor increases, with the coding capacity of the communication channel as a hard limit. Increased access clearly requires increased computational and memory resources for the metaprocessor. From a design perspective, there is a tradeoff between the algorithmic and computational resources allocated to object- and meta-level processing. Even in a technological setting, the object level can become “frozen” for reasons outside a designer's control, forcing the metaprocessor to take on tasks beyond simply monitoring and regulating the performance of the object processor. As evolution tends to proceed by "frozen accidents,” we can expect metaprocessors taking over what would, from a design perspective, be regarded as object-level tasks, to be ubiquitous in the architectures of living systems. Epigenetic regulation can, for example, be viewed as a meta-level regulator of gene expression that “takes over” the role ordinarily played by transcription factors when the latter cannot evolve either fast enough or with sufficient specificity to meet a selective challenge. In what follows, however, we will focus on the more traditional role of metaprocessors as regulators, not partial replacers, of object-level functions.

[PAGE 4] The architecture shown in Figure 1 is clearly hierarchical. The metaprocessor “knows” about the object-level processor only the information encoded in its inputs; the object-level processor, similarly, “knows” about the metaprocessor only the information encoded in the metaprocessor's outputs. Any such architecture is amenable to virtualization: either processor can be replaced with any other processor having the same I/O behavior. The architecture in Figure 1 can, therefore, be considered a simple (two-level) virtual machine hierarchy [62]; it can be extended into a hierarchy of arbitrary depth. A thermostat, for example, is insensitive to whether the gas furnace it has controlled is swapped out for an electric furnace, or even for a heat pump. Genetic engineering is, similarly, effective to the extent that the expression and function of a gene are only minimally and controllably dependent on the cellular environment into which it is placed.


[PAGE 4/5] The local memory available to the metaprocessor determines both the maximum resolution of its model of the object-level process and the maximum window of history data that it can maintain. Hence, local memory is a critical resource for performance-based regulation. In simple systems such as thermostats or the CheY system in E. coli, limited memory can be compensated for by a long characteristic time for regulatory changes; thermostats take advantage of the thermal mass of rooms, while CheY acetylation is much slower than phosphorylation. More sophisticated systems that regulate toward long-term goals, however, require sufficient memory, implementing appropriate data structures, to maintain records of performance history. Metaprocessors that regulate learning systems to maximize learning efficiency provide an example. Any learning algorithm can be considered a function:

(1) L : (f,T) → f'

where f is a function and T is a set of training data. Hence, any learning algorithm can be considered a metaprocessor. Autonomous systems in complex environments must, however, select their own training data by implementing an exploration strategy that focuses on nontrivial but learnable features of the task environment [63]. Such selection systems may be fixed—effectively hardwired—or support learning. Improving training set selection through learning requires a higher-level, longer-time-window metaprocessor that regulates training-set selection by measuring learning progress. Humans accomplish this task heuristically, as expected for systems with limited computational and memory resources [64].

Metaprocessors serving long-term goals typically employ input from multiple sources and coordinate multiple types of actions; autonomous vehicles provide an example [65]. In such systems, the metaprocessor effectively serves multiple object-level processors, regulating their joint behavior toward both near-term (e.g., collision avoidance) and longer-term (e.g., timely arrival at a destination) objectives. As resources become constrained, shared memory and input queuing can be expected to replace true parallelism on both input and output sides. Global workspace (GW) models of human attention allocation and executive control invoke such resource-limited solutions for integrative metaprocessing [36,37]; the LIDA architecture replicates a GW model for robotic control [66].

From a design perspective, metaprocessor architectures are typically explicitly hierarchical and metaprocessors are typically explicitly centralized. Nonhierarchical distributed-system architectures, for example ACT-R [67], lack explicit metaprocessors. Hierarchical recurrent architectures, for example ART [68], similarly exhibit no explicit metaprocessing. We will show below, however, that metaprocessing emerges generically as an effective or apparent function when the interaction between a system and its environment is characterized by fitness functions with multiple characteristic timescales. To show this, we require a suitably generic way to describe both interaction and fitness, to which we now turn.

[PAGE 5/6] 2.3. Interaction across a Markov Blanket
Biological systems are finite, and their interactions with their environments exchange only finite quantities of energy. The most general representation of such finite interactions is bipartite: some finite system S interacts with a finite environment E that is defined to be everything other than S. This definition renders the joint system SE both finite and closed. We can, therefore, represent the interaction as in Figure 2a: the systems S and E interact via a Hamiltonian (total energy) operator HSE that is defined at the boundary B separating S from E. Formally, the boundary B is a decompositional boundary in the state space of the joint system SE; it separates states of S from states of E. The Hamiltonian HSE is, formally, a linear operator on this joint state space. The conservation of energy requires that the net energy flow between S and E is asymptotically zero, i.e., that the interaction is asymptotically adiabatic; we will assume for simplicity that it is adiabatic over whatever time scale is of interest.


[PAGE 6] We now make an explicit assumption that the states of the systems S and E are independently well-defined. This is always the case in classical physics; in quantum theory, it is the assumption that S and E are not entangled (i.e., the joint quantum state |SE) is separable and factors as |SE) = |S⟩|E)) over the time period of interest. In this case, the interaction HSE can be represented, without loss of generality, as a sequence of exchanges of finite information between S and E, i.e., S writes a finite bit string on B, which E then reads, and the cycle reverses [22,33,69]—see [70,71] for technical details. The information exchanged between S and E—hence the information encoded on B in each interaction cycle—can be specified exactly: at each instant, B encodes the current eigenvalue of the operator HSE.

Under these conditions—finite interaction in the absence of quantum entanglement—the decompositional boundary B functions as an MB between S and E [72]. An MB surrounding a system S is, by definition, a set of states m = (s, a) such that internal states i of the blanketed system S (if any) depend on external or environmental states e only via their effects on m [25,26]. In practice, an MB exists whenever the state space of S(E) includes states not on the boundary B. Friston [27,28] introduced the decomposition of the MB into sensory states s and active states a, with the further constraint that sensory states are not influenced by internal states and active states are not influenced by external states; this distinction is illustrated in Figure 2b and, in two biological contexts, in Figure 3. Any MB is, clearly, completely symmetrical; the “sensory” states of E are the "active" states of S and vice versa. An MB generalizes the function of an API, in effect defining, and therefore limiting, the communication channel between any system S and its environment E.

Note that nothing in the above places any constraints on the internal interactions HS and HE of S and E, respectively. In particular, neither Hs nor He can be inferred from the interaction HSE defined at the boundary B that implements the MB separating S from E. The existence of the MB thus places significant restrictions on what S can "know" about E and vice-versa. Because S has no direct access to the state e of E, S cannot measure the dimension of e, and hence cannot determine the number of degrees of freedom of E. The MB similarly prevents any direct access to the internal interaction HE of E. We can, therefore, construct any finite decomposition E = FG with internal interaction HE = HF + HG + HFG without affecting what S can detect, i.e., without affecting the information encoded on the MB by HSE in any way. Hence, S cannot “know” about decompositions of E. Any “subsystems” of E represented by S are, effectively, sectors of the MB that are defined by computational processes implemented by S; see [22,33,69,73] for formal details and further discussion. As the MB is completely symmetrical, these considerations apply equally to E.


[PAGE 8] dependence in HSE will also induce time-varying states in both S and E (see [33] for further discussion). The Free-Energy Principle (FEP) [27-29,32] is the statement that closed joint systems SE will asymptotically approach a constant HSE and, hence, that both S and E have NESS densities as attractors; it is shown in [33] that this principle is, for quantum systems, asymptotically equivalent to the Principle of Unitarity, i.e., of the conservation of information within the joint system.

[PAGE 8] 2.4. Active Inference Framework
The Bayesian inference framework enables quantitative models linking sensory mechanisms (i.e., inputs) with functional behaviors (i.e., outputs) in arbitrary classical [32] and quantum [33] systems. This framework has been applied extensively to biological systems [28-31,74–78]; here, we consider it more generally. The variational free energy (VFE) that is being minimized in Bayesian inference follows out of classical analytical and statistical physics considerations as a unique form of a least action principle. With the partition defined by the MB in hand, we can interpret internal states as parametrizing some arbitrary probability density q(e) (its “beliefs”) over external states. This replaces the Lagrangian being used to compute the gradient descent in classical least action (or variational) principles with a VFE functional F of form (cf. Equation (8.4) in [32]):

F(m,i) = – ln p(m, i) + DKL(q(e)||p(e | m,i)), (2)

where

DKL(pq) =  ∫ p(x) ln (p(x)/q(x)) dx (3)

is the Kullback–Leibler Divergence defined as the expectation of the logarithmic difference between the probabilities p and q. This makes Equation (2) a logarithmic difference between the (variational) density or Bayesian “beliefs” about external states q(e) and actual probability densities p(e|m,i) of external states given the MB state m and the internal state i. Note that the VFE is a measure of uncertainty about how the current behavior of the environment E (i.e., the external state e) will affect the future state m of the MB; the VFE is thus completely distinct from the thermodynamic free energy required to drive computation, as discussed in Section 3.1 below.

The first term in Equation (2) is known as (Bayesian negative log) model evidence, or marginal likelihood, describing the likelihood that the sensory inputs (i.e., the MB state m) were generated by the system's generative model, implicit in the internal state i. Because negative evidence is equivalent to unexpected sensory inputs, this term is also referred to as surprisal. The second term is referred to as relative entropy, which minimizes the divergence between the variational and posterior densities q(e) and p(e|m,i), respectively. As a result, maximizing model evidence—by either changing i to encode new beliefs or acting on E so as to alter m—is equivalent to minimizing the VFE F(m, i) of the system [28-30,32]. Because the divergence of the second term can never be less than zero, free energy is an upper bound on the negative log evidence. For this reason, the Kullback–Leibler term is also referred to as a bound on surprisal.

With this definition of VFE, the FEP can be stated as the principle that any system S with an MB will behave so as to minimize VFE, i.e., so as to maximize its ability to predict the future actions of its environment E on its MB. This is active inference [27-29,32]. Maximal predictive ability clearly implies minimal time variation in both HSE and Hs, i.e., S occupying or being very close to its asymptotic NESS. The converse is also true:


[PAGE 9] Theorem 1. Time variation in HSE increases VFE for S(E)

Proof. Time variation in HSE is time variation in the spectrum of HSE, i.e., in the eigenvalues of HSE encoded on the MB. VFE measures the unpredictability of these eigenvalues; hence, variation in the eigenvalues increases VFE.

Theorem 1 imposes an additional limit on what S can "know" both about E and about itself. Consider a time evolution U = SE → U = S'E' that “moves” the decompositional boundary B within a fixed joint system U. This evolution corresponds to an exchange of degrees of freedom between S and E; S engulfing part of E or vice versa would be an example. Any such evolution changes the Hamiltonian, HSE → HS'E' . If by theoretical fiat we choose to regard S and S' as “the same system”—e.g., we treat an organism as “the same thing" after it ingests a meal—we can conclude via Theorem 1 that the VFE experienced by that system has increased. Its VFE may, of course, thereafter decrease if the new interaction HSIE' proves more predictable than the previous HSE.

Theorem 1 has a significant consequence for the FEP: no system can reliably predict, from the information available on its MB, whether it has reached its NESS. Indeed, to do so would require reliably predicting that its environment has reached its respective NESS, a feat explicitly ruled out by the MB condition.

For the far-from equilibrium systems that predominate in biology, the active inference formalism transforms an intractable integration problem of a global thermodynamic potential into a tractable integration over a probabilistic model of how a system “thinks" it should behave, i.e., a model of the behavior of the states of its MB. To illustrate this, recall Equation (1) and consider how a system learns about, and hence enhances its ability to predict the behavior of, its environment. At each instant, the state m of the MB can be considered the "next" element of the training set T. The internal state i of S can be considered to implement some function f; indeed this f is, in the language of the FEP, S's generative model at that instant. If S is an artificial neural network, for example, f is implemented by the node transfer function (which can be taken to be identical for all nodes) and the weight values. In the early stages of learning, i.e., far from the NESS, significant variation in m will induce significant variation in f and hence in i. An active system, e.g., an organism or an autonomous robot, will modify m by behaving, i.e., acting on E to provoke some response. As the predictive capability of f improves, i.e., as i approaches in NESS, changes in m will invoke smaller changes in f, with no further changes in f (i.e., no further learning) as the limit in which "perfect prediction" and the NESS are achieved.

This principle of active inference has recently been applied directly to biological self-organization, beyond its initial applications in neuroscience, from lower levels of biological complexity such as a primordial soup [28], to higher levels of cells and tissues during morphogenesis [29,31]. Indeed, it has been shown to map quite well to the evolution of larger-scale, higher-complexity systems. If each of a collection of small-scale systems (e.g., cells or individual organisms) can minimize its own, individual VFE (and hence maximize its predictive success) by closely associating with other small-scale systems, a larger-scale system (e.g., a multi-cellular organism or an organismal community) will emerge; such a larger-scale system will remain stable as a macroscopic, collective entity, and hence have a well-defined MB at its larger scale, but only if its dynamics successfully minimize VFE at its larger scale [79]. In this context, natural selection at each scale can be viewed as Bayesian model selection (also referred to as structure learning, because the structure of the generative model itself is evolved and learned [80]) at that scale [30]. This view of natural selection as Bayesian belief updating can be traced back to Fisher's fundamental theorem of natural selection [81], which equates the rate of increase in fitness to the genetic variance in fitness [81,82]. Fisher's theorem itself has been cast in information theoretic terms by S.A. Frank in relating the rate of fitness increase to the amount of information that a population acquires about its environment based on the price equation [83], which itself can readily be shown to be an instance of Bayes' theorem [81]. For an evolving organism, each instantaneous interaction with the environment serves as a "training example" for the generative model as discussed above [84]. How selection pressure generates new neuronal structures underlying new generative models as its own variational free energy minimizing process has been shown in [75]. The fundamental operations of expanding and adapting generative models in response to selection pressure have been outlined in [85] and form a gradual emergence of complexity, balancing model complexity and accuracy demands from increased environmental dynamic complexity driving evolution.

[PAGE 10] The limit in which selection (i.e., the modification of the generative model) stops and the NESS is achieved is the limit of "perfect adaptation" to the local environment. This process can be formulated in the traditional language of fitness as follows. As “fitness” can be considered a probability of future existence—i.e., of survival and/or reproduction—it can be expressed as a (instantaneous) function g : e,m,i → [0,1], where, as above, e, m, and i, are (instantaneous) external, blanket, and internal states, respectively (hence (m, i) is the total organism state). Perfect predictions minimize VFE and maximize survival probability; indeed, the fundamental prediction of any system is its own future existence [32]. Hence, with appropriate normalization, we can write g(e, m, i) = 1 – δ, where δ is the total prediction error of the generative model implemented by the internal state i when exposed to the external state e as represented on the blanket state m (see also [86], where this is made time-dependent using a path-integral formalism). Systems with large values of δ are unlikely to survive/reproduce; those with small δ are more likely to survive/reproduce. It is important, moreover, to note that nothing in the theory guarantees the stability of emergent large-scale structures; indeed, many organisms are only facultatively multi-cellular, and as human history amply demonstrates, multi-organism communities often collapse. It is also important to note that while MBs are often associated intuitively with cell membranes or other biophysically implemented spatial boundaries, the MB itself is a decompositional boundary in an abstract state space that may or may not assign spatial degrees of freedom to every state.

It is not our intention here to develop a general theory of evolution as Bayesian satisficing; for some initial steps in this direction, see [84]. Our interest is in a particular aspect of this process, the development of metaprocessor architectures as a way of obtaining more predictive efficacy at a smaller free-energy cost. We also explicitly set aside the question of how one agent identifies another and tracks the other's identity through time. Identification and tracking require that some criterial component of the identified system maintains an NESS; detailed models of the identification and tracking process are developed in [33,69].

[PAGE 11] 3. Results
In the following sections, we will introduce a formal description of metacognition in an evolutionary setting and discuss how different time scales in the interaction dynamics of simple, well-known and -studied systems induce metacognition in these systems. In Section 3.1, we will formally show how metacognitive abilities confer a fitness advantage in an evolutionary setting based on information theoretic energy considerations. In Section 3.2, we start by reviewing some different computational examples of such simple interaction networks and discuss how complex system dynamics in each case necessitate some form of metacognition. We will first look at multi-agent active inference networks, which are simultaneously the most general models of interaction, and exhibit the most explicit form of metacognition we discuss here. Then, we move on to the more specific, established computational frameworks of multi-agent/network models, including Lotka-Volterra-based systems of predator-prey interactions, coupled genetic algorithms, and GANs, highlighting and exploring their various roles in modeling coadaptation and co-evolution. While we will mostly focus on two agent/network interactions, we note that an arbitrary number n of interactions can, while often technically challenging, be decomposed, without the loss of generality, into n binary interactions, each between one of the n agents and the remaining n - 1 agents considered as a single system. This follows directly from the Markov blanket condition; at a deeper level, it follows from the associativity of vector-space decompositions [73]. Finally, in Section 3.3, we will discuss how spatio-temporally coarse-grained structures emerge naturally in any resource-limited system with sufficiently complex interaction dynamics.

[PAGE 11] 3.1. Formal Investigation of Metacognition in Evolution
Our primary claim in this paper is that selection on multiple timescales induces metacognition. We can make this claim precise by employing the general formalism for a finite system S interacting with a finite environment E via a Markov blanket MB developed in Section 2.3. Interesting interactions involve nontrivial information processing between reading the MB B (i.e., sensation) and writing on B (i.e., action) on the part of both S and E. Landauer's principle [87,88] imposes a free-energy cost of at least ε = kβT ln2 per erased bit (and hence per rewritten bit) on (classical) information processing, where kB is Boltzmann's constant and T > 0 is the temperature of the system performing the computing. Note that this is a lower limit; a realistic system will have a per-bit processing cost of ε = βkBT, with β > ln2 a measure of thermodynamic efficiency. As this free energy must be sourced, and the waste heat dissipated, through B asymptotically, we assume for simplicity that it is sourced and dissipated through B on every cycle. We can, therefore, represent B as in Figure 2b. The bits allocated to free-energy acquisition and waste heat dissipation are uninformative to S, as they are burned as fuel for computation. The bits allocated to action by S are similarly uninformative to S, although they are informative to E. The bits allocated to sensation by S are informative to S and serve as the inputs to S's computations. This distinction between informative and uninformative bits is often not made explicitly (such as in Figure 3), where the flow of free energy as incoming fuel and outgoing waste is ignored; however, it is required whenever the joint system SE is thermodynamically closed [33]. To emphasize this distinction, we will use s for informative sensory states and a for states encoding “interesting” actions, i.e., actions other than waste heat dissipation.

[PAGE 11/12] With this formalism, we can place lower limits on the free energy cost of the computational processes that determine S's actions on E, given various assumptions about E's actions on S and S's computational architecture. As noted above, E's actions on S implement natural selection. The characteristic times of E's actions are, therefore, the characteristic times of the selective pressures that S faces.

[PAGE 12] In the simplest case, E acts on S via some stochastic process that varies randomly with a characteristic timescale τ. As S has, in principle, no access to the internal dynamics of E except through B, from S's perspective, τ is the response time of E to actions by S. We assume for simplicity that S's response time is also τ, i.e., that S is capable of computing its next action in time τ from detecting E's current action.

We also assume for simplicity that S devotes the same number n of bits on B to sensation and action, i.e., S computes a function f: {0,1}ⁿ → {0,1}ⁿ, and that this function depends, in general, on the values of all n input bits (i.e., S does not ignore informative bits). If computing this function requires m steps, including all relevant classical memory write operations, then S has a free-energy cost of n m ε per sensation-action cycle.

Now consider a situation in which E acts on S via two independent stochastic processes with two different characteristic timescales, τ₁ and τ₂, and assume that S detects and processes the inputs from these two environmental processes independently. Assume also that S's thermodynamic efficiency is the same for detecting and processing the inputs from these two environmental processes. In this case we can factor the state s into two sectors, s₁ and s₂, encoding n₁ and n₂ bits, respectively, the states of which vary at τ₁ and τ₂, respectively. Suppose now that τ₁ = τ and τ₂ > τ₁, i.e., suppose S experiences independent selective pressures with two different characteristic timescales. In this case, S's free-energy cost to process only the fast-varying inputs from s₁ is n₁mε per sensation-action cycle, while S's free-energy cost to process only the slowly varying inputs from s₂ is (τ/τ₂)n₂mε per sensation-action cycle. Hence, S's total free-energy cost for processing the two inputs separately is:

(4) Ξ = (n₁ + (τ/τ₂)n₂)mε + Δ,

where Δ is the overhead cost, which must also be sourced as free energy from E, of detecting the change in s₂ and devoting resources to processing it. Whether Ξ < nmε, i.e., whether processing the inputs separately is more energy efficient than processing all n input bits together on every cycle, clearly depends on the value of Δ. Hence, we can state:

[PAGE 12] Theorem 2. If informative inputs vary at different timescales τ₁ and τ₂, τ₂ > τ₁, processing the inputs separately is energetically advantageous provided