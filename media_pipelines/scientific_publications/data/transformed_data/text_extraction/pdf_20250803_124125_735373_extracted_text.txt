Advancing the Scientific Method with Large Language Models:
From Hypothesis to Discovery
Yanbo Zhang1,*+, Sumeer A. Khan2,6,7, Adnan Mahmud³4, Huck Yang, Alexander
Lavin, Michael Levin¹º, Jeremy Frey10, Jared Dunnmon¹¹, James Evans12,13, Alan
Bundy14, Saso Dzeroski15, Jesper Tegner2,16,17,18,19,*+, Hector Zenil20,21,22,23,24*+


[PAGE 2] Abstract
With recent Nobel Prizes recognising AI contributions to science, Large Language Models
(LLMs) are transforming scientific research by enhancing productivity and reshaping the
scientific method. LLMs are now involved in experimental design, data analysis, and
workflows, particularly in chemistry and biology. However, challenges such as
hallucinations and reliability persist.

In this contribution, we review how Large Language Models (LLMs) are redefining the
scientific method and explore their potential applications across different stages of the
scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve
as relevant and effective creative engines and productivity enhancers, their deep integration
into all steps of the scientific process should be pursued in collaboration and alignment
with human scientific goals, with clear evaluation metrics.

The transition to AI-driven science raises ethical questions about creativity, oversight, and
responsibility. With careful guidance, LLMs could evolve into creative engines, driving
transformative breakthroughs across scientific disciplines responsibly and effectively.
However, the scientific community must also decide how much it leaves to LLMs to drive
science, even when associations with ‘reasoning', mostly currently undeserved, are made
in exchange for the potential to explore hypothesis and solution regions that might
otherwise remain unexplored by human exploration alone.


[PAGE 3] Introduction
Recent advances in artificial intelligence (AI) have transformed multiple areas of society,
the world economy, and academic and scientific practice. Generative AI and Large
Language Models (LLMs) present unprecedented opportunities to transform scientific
practice, advance Science, and accelerate technological innovation. Nobel Prizes in
Physics and Chemistry were awarded to several AI leaders for their contributions to AI
and frontier models, such as Large Language Models (LLMs). This promises to transform
or contribute to scientific research by enhancing productivity and supporting various stages
of the scientific method. The use of AI in science is booming across numerous scientific
areas and is impacting different parts of the scientific method.

Despite the potential of LLMs for hypothesis generation and data synthesis, AI and
LLMs face challenges in fundamental science and scientific discovery. Hence, our premise
in our perspective is that AI, in general, has so far been limited in its impact on fundamental
science, which is defined here as the discovery of new principles or new scientific laws.

Here, we review how LLMs are currently used as a technological tool – to augment the
scientific process in practice and how they may be used in the future as they become more
powerful tools and develop into powerful scientific assistants. Combining data-driven
techniques with symbolic systems, such a system could fuse into hybrid engines that may
lead to novel research directions. We aim to describe the gap between LLMs as technical
tools and “creative engines” that could enable new high-quality scientific discoveries and
pose novel questions and hypotheses to human scientists. We first review the current use
of LLMs in Science, aiming to identify limitations that need to be addressed when moving
toward creative engines.

There is solid recognition and excitement for the transformative potential of AI in
Science. For example, leading machine learning conferences (NeurIPS, ICML) have
recently (2021-2023) arranged targeted workshops on AI4Science. Some recent reviews
and papers include1–38. This demonstrates the energy and potential of using automated (i.e.,
AI tools) for Science. This “dream” can be traced back to the times of Turing and the
emergence of Artificial Intelligence in the 1950s39. With recent advancements in
computational techniques, vastly increased production of scientific data, and the rapid
evolution of machine learning, this long-held vision can be transformed into reality. Yet,


[PAGE 4] 
most current reviews and original papers focus on specifically designed machine learning
architectures targeting particular application domains or problems.

For example, recent reviews have explored how to use variants of Deep Learning,
Geometric Deep Learning, or Generative AI in its generality (including different
architectures such as CNNs, GNNs, GANs, diffusion models, VAEs, and Transformers) as
a tool for assisting Science 3,11,13,15,19,22. For example, Wang et al.¹, reviews breakthroughs
in how specific techniques such as geometric deep learning, self-supervised learning,
neural operators, and language modelling have augmented Science in protein folding,
nuclear fusion, and drug discovery. An essential thread in their review is the vital notion
of representation, pointing out that different AI architectures can support valuable
representations of scientific data and thereby augment Science. Recent papers demonstrate
the appeal and the potential of using AI-driven and augmented tools for automating
science1,4,13,40. Traditional scientific advancements have been primarily driven by
hypothesis-led experimentation and theoretical development, often limited by human
cognitive capacities and manual data processing. For example, the formulation of
Newtonian mechanics required meticulous observation and mathematical formalization
over extended periods. Here, the rise of AI4Science represents a paradigmatic revolution
that could reach beyond human cognitive limitations. AI-driven advancements promise to
enable rapid processing and analysis of massive data sets, revealing complex patterns that
surpass human analytical capabilities. For example, DeepMind's AlphaFold dramatically
transformed protein structure prediction, a longstanding scientific challenge, using deep
learning to predict protein folding accurately. Furthermore, AI4Science could reverse the
slowdown in scientific productivity in recent years, where literature search and peer-review
evaluation41-43 are bottlenecks.

In contrast to previous reviews, here we first address the use of LLMs, regardless
of the specific underlying architecture, and their use as a tool for the scientific process. We
assess how different areas of science use LLMs in their respective scientific process. This
analysis sets the stage for asking how LLMs can synthesize information, generate new
ideas and hypotheses, guide the scientific method, and augment fundamental scientific
discoveries. Here, we ask to what extent AI can be described as a “general method of
invention," which could open up new paradigms and directions of scientific investigations.


[PAGE 5] 
Hence, complementary to a purely representational and architectural viewpoint of
AI4Science, we find it constructive to ask and assess to what extent the nature of the
scientific process, both its inductive and deductive components, can and should be
transformed by AI techniques.

[PAGE 5] Current use of LLMs – From Specialised Scientific Copilots to LLM-
assisted Scientific Discoveries
The ability of Large Language Models (LLMs) to process and generate human-like text,
handle vast amounts of data, and analyse complex patterns with potentially some reasoning
capabilities has increasingly set the stage for them to be used in scientific research across
various disciplines. Their applications range from simple tasks, such as acting as copilots
to assist scientists, to complex tasks, such as autonomously performing experiments and
proposing novel hypotheses. We will first introduce the fundamental concepts of LLMs
and then review their various applications in scientific discovery.

[PAGE 5] Prompting LLMs: From Chatbot to Prompt Engineering
Current mainstream LLMs are primarily conditional generative models, where the input,
such as the beginning of a sentence or instructions, serves as a condition, and the output is
the generated text, such as a reply. This text is typically sampled auto-regressively: the next
token (considered the building block of words) is sampled from a predicted distribution.
See Figure 1A.

Given LLMs' capabilities in computation and emerging potential for reasoning, which we
define as the ability to solve tasks that require reasoning, they can be considered
programming languages that use human language as the code that instructs them to perform
desired tasks. This code takes the form of “prompts.” For instruct-tuned LLMs, the prompt
often consists of three parts: the system prompt and the user prompt, with an LLM's reply
considered the assistant prompt. Hence, a chat is frequently composed of
<system><user><assistant><user><assistant>, see Figure 1B. The system prompt
typically includes general instructions for the LLMs, such as behaviour, meta-information,
format, etc. The user prompt usually contains detailed instructions and questions. Using
these prompts, the LLMs generate replies under the role of "assistant."

Since LLMs do not have background knowledge about the user, and prompts are
their major input, designing a good prompt is often critical to achieving the desired output
and superior performance. Researchers have shown that specific prompts, including
accuracy, creativity, and reasoning, can significantly improve output performance.
Specifically, the chain-of-thought (CoT) method44 can instruct LLMs to think step-by-step,
leading to better results. Beyond these, the Retrieval-augmented Generation (RAG)
method45 can incorporate a large amount of context by indexing the contents and retrieving
relevant materials, then combining the retrieved information with prompts to generate the
output. Due to the importance of prompts and LLM agents, designing prompts is now often
called "prompt engineering,” and many techniques and tricks have been developed in this
area46,47, such as asking JSON format outputs, formulating clear instructions, setting
temperatures, etc47,48.

While carefully designed prompts can accomplish many tasks, they are not robust
and reliable enough for complex tasks requiring multiple steps or non-language
computations, nor can they explore autonomously. LLM agents are developed for these
requirements, especially for complex tasks. LLM agents are autonomous systems powered
by LLMs, which can actively seek to observe environments, make decisions, and perform
actions using external tools49. In many cases, we need to ensure reliability, achieve high-
performance levels, enable automation, or process large amounts of context. These tasks
cannot be accomplished solely with LLMs and require integrating LLMs into agent
systems. Early examples include AutoGPT50 and BabyAGI51, where LLMs are treated as
essential tools within the agent system (Figure 1C). In scientific discovery, LLM agents
become even more critical due to the complexity of science and its high-performance
requirements. Many tools have also been developed to provide easy access to these
prompting and agent methods, such as LangChain52 and LlamaIndex53. Automated prompt
design methods, such as DSPy54 and TextGrad55, are also being developed to design
prompts and LLM agents in a data-driven way.

[PAGE 7] LLMs as Practical Scientific Copilots
The ability of LLMs to work with a large body of text is being exploited in the practice of
science. For example, LLMs assist in proposing novel ideas, writing scientific papers and
generating computer code, thereby improving productivity; they also adapt texts for diverse
audiences ranging from experts to broader audiences, thus supporting communication in
science.

Furthermore, LLMs can sift through vast bodies of scientific literature to identify
relevant papers, findings, and trends. Such reviewing of the relevant literature helps
investigators quickly digest and identify gaps in enormous bodies of scientific knowledge.
These capabilities can also mitigate discursive barriers across different scientific fields,
supporting interdisciplinary scientific collaborations and knowledge sharing. Recently,
chatbots have emerged in several disciplines as virtual assistants answering scientific
queries posed by scientists. Such tools exploit the power of LLMs to extract and detect
patterns, data, and knowledge. These techniques may also serve as important tools in
science education and communication.

These examples demonstrate the rise of LLMs in extracting and sharing information
and the exciting open research frontier of the potential of reasoning that they represent in
different scientific domains56–58. For instance, Caufield et al. proposed the SPIRES
method59, which uses LLMs to extract structured data from the literature. Beyond data
extraction, LLMs have also shown evidence of outperforming annotation tasks60,61,
enabling scientists to scale data annotation. Some domain-specific models also show
superior performance in classification, annotation, and prediction tasks62–64. With the help
of RAG methods45, LLMs can directly apply their information extraction and distillation
capabilities to large amounts of text data. With the combination of diverse capabilities of
LLMs interconnected through LLM-agents, the recent “AI co-scientist65” demonstrates
impressive ability in generating novel research ideas by leveraging existing literature,
engaging in internal LLM-agent debates, and refining its outputs. This process leads to
constructive progress when applied to real scientific tasks.

Moreover, LLMs are currently used to automate the experimental design and the
execution of experiments. For example, Boiko, et al.66 propose an autonomous LLM
capable of performing chemical experiments. This work employs an LLM planner to
manage the experimental process, such as drawing patterns on plates or conducting more
complex chemical syntheses. Compared to hard-coded planners, the LLM-based planner is
more flexible and can handle unexpected situations. Similar kinds of loop and tool usage
are also shown in67, which includes literature tools, consulting with humans, experimental
tools, and safety tools.

In the biological domain, for instance, the CRISPR-GPT68 represents a significant
advancement in biological research. It utilizes LLMs to automate the design of gene-editing
experiments, enhancing both the efficiency and precision of genetic modifications, which
is pivotal in speeding up genomic research and applications. Another advance in the
application of LLMs in the biological domain is BioDiscoveryAgent69. These tools
augment scientists' capabilities and accelerate scientific discovery.

The capabilities described thus far capture the current use of LLMs as knowledge
engines. Summarising, extracting, interfacing, and reasoning about (scientific) text,
alongside automating experimental design and execution. While immensely useful, it
remains an open frontier on how to do this safely and efficiently. It largely depends on how
prompting is performed and how LLM agent systems are designed.

[PAGE 9] Foundation Models for Science
A key observation when using LLMs as clever text engines or exploiting the underlying
machine learning (neural) architecture for solving specific scientific problems was the
importance of scale. Larger models trained on larger amounts of data, or spending larger
amounts of computation during inference time yielded an increase in performance56,70,71.
The discovery of such scaling laws72 demonstrated that LLMs' performance improves as
the number of parameters increases. Thus, we can expect the above trends to grow in
importance as these systems are trained on ever larger amounts of data. Emergent
behaviours, such as reasoning were suggested when models increased in scale73.
Concurrent with the appreciation of scaling laws came the realisation that instead of using
LLMs for specialised problems or as text engines, one could potentially train them on large
amounts of data, not necessarily text, but different modalities of scientific data. This is the
idea of a foundation model. These are large-scale pre-trained models that, when trained
with a sufficient amount of data of different types, such models “learn” or “encapsulate”
knowledge of a large scientific domain, thus reaching beyond a specific scientific problem.
When fine-tuned to particular tasks, such models can solve a wide range of downstream
tasks. The notion of foundation models refers to their generality in that they can be adapted
to many different applications, unlike task-specific engineered models solving a
specialised task such as protein folding. Notably, the famous transformer architecture that

[PAGE 10] 
fuels LLMs has become the architecture of choice when constructing the foundational
models in different domains of science. These self-supervised models are usually pre-
trained on extensive and diverse datasets. This enables them to learn from massive
unlabelled data since masking parts of the data and then requiring the model to predict the
occluded parts provides foundation models with their learning objective. This technique is
used when training LLMs on large amounts of text. The idea is thus exploited in scientific
domains where multi-modal data is used to train self-supervised foundation models. Once
trained, the model can be fine-tuned for various downstream tasks without requiring
additional training. Consequently, the same model can be applied to a wide range of
downstream tasks. The foundation model encapsulates a large body of scientific
"knowledge” inherent in the training data.
Leveraging these ideas, there has been a rise in the number of foundation models
of science. For example, the Evo and Evo 2 models enable prediction and generation tasks
from the molecular to the genome scale74. While Evo is trained on millions of prokaryotic
and phage genomes, Evo 275 includes massive eukaryotic genomes, and both demonstrate
zero-shot function prediction across DNA, RNA, and protein modalities. It excels at
multimodal generation tasks, as shown by generating synthetic CRISPR-Cas molecular
complexes and transposable systems. The functional activity of Evo-generated CRISPR-
Cas molecular complexes and IS200 and IS605 transposable systems was experimentally
validated, representing the first examples of protein-RNA and protein-DNA co-design
using a language model. Similarly, scGPT is for learning single cell transcriptional data76,
ChemBERT encodes molecular structures as strings, which then can be used for different
downstream tasks such as drug discovery and material science77. Similarly, OmniJet-a is
the first cross-task foundation model in particle physics, enhancing performance with
reduced training needs78. Additionally, multiple physics pretraining (MPP) introduces a
task-agnostic approach to modelling multiple physical systems, improving predictions
across various physics applications without extensive fine-tuning79. The LLM-SR80
implements similar symbolic regression methods iteratively, generating and evaluating
hypotheses, using the evaluation signal to refine and search for more hypotheses.
Incorporating diverse scientific data modalities, which represent different
"languages" to interact with observations beyond natural language, is crucial. There are

[PAGE 11]
two major approaches emerging: 1) End-to-end training on domain-specific modalities:
Models like ChemBERT77 (using chemical SMILES strings) and scGPT76 (using single-
cell data), as mentioned above, are directly trained on these specialized data types. 2)
Separate training with compositional capabilities: This involves training separate encoders
for new modalities or enabling LLM agents to utilize tools that interact with these
modalities. For instance, models like BiomedCLIP81 connect biological images with
natural language, while PaperCLIP82 and AstroCLIP83 link astronomical images and
spectral data to textual descriptions. Furthermore, frameworks like ChemCrow84 leverage
the tool-using abilities of LLMs to connect with non-natural-language modalities, such as
chemical analysis tools.
Yet, as with text-based LLMs, several challenges remain. These include potential
biases in datasets, which can bias the performance and output of these models. Since
science is mainly about understanding systems, the scale, and opaqueness of these models
make interpretation a particularly challenging problem. Also, several observations, such as
their capability for generalisation, multi-modality, and apparent emergent capabilities,
have led to intense discussions at the research frontier on the extent to which these
foundation models can reason within and beyond their training regimes. The text-based
LLMs (or models incorporated with text modality) discussed above are constructed using
these techniques. Examples include GPT-4 (OpenAI)85, BERT (Bidirectional Encoder
Representation from Transformers)86, CLIP (Contrastive Language-Image Pre-training,
OpenAI)87, and DALL-E from OpenAI88.
These foundation models have the potential to achieve professional human-level
performance or even surpass human capabilities when trained using reinforcement
learning, particularly with feedback from reliable formal systems. For example,
AlphaProof 89 has become state-of-the-art in automated theorem-proving systems,
achieving mathematical capabilities comparable to human competitors at IMO 2024.
Approximately one million informal mathematical problems were translated into the
formal language LEAN, a mathematical proof verification language, enabling the LLM to
be trained through reinforcement learning. Solutions generated by the LLM in LEAN are
either proved or disproved by the LEAN compiler, with the resulting correct or incorrect
solutions serving as feedback to refine the LLM. While this approach has been explicitly

[PAGE 12]
applied within the mathematical domain, it demonstrates significant potential for training
LLMs to surpass human performance in highly complex and deductive reasoning.
Although developing formal systems for general tasks remains challenging, reinforcement
learning methods are employed to build foundation models with enhanced deductive
capabilities, leading to the rise of reasoning models such as OpenAI 01/0370, Deepseek
R156, and others. In scientific domains such as physics, external and reliable feedback
mechanisms are already used to improve answer quality, highlighting the potential for
creating domain-specific foundation models.
In conclusion, the rise of foundation models will continue to affect and disrupt
science due to their powerful nature, scaling properties, and ability to handle very different
data modalities. However, for our purposes, the question remains of what extent foundation
models could be a proper gateway to making fundamental scientific discoveries. To what
extent can foundation models be creative and reason outside their training domains?

[PAGE 12] Toward Large Language Models as Creative Engines for Fundamental
Science
Here, we ask how AI can impact fundamental Science? That is, what is required for an AI
to be able to discover new principles of scientific laws from observations, available
conjectures, and data analysis? Broadly, can generative AI develop to become a “creative
engine" that can make fundamental scientific discoveries and pose new questions and
hypotheses? Einstein famously stated, “If I had an hour to solve a problem, I'd spend 55
minutes thinking about the problem and 5 minutes thinking about solutions”. This
underscores the importance of carefully considering the question or problem itself, as
posing hypotheses effectively can be the most intellectually demanding part of Science. As
a first approximation, the ability to pose novel hypotheses is at least for us humans
what appears to be essential for making novel discoveries. Thus, what is required for an AI
to advance beyond a valuable tool for text generation and engineered systems for solving
a particular problem? Or could foundation models provide a possible path forward?
In our view, if LLMs are to contribute to fundamental Science, it is necessary to
assess what putative roles LLMs can play in the core of the scientific process. To this end,


[PAGE 13] Augmenting the Scientific Method
As a first approximation, scientific discovery can be described as a reward-searching
process, where scientists propose hypothetical ideas and verify or falsify them through
experiments91. Under this Popperian formulation, LLMs can assist scientific discovery in
two ways (Figure 2): On the one hand, LLMs could assist in the hypothesis-proposing
stage, helping scientists find novel, valuable, or potentially high-reward directions or even
propose hypotheses that human scientists might have difficulty generating. On the other
hand, LLMs have the potential to make experiments more efficient, accelerate the search
process, and reduce experimental costs.
At the stage of proposing hypotheses, scientists choose unknown areas to explore,
which requires a deep command of domain knowledge, incorporating observational data,
and manipulating existing knowledge in novel ways45,92. Their expertise and creativity
could carry the potential for proposing novel research hypotheses.
Then, at the verification stage, experiments are conducted to obtain relevant
information and test hypotheses. This requires the ability to plan and design experiments
effectively. Given LLMs' planning capabilities and potential understanding of causality93-
95, they can help scientists design experiments. By incorporating tool-using abilities",
LLMs can directly implement experiments. LLM agents can perform complex workflows
and undertake repetitive explorations that are time-consuming for human scientists. This
allows us to search for novel results efficiently, which is key to scientific discovery97,98.

we discuss below how LLMs can augment the scientific method. This includes how LLMs
could support observations, automate experimentation, and generate novel hypotheses. We
will also explore how human scientists can collaborate with LLMs.



[PAGE 14] Expanding or Narrowing the Observation Process
Scientists rely upon observational results for guidance in proposing hypotheses, designing
and refining experiments, evaluating experimental results, and validating their hypotheses.
In general, observations act as dimension reduction methods99, which include annotating,
classification, and information extraction.

General purpose LLMs, such as GPT-4, Llama, can be good observers for language
and image data for general purposes. Their in-context and zero-shot learning capabilities104
can be used as universal classifiers to extract specific information from these data, such as
annotation and evaluation. In domains like NLP and Social Science, annotating and
evaluating language data at scale is a fundamental task for downstream experiments.
Trained humans or crowd-workers have often done such jobs. However, LLMs, such as
ChatGPT, can perform higher or comparable performance levels relative to crowd-workers
on annotation tasks, especially on more challenging tasks60,61.

Besides language processing, scientists must also describe complex behaviours at
scale qualitatively. LLMs show potential in describing such complex black-box systems,
where we observe only their inputs and outputs without knowledge of their underlying
mechanisms. Although deciphering such systems can often become a stand-alone research
question, having a qualitative description can still be helpful when faced with large-scale
data. With LLMs, black-box systems, such as language input-output, mathematical input-
output pairs, fMRI data100, or observational data, can be described using natural
language100.

Beyond text and text-represented systems, different data modalities represent
different "languages" to interact with observations, and domain-specific modalities are
extremely important for scientific discovery. Scientific research often involves other data
types, including image, video, audio, table101,102, or even general files103, as well as domain-
specific modalities like genomic sequences, chemical graphs, or spectra76,77,82,83. Multi-
modality LLMs can play the observer role vis-a-vis these data. However, most multi-
modality LLMs are still struggling to handle some domain-specific data formats, such as
genomic data or chemical compounds, which may require converting and where
information may be lost during the conversion process. For example, chemical
compounds are often represented as SMILES strings105, which can be processed by LLMs
after converting them into text. However, such conversions may lose information about
the 3D structure of the compounds, which is critical for many chemical and biological
applications. Similarly, genomic sequences are often represented as strings of letters, which
can be processed by LLMs. However, such representations may lose information about
the spatial organization of the genome, which is also critical for many biological
applications. Hence, developing new methods to represent and process domain-specific
data modalities is crucial for advancing scientific discovery with LLMs. This includes
developing new LLMs that can directly process these data modalities without requiring
conversion, as well as developing new methods to convert these data modalities into text
while preserving as much information as possible. This is particularly important for
scientific domains where the data is highly complex and multi-dimensional, such as
genomics, proteomics, and systems biology106.

[PAGE 16] Experimentation and Automation
The experiment is a critical part of all research steps, including making observations and
validating the hypothesis. Both humans and LLMs need external tools to implement
experiments. Specifically, this involves calling external functions or directly generating
and running code. LLMs that have been fine-tuned for tool usage 85,96 can generate
structured entities (often in JSON) that contain the function name and inputs to be
implemented by external functions. These functions are versatile and can include simple
calculations, laboratory control functions, external memory, requests for assistance from
human scientists, etc. LLMs can also direct programming by generating and running code
for complex experiments requiring fine-grained control or enhancing the calculation
abilities of LLMs107,108. Beyond this, generated programs can also call other functions or
be saved into a function library, enabling the combinatory development of complex
actions109.

For complex experiments, planning becomes important, which involves setting up
an objective and decomposing it into practical steps. This is critical to solving complex
tasks while sustaining coherent behaviour. While the planning capabilities of LLMs are
questioned in many studies175, certain tools and methods still demonstrate valuable assistance.
The chain-of-thought (CoT)44 method significantly improves various tasks by
decomposing a question into steps. In complex tasks with more steps, where LLMs seek
long-term objectives and interact with environments, they can generate plans in natural
language based on given objectives110. It is also important to adapt to observations and
unexpected results. For this reason, methods like Reflexion111, ReAct112 combine the CoT
and planning, dynamically update its plans, manage exceptions, and utilizes external
information. And it also overcomes hallucination and error propagation in the chain-of-
thought.  Integrating LLMs with traditional planning methods can enhance success rates, reduce research time, and provide more flexible interaction during plan development176.

[PAGE 17] Automation
Automation is a significant aspect of LLM-assisted research, serving as a key
contributor to accelerating scientific discovery. Automation involves repetition and
feedback loops113. LLMs can be seen as a function – prompt in, reply out – with human
users as the driving force behind making LLMs produce output. To automate such a
system, the key is to replace the human user. For instance, an LLM-powered chemical
reaction system can perform Suzuki and Sonogashira reactions by incorporating an LLM-
based planner, which replaces the human user. The planner reasons through the given task
and determines the next steps, including searching the internet for information on both
reactions, writing Python code to calculate experimental parameters, and finally calling
external tools to conduct the experiments. At each step, the results, i.e., the search outcomes
and calculation results, are fed back to the LLM-based planner to automate the system66.
Another approach is to replace the human user with multiple LLMs and allow them to
communicate with each other114. Since such automation is not fully hard-coded and the
core of this automation is also an LLM, they can exhibit some emergent behaviour66,114,
adapting unexpected situations, which is vital for exploring new knowledge. Specifically,
automated LLMs can help in three dimensions of scientific discovery: scaling, enhancing,
and validation.

[PAGE 17] Scaling: 
Automated LLM agents can scale previously challenging experiments for
large-scale studies. Examples include inferring underlying functions from input-output
pairs115. The LLMs perform multiple rounds of trial and error to find the correct function.
This approach can extend to neuron interpretation of GPT-2 using GPT-4, which has
billions of parameters116. This method involves two layers of loops: the trial-and-error
process and the application to all the billions of neurons117,118. Both layers are time-
consuming for human scientists, and LLMs make such studies feasible. Another example
is when LLMs are used to infer the functionality of human brain voxels from fMRI
activation data, their proposed functions are first validated by calculating the probability
of observing the activation data given a specific functional hypothesis. Subsequently, the
hypotheses with the high probability are selected to aid in generating new hypotheses and
improving overall performance100. Lab experiments can also be parallelized with the help
of LLMs, which further accelerate the experiment speed and increase the potential for
scaling the scope of experiments110,119.

[PAGE 18] Enhancing: 
The aforementioned scientific methods, such as hypothesis generation,
experiments, and observations, can all be enhanced by automation. One direct application
is using LLMs as optimisers: by iteratively providing historical solutions and scores, LLMs
can propose new solutions and ultimately achieve superior performance120. In both the
hypothesis-validation loop and in experimental trials, failed cases constitute valuable
feedback. When evaluators and reflection are incorporated into the workflow, LLMs can
improve their decisions, showing significant performance improvements compared to
simply using LLMs111. Iteration can also enhance the hypothesis generation stage. By
comparing hypotheses with existing literature on related topics, LLMs can iteratively
improve novelty by using this literature as a source of negative examples121. Another
enhancement comes from accumulating knowledge, which is critical to research success.
Many exploration tasks require accumulating knowledge and developing new strategies
based on this knowledge122. For example, Voyager109 uses GPT-4 to examine the space of
the Minecraft game. This study consists of three main parts: an automatic curriculum to
propose exploration objectives, an iterative prompting mechanism to write code to control
the game, and a skill library to accumulate the knowledge and skills gained during the
exploration, which is then reused in future explorations. Equipped with all these
components, this LLM-assisted explorer can explore the game more efficiently. While
game environments in silico are a non-trivial departure from real worlds in situ, they are
not too dissimilar from the biochemical simulation engines123 that scientists rely on today.
However, the current “physics" engines in-game systems are still inconsistent with the
physical sciences, and new simulation software technologies are needed to allow for any
AI-based exploration of multi-physics environments113. From a macroscopic viewpoint,
scientific discovery can also be considered a quality-diversity search process124,125, and this
Voyager study has shown how LLMs can assist diversity search in a new way by proposing
objectives, iteratively solving problems, and contributing to and utilising literature (skill
library).

[PAGE 19] Validation: 
Automated LLM agents are critical for validating hypotheses. Beyond
scaling and enhancing performance, research often involves multiple rounds of the
hypothesis-experiment loop to meet scientific discovery's rigor and safety requirements.
This loop is essential given the probabilistic nature of LLMs126 and the hallucination
problem of LLMs127,128. Experiments show that repeatedly verifying the results from
LLMs' observations and proposed hypotheses increases the likelihood of obtaining reliable
results129,130. A promising direction is leveraging formal systems to validate results and
hypotheses by translating generated hypotheses and answers into formal languages, such
as LEAN or Prover9131,132. For instance, in131, LLMs first generate multiple answers.
These answers are then translated into the LEAN language and verified using the LEAN
compiler to choose the correct responses. With these filtered answers, LLMs can aggregate
toward a final answer. Another example involves using Python code to aid validation.
While general programming languages are often not considered formal systems, they can
still disprove certain hypotheses. In133, LLMs were prompted to solve the Abstraction and
Reasoning Corpus (ARC) tasks134, which involve identifying underlying laws and making
predictions based on new initial states. LLMs initially propose hypotheses, which are then
translated into Python code. This Python code is used to disprove incorrect hypotheses.
Although these non-formal systems cannot fully validate hypotheses, they partially
perform validation and improve predictive accuracy. While humans could also conduct
such translation and validation processes, the high speed of hypothesis generation by LLMs
makes automated approaches more suitable. A limitation, however, is the reliance on LLMs
to translate hypotheses into formal languages, which may introduce errors in the process.
This suggests the need for caution when interpreting results, even if they have been tested
using formal systems.

[PAGE 19] Expanding the Literature Review and the Hypothesis Horizon
In brief, advancing beyond current knowledge includes using LLMs to explore unknown
territories in knowledge space, encompassing human discoverable, human-machine
discoverable, non-human-machine discoverable, and the entirety of the knowledge space,
as illustrated in Figure 2. Namely, to perform hypothesis generation and develop predictive
models of more complex systems. Hence, can LLMs do open-ended Exploration of the
Hypothesis Space? Can LLMs also explore complex environments in an open-ended way?
These are open-ended challenges addressing the (unknown) limits of the capabilities of
LLMs and Generative AI.

Proposing hypotheses is a crucial step in scientific discovery, perhaps the most
important since it often involves significant creativity and innovation. Scientists propose
hypotheses to explore unknown topics or address research questions. This step often
involves novel ideas, recombining existing literature, and key insights. Experiment design
and subsequent verification are based on these hypotheses. Thus, hypothesis proposing is
a central step that connects observation and experiments.

Evidence indicates that LLMs can propose novel ideas, such as drug
combinations135, with designed prompting, thus underscoring the importance of prompting,
as discussed previously. An example is the use of LLMs for drug discovery: In135 LLMs
are prompted to propose novel combinations of drugs for treating MCF7 breast cancer cells
while incorporating additional constraints such as avoiding harm to healthy cells and
prioritizing FDA-approved and readily accessible drugs. The experiment results
demonstrate that LLMs can effectively propose hypothetical drug combinations. More
advanced techniques can