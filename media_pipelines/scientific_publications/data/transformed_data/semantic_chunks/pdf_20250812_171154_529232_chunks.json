{
  "file_metadata": {
    "text_file": "data/transformed_data/text_extraction/pdf_20250812_171154_529232_extracted_text.txt",
    "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
    "pdf_filename": "pdf_20250812_171154_529232.pdf",
    "file_size": 30743,
    "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
    "journal": "Unknown",
    "doi": "Unknown",
    "year": "2025",
    "title": "Memory via Temporal Delays in weightless Spiking Neural Network",
    "confidence_score": 0.5,
    "document_type": "scientific_paper"
  },
  "chunks": [
    {
      "text": "A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection.",
      "section": "Abstract",
      "topic": "Weightless Spiking Neural Networks",
      "chunk_summary": "This paper introduces a weightless spiking neural network prototype where memory is encoded in timing delays between neurons, trained using Hebbian STDP.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "The role of synaptic plasticity in learning and memory is an open question in neuroscience. The current understanding of synaptic modification dates to Hebb's postulate [1], which states that if cell A persistently excites cell B, cell B will be more easily excitable by A (colloquially, neurons that fire together, wire together). This postulate and its notion of past activity remodeling synapses is the basis for modern research done in neural computation. This perception may have also influenced modern computer science to use numerically weighted connections during learning, in addition to weighted connections being a mathematical convenience for modulation.",
      "section": "Introduction",
      "topic": "Synaptic Plasticity and Learning",
      "chunk_summary": "Hebb's postulate, emphasizing the strengthening of connections between co-firing neurons, forms the basis of current understanding of synaptic plasticity and has influenced the use of weighted connections in computational models.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "An alternative mechanism for manipulating information and storing memory is regulating the speed of spike propagations between neurons [2], [3]. Myelin, although only present in vertebrates, is a crucial element of neurons in the central nervous system. Similarly in non-vertebrates, the thickness of the axon can change the speed of spike propagations (other analogous mechanisms to the functionality of myelin in invertebrate neurons may also be found). Myelin is a fatty tissue that surrounds nerve axons and acts as electrical insulation. This insulation allows the electrical signals (spikes) to travel further and with less degradation. More importantly, myelinated axons can propagate spikes much faster and with less energy consumption [4], [5] compared to unmyelinated axons. Experimental findings on the nervous system have shown that delay times can vary between organisms. For example, Sprague-Dawley rats have delays of 1 to 17 ms [6], rabbits 1 to 32 ms [7] and cats 1 to 30 ms [8]. Furthermore, the amount of myelin around axons is not constant and can be increased or decreased during myelination and demyelination [8]-[10] respectively.",
      "section": "Introduction",
      "topic": "Myelin and Spike Propagation Speed",
      "chunk_summary": "Myelin, by acting as insulation, allows faster and more efficient spike propagation, and the amount of myelin can change, affecting delay times between neurons.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "In this paper, we set out to explore different architectures for plasticity in computational learning models, with a focus on biological plausibility. We emphasize biological plausibility because we believe the empirical success of biological networks can provide insights to developing high performance learning models, as well as further our understanding of neural computation. We present a proof of concept, that timing of spike transmission alone can be used for computations in biology. In our simple neural network, which we call a weightless spiking neural network (WSNN), learning occurs not with weight adaptation but with spike propagation times. We show that our weightless networks can be trained on the MNIST dataset in an unsupervised manner, resulting in comparable performances with unsupervised weight-based models. The purpose of this paper is to suggest an alternative to weight-based networks using delays as a means to modulate synchrony and spike intensity.",
      "section": "Introduction",
      "topic": "Weightless Spiking Neural Networks (WSNNs)",
      "chunk_summary": "This paper proposes WSNNs, where learning occurs through spike propagation times rather than weight adaptation, offering a biologically plausible alternative to traditional weighted networks.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Neurons in biology are believed to operate in an asynchronous manner, i.e. they can process and generate signals at any point in time, without the need for a clock which dictates their activity. This has led to the popular belief that the timing between spikes may be used as an additional dimension for information encoding, memory storage, memory capacity, and a possible mechanism for decision-making. In decision making, the timing between spikes can be used as a faster mechanism to react to external stimuli, a quality that can give the organism an evolutionary advantage for survival. Several papers propose to use the scheme of Time To First Spike (TTFS) [11], [12] for classification tasks while providing the minimal possible information to the network. This in turn makes the network significantly faster as there is less information to process.",
      "section": "Previous work",
      "topic": "Time To First Spike (TTFS)",
      "chunk_summary": "The timing between spikes, particularly TTFS, is explored as a mechanism for information encoding, memory, and faster decision-making in biological and artificial neural networks.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Other approaches use the frequency of communication between neurons as clues for storing information and performing computations using oscillations with networks based on timing and delays. Examples for such a network can be found in Izhikevich [13],[14] that present the polychronization network model. The model was randomly initialized and exhibited self-organization into groups of neuron clusters with different firing patterns and shifts between different firing oscillations. The author of [13] suggests that the number of different groups that generate unique patterns exceeds the number of neurons in the network, evidence for the high memory capacity of the system. Wright et al [15] demonstrated an algorithm that modifies the mean and variance of postsynaptic spikes during training. Using this method, the authors were able to recognize a temporal sequence of spike trains in both supervised and unsupervised learning schemes.",
      "section": "Previous work",
      "topic": "Oscillations and Spike Frequency",
      "chunk_summary": "Studies using oscillations and spike frequency, such as Izhikevich's polychronization model, demonstrate the potential of timing and delays for information storage and computation in neural networks.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Lastly, the authors Zhang et al [16] proposed a supervised learning rule that updates both the weights and delays of a synaptic connection. Using this learning rule, the authors trained a network on the TIDIGITS corpus, and concluded that the success of their model demonstrates that a combination of weights and delays can surpass the standard weighted network. The above works show the latent potential of using delays in the connections. Our approach in this paper is taking another step forward by presenting an unsupervised learning rule that also works on static data and operates on the level of the individual neuron and their weightless connection.",
      "section": "Previous work",
      "topic": "Weights and Delays",
      "chunk_summary": "Previous work by Zhang et al. showed that combining weights and delays can improve network performance, while this paper focuses on a purely delay-based, unsupervised learning rule for weightless connections.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "The networks in this paper are built with BindsNET [17], a flexible and open source framework for experimenting with spiking neuron architecture. We propose a biology-inspired myelination process to modify the axonal delays of a 2-layered, feed-forward, Spiking Neural Network (SNN) with no synaptic weights. The network uses a delay-encoded input, and an output layer which utilizes a TTFS scheme. Competition in the output layer is enforced in two dimensions: 1. A sliding threshold that tunes the firing activity of each neuron 2. By a Winner-Take-All mechanism (WTA) where the first spike in the output layer prevents all other neurons in the output layer from firing. This is simulated as a full lateral inhibitory connection similar to [18], [19]. A linear decoder (readout) analyzes the outputs of each sample, and statistically assigns output neurons to input MNIST digits. Note that the linear decoder applied directly to the MNIST dataset reaches an accuracy of 62% by itself. For the sake of computational costs, simulations were stopped after the first emitted spike(s) on the output layer.",
      "section": "Method",
      "topic": "Network Architecture and Training",
      "chunk_summary": "The paper uses BindsNET to implement a 2-layer weightless SNN with delay-encoded input, TTFS output, and a competitive mechanism using sliding thresholds and WTA.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Output neurons in the network are Leaky Integrate and Fire neurons (LIF) similar to [18], [19] with an adaptive firing threshold. A unique feature of the spiking neurons is their ability to process spatio-temporal signals and transmit them in spike form. These neurons can take advantage of their multidimensional capabilities by encoding information propagated through the network in the form of precise spiking times, ordered spiking sequences (bursts), and spike frequency. Leaky Integrate and Fire Neuron (LIF) Equations: dv/dt = (vrest - v) + ge(Eexc - v) + gi(Einh - v)",
      "section": "Network architecture",
      "topic": "Leaky Integrate and Fire (LIF) Neurons",
      "chunk_summary": "The network uses LIF neurons with adaptive firing thresholds, capable of processing spatio-temporal signals and encoding information through spike times, bursts, and frequency.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "One common issue with SNN's is the oversaturation of signals. For example, if the firing thresholds for neurons in a network are too low, they will become highly sensitive to incoming input. This will result in excessive spiking from neurons in the network, subsequently making information gain very difficult. This is comparable to a seizure seen in biological networks. To combat this, the LIF neurons used in the following experiments utilize a threshold adaptation mechanism which adjusts the sensitivity of the neuron to the quantity of incoming spikes. To do this, we utilized two variables, θo and θi, to modulate the value of the membrane threshold Ti at timestep i: Ti = θo + θi In this formula, θo represents a constant base value for the membrane threshold which does not change during runtime. The variable θi acts as a modulator to the membrane threshold, and adapts according to behaviors observed at each timestep i. As more outgoing spikes are produced by the neuron, θi increases in value and thus increases the total value of the membrane threshold. The value of θi is subject to a constant decay at each timestep, allowing the neuron to also adapt to scenarios where spiking is sparse. This threshold adaptation helps neurons to maximize information gain and encode inputs in a more meaningful way, by avoiding signal saturation and over-sensitivity to input intensity changes. Moreover, we hypothesize that adaptive thresholds add a form of temporal memory to the neuron, since certain repeated or recognizable patterns will trigger varying amounts of activity in different regions of the network. This is an ability that we attempt to capitalize on in the learning algorithm. The results of our experiments and those mentioned in the background section support this claim. dθi/dt = -decay * θi + S+ To optimize hyper-parameter searches, we increased the competition between neurons by forcing at least one of them to have a theta value of zero at all times: θi(t+1) = θi(t) - min(θ) The learning rule used was a homeostasis process between sliding threshold and Hebbian STDP (as a result from the WTA). Together, they create a competition mechanism that tunes the spike delays.",
      "section": "Network architecture",
      "topic": "Adaptive Firing Thresholds",
      "chunk_summary": "The network employs adaptive firing thresholds (Ti = θo + θi) to prevent signal oversaturation, where θi dynamically adjusts based on spiking activity and decays over time, enhancing information gain and adding temporal memory.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Inter-neurons delay plasticity is a popular, biologically inspired learning rule which has been successfully used to train Spiking Neural Networks (SNN) in both supervised and unsupervised paradigms. As opposed to traditional Artificial Neural Networks (ANN), signals in SNN's are propagated in the form of spikes emitted at precise times. Inspired by mechanisms observed in biological networks, STDP (Hebbian or anti-Hebbian) has been widely adopted to train the relative weights of SNN's. Currently, most SNN designs use a static delay time between neuron layers (usually the length of one-time step used in the simulation), if any at all. For our WSNN's, we've chosen to use an emulation of myelin insulation which allows for dynamic adjustments of delay times. Because myelination can modulate the timing of spikes, using it in conjunction with STDP can tune neurons to spike in synchrony. The synchronized firing of neurons has shown to be a critical mechanism for learning [21]-[24] and decision making [25] in the human brain, and has also proven to be effective in previous works with weighted SNN's. As described previously, weighted networks are reasonable replications of biological networks, but are still widely regarded as biologically implausible. Thus, we have chosen to attempt replacing such weights with time delays.",
      "section": "Network architecture",
      "topic": "Spike Timing Dependent Plasticity (STDP)",
      "chunk_summary": "The paper uses a biologically inspired STDP rule, emulating myelin insulation, to dynamically adjust delay times between neurons, promoting synchronous firing for learning and decision-making.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Describing prior work",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "In mammalian brains, STDP is one of the known synaptic adaptation mechanisms, which dynamically modifies the properties of afferent neurons. In general, STDP is a learning process that follows the principles of Hebbian learning (see Intro.) through the utilization of long-term potentiation and long-term depression. In previous works with SNN's trained on STDP, changes in weights were primarily responsible for the adaptation of information gain through synchronization. Intuitively, if the synaptic connection between a source (A) and target neuron (B) sees a weight increase, then the probability of B firing as a result of activity from A increases (note that the inverse of this statement is also true, where depressed connections reduce the probability). A similar conjecture could be made for dynamic delays: if the synaptic connection between A and B sees a delay decrease, then the probability of A firing as a result of activity from B increases. For our experiments, we have chosen to adopt this interpretation of STDP to train our networks. Like most neural networks in machine learning, Our WSNN models use several hyperparameter values which can be optimized for performance. For this process, we chose to use a PSO (Particle Swarm Optimizer) algorithm. Optimal hyper parameters values are reported in Table 2: • Additive Decay: a small constant continuously added to the normalized synaptic delays. • Encoding Time: the maximum delay (in ms) for encoding the input signal using TTFS. • Learning Rate: the maximum magnitude of STDP changes to the delay connection depending on the timing of the stimulus. • Max. Synaptic Delays: the longest possible inter synaptic delay. • Neuron Threshold: the base neuron threshold value θo. • Delay Norm: the normalization value of the synaptic delays, ranging from 0 to 1. • Spike Intensity: the voltage increase applied to a neuron for each of its incoming spikes. All spikes in the network produce the same spike intensity. • Theta Plus: the constant voltage increase of a neuron's membrane threshold after an incoming signal is received. This constant voltage increase is represented by θi for neuron i. The proposed modified Hebbian STDP rule applies to the transmission delay of the axons, according to the delay between afferent and efferent spikes Δt: dmij/dt = -µ * e^(-Δt/τ) , if Δt > 0 where mij represents the transmission delay of the axon between cell i and cell j, and µ is the learning rate. Note that Δt is computed at the neuron level, between incoming spikes and outgoing spikes, regardless of the incoming spike transmission time. The transmission delay (expressed in number of dt) of a synapse is evaluated as: dij = [max delay * mij] A delay buffer in each synapse simulates the variable transmission delay between neurons.",
      "section": "Network architecture",
      "topic": "Hebbian STDP and Hyperparameters",
      "chunk_summary": "The paper adapts Hebbian STDP to modulate delays, using a PSO algorithm to optimize hyperparameters such as additive decay, encoding time, learning rate, max synaptic delays, neuron threshold, delay normalization, spike intensity, and theta plus.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Similar to most STDP-based SNN, we apply a normalization mechanism to the transmission delay, reflecting the locally limited biological resources to construct the myelin sheath: m(t+1)ij = Mnorm * (Σ m(t)ij / m(t)ij) A slow constant increase of the transmission delay is also simulated, reflecting a natural degradation process of the myelin sheath (demyelination). This allows neurons to focus on the most common patterns, while less common patterns are slowly forgotten (known as catastrophic forgetting): Mij(t+1) = min(mij(t) + c, 1) Figure 1 illustrates the full connection from input to output layer, with variable numbers of the time step for each connection.",
      "section": "Network architecture",
      "topic": "Delay Normalization and Demyelination",
      "chunk_summary": "The model incorporates delay normalization and a simulated demyelination process, allowing the network to prioritize common patterns and mitigate catastrophic forgetting.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "For our experiments, we used the MNIST dataset to test our networks. Preprocessing of samples consisted of normalizing image data to reduce imbalance between dark and bright images. As samples are passed into the WSNN, input neurons fire only one spike using a TTFS scheme [26] at a time proportional to the pixel's brightness/intensity. Brighter pixels will spike earlier, with black pixels emitting no spikes at all. The encoding time window is chosen in the range 20 to 32ms. The motivation for using TTFS is the reduced number of spikes, and reduced simulation time. [27] showed that models using TTFS-encoded images perform similarly to those using Poisson-encoded images (Figure 2), in network topologies similar to ours.",
      "section": "Input encoding",
      "topic": "MNIST Dataset and TTFS Encoding",
      "chunk_summary": "The MNIST dataset is used with TTFS encoding, where input neurons fire a single spike at a time proportional to pixel brightness, reducing spikes and simulation time.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Networks were evaluated using 1000, 2000, 3000, and 4000 neurons, and their performances are shown in Figure 5. Accuracies were reported after 1 epoch, as further training led to negligible accuracy increases. During training, the average number of simultaneous output spikes decreased from 20 to ~2.5, with correctly predicted digits leading to fewer simultaneous spikes (about 30% less). Additionally, time to the first output spike decreased, with correctly predicted digits leading to shorter output spiking times (about 6% shorter) as shown in Table 1 and Figure 3. The network schematic can be shown in Figure 1 and results can be found in Figure 4, Figure 5, Figure 7, and Table 2. By using only Time To First Spike (TTFS) and delay-learning, our model takes less time (iterations) to compute and has less spikes to predict when compared with equivalent weight-based models using Poisson encoding [18] see Table 1. Final delays are quantizable and can be encoded with only 5 bits (maximum synaptic delay < 32-time steps). The memory footprint for a 4000-neuron model with 5-bit encoded delay and 28x28 inputs is about 2 MB.",
      "section": "Results",
      "topic": "Network Performance",
      "chunk_summary": "The WSNN, evaluated with varying neuron counts, showed improved accuracy with fewer simultaneous output spikes and shorter spiking times for correctly predicted digits, outperforming weight-based models in terms of computation time and spike count.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Presenting new results",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Looking at the misclassified MNIST (see Figure 6) 7 digits clearly shows the limitations of our model. With enough misplaced bright pixels, a sample's input pattern can induce early firing of a wrong output neuron. Penalizing images for having excess white pixels was attempted but did not improve the model. We believe that this issue could be mitigated by adding another layer of WTA, or another layer that uses TTFS encoding scheme with inhibition. As seen in [28], using twin excitatory and inhibitory input layers can add the computational power to distinguish between similar input patterns, but will also require a more complex learning mechanism, i.e., Hebbian STDP with two simultaneous input layers. In classic weight-based SNN models, the temporal dimension is discretized for the whole network and represented by the dt parameter. This discretization helps to compute the voltage activity of neurons in a more computationally efficient fashion (more efficient than solving the differentials in the Network Architecture section), but will also reduce the accuracy of spike times. Intuitively, a higher granularity will provide a closer replication of a continuous time interval, and thus a higher accuracy. Weights themselves are unaffected by the discretization of time because they only modify the magnitude of the output, not the timing; however, the same cannot be said for delays. Delays directly modify the outgoing times of spikes, and must be measured using multiples of dt. If the value for dt is too large and inaccurate, then the delays will suffer as well. As a result, we believe WSNN's have an inherent reduction in precision due to their dependence on delays. In our model, the loss of precision due to time quantization was hard to estimate precisely, since every attempt with a different maximum TTFS encoding value for the delays needs a complete reconfiguration of the other parameters. This needs further examination.",
      "section": "Limitations",
      "topic": "Model Limitations",
      "chunk_summary": "Limitations include misclassification due to misplaced bright pixels, time discretization affecting delay precision, and sensitivity to hyperparameters, particularly firing thresholds.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "The TTFS-encoded input may prove more beneficial in the case of a dataset containing a more evenly distributed set of pixel values, whereas black and white pixels comprise over 90% of all pixels in the MNIST dataset. Further research should include less contrasted inputs, such as CIFAR. Another issue with this model is its sensitivity to hyper-parameter values. One potential issue is that using TTFS does not guarantee an output spike will occur. This can happen when firing thresholds for neurons are set too high, resulting in too little spiking activity to reach the output layer. This limits the perimeter of the valid parameter space, as we need at least one output spike for the model to make a prediction. The possible values for plasticity parameters (neuron's thresholds and delay-STDP learning rates) are therefore limited. The drawback of this approach is that the current neuronal model solely relies on the time aspect of the first spike from the input and can miss some of the existing information in the static or timeless data. Moreover, the presented model uses a winner-take-all architecture [18], [19] to reduce the excessive spikes that are left in the system after the winner has been chosen. To alleviate the excessive spike in this model, IF (Integrate and Fire; no ‘leaking') neurons can replace LIF with no loss in accuracy. This can be tested by setting the neuron's decay speed to infinity.",
      "section": "Limitations",
      "topic": "TTFS Limitations and Hyperparameter Sensitivity",
      "chunk_summary": "The limitations of TTFS encoding, particularly its dependence on evenly distributed pixel values and sensitivity to hyperparameters, are discussed, along with potential improvements using Integrate and Fire neurons.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "We presented a proof of concept for a time-based model which uses only the timing of the spikes and the firing thresholds to perform classical classification tasks. To emphasize the fact that connections do not have numerical weights, we call our model Weightless Spiking Neural Networks (WSNN). These networks utilize a learning strategy that is consistent with observations in neuroscience that suggest spike timing conveys information for learning and decision making [2], [3]. The successful learning capability of our model suggests that the mechanism which changes the speed and the conductances of spike transmissions can also be part of learning and event encoding in neuronal models. Moreover, changing the spike transmission timing seems to be a natural continuation for known learning rules that rely on the timing and frequency of stimuli. In the proposed WSNN, synaptic adaptations are done using the standard Hebbian STDP rule to adjust the communication timing between neurons. The result of the proposed learning scheme is that the information encoded in the delay connection can use both the rate of the spikes (rate code) and the timing of the spikes (TTFS) to convey and encode information. Additionally, this network uses learning rules which utilize both the temporal and spatial dimensions of SNN's, adjusting both the quantity (neuron adaptive threshold) and timing (synaptic delays) of the spikes. In comparison, prior models only dealt with the spatial aspect by using synaptic weights to influence only the total spike quantity.",
      "section": "Discussion",
      "topic": "WSNNs and Biological Plausibility",
      "chunk_summary": "The WSNN model, using spike timing and firing thresholds for classification, supports the biological plausibility of using spike transmission speed and conductance for learning and encoding.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Motivated to address the biological implausibility of weighted connections, the model presented in this paper shows that the manipulation of communication delays between neurons is a viable method for training SNN's and can be used in replacement of weights. This learning method also benefits from being compatible with Hebbian learning rules, such as STDP, which were originally designed to operate on a continuous-time interval. As such, rules like STDP were a natural and intuitive method of modulating delays. Using delay-based connections can benefit threefold: shorter decision times, fewer computational resources, and reduced energy consumption. Since the network operates on TTFS, the decision can be reached significantly faster (see Table 1). The computational benefits of using delayed connections can be addressed in two ways: number of computational operations and memory utilization. The network uses only binary signals, meaning significantly fewer operations and resources are needed to store, train, and simulate the network. Altogether, a shorter run, fewer computations, and less resources makes this model ideal for low resource environments like those found in biology.",
      "section": "Discussion",
      "topic": "Advantages of Delay-Based Connections",
      "chunk_summary": "Delay-based connections offer advantages in terms of shorter decision times, reduced computational resources, and lower energy consumption, making them suitable for resource-constrained environments.",
      "position_in_section": "Middle",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "Addressing the time aspect of connections between neurons as a means to prioritize communication between cells can open new possibilities for learning. Moreover, addressing the need to adjust the time delays between neurons opens up new dimensions for training algorithms. Furthermore, most of the information around us is time-driven data where time is an integral dimension to the data, and itself contains information. Considering that traditional neuronal networks do not excel with dealing with the time dimension and since the use of Hebbian learning rules like STDP and others as a training algorithm in spiking neuronal networks didn't yield the performance that we hope. Seems reasonable to assume that more properties in conjunction with the Hebbian rules are needed. Adding the time aspect to the connection has the potential to that edge. Future work will focus on models with more layers and higher neuron complexity that benefit from the timing aspect of the delay connections. Furthermore, using time-driven data such as video or sound can use the true potential of the delay network that uses the time aspect in the information to make a decision.",
      "section": "Discussion",
      "topic": "Future Directions",
      "chunk_summary": "Future work will explore multi-layered models with complex neurons and time-driven data to fully leverage the potential of delay-based networks for learning and decision-making.",
      "position_in_section": "End",
      "certainty_level": "High",
      "citation_context": "Drawing conclusions",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    },
    {
      "text": "We would like to thank Peter Marathas for his help in the writing and programming. The simulation done in this paper where run on two HPC's, the authors acknowledge the Tufts University High Performance Compute Cluster (https://it.tufts.edu/high-performance-computing) and the Extreme Science and Engineering Discovery Environment (XSEDE) Bridges GPU Artificial Intelligence at Pittsburgh Supercomputing Center through allocation CCR200032. M.L. gratefully acknowledges support of The Elisabeth Giauque Trust and the Barton Family Foundation.",
      "section": "Acknowledgment",
      "topic": "Acknowledgments and Funding",
      "chunk_summary": "The authors acknowledge contributions from Peter Marathas and support from Tufts University HPC, XSEDE Bridges GPU AI, Elisabeth Giauque Trust, and Barton Family Foundation.",
      "position_in_section": "Beginning",
      "certainty_level": "High",
      "citation_context": "None",
      "page_number": null,
      "pdf_filename": "pdf_20250812_171154_529232.pdf",
      "original_filename": "pdf_20250812_171154_529232_extracted_text.txt",
      "authors": "Haneul Hazan, Simon Caby, Christopher Earl, Hava Siegelmann, Michael Levin",
      "year": "2025",
      "journal": "Unknown",
      "doi": "Unknown"
    }
  ],
  "gemini_response": "",
  "processed_at": "2025-08-12T17:17:46.899985"
}