[PAGE 1] Thoughts and thinkers: On the complementarity between objects and processes

ABSTRACT
We argue that "processes versus objects" is not a useful dichotomy. There is, instead, substantial theoretical utility in viewing "objects" and "processes" as complementary ways of describing persistence through time, and hence the possibility of observation and manipulation. This way of thinking highlights the role of memory as an essential resource for observation, and makes it clear that "memory" and "time" are also mutually inter-defined, complementary concepts. We formulate our approach in terms of the Free Energy Principle (FEP) of Friston and colleagues and the fundamental idea from quantum theory that physical interactions can be represented by linear operators. Following Levin (2024) [30], we emphasize that memory is, first and foremost, an interpretative function, from which the idea of memory as a record, at some level of accuracy, of past events is derivative. We conclude that the distinction between objects and processes is always contrived, and always misleading, and that science would be better served by abandoning it entirely.

Thought is itself the thinker.
[William James [1] p. 401]

[PAGE 1] 1. Introduction
Is it advantageous to think of the world in terms of processes instead of objects, as suggested by Whitehead [2] among others? Is it, in particular, advantageous to think of organisms as processes instead of objects? Does doing so resolve, or avoid, the problem of identity over time, or does it merely reformulate this problem? Does adopting a process view of biology, for example, help us understand what it means to say that an individual human is an individual, something that maintains a single identity from birth to death, despite continuous change in properties at multiple levels? Does it help us to understand what it means to say that *Homo sapiens sapiens* has remained the same subspecies for the past *ca*. 250,000 years? Does it help us decide whether a clonal colony of *E. coli*, or a plate full of adult planaria that are all regenerated from the fragments of a single planarian cut up in a laboratory, comprises a single "individual organism" or many? And, reaching forward towards empirical utility in fields like regenerative medicine and bioengineering, does it facilitate interesting new discoveries, capabilities, and research programs?

The notations employed in much of mathematics suggest – though do not require – an interpretation in terms of processes: writing 'f(x)', for example, suggests a process f acting on an object x, while the notation 'A → B' suggests a process, indicated by '→' that converts A into B, or perhaps moves something from A to B. The current foundational language for mathematics is category theory. A (small) category comprises a set of O objects and a set M of morphisms, or arrows, such that:

1. for every object o in O, there is a morphism Id in M, the “identity morphism for o", such that Id: o → o, and
2. for all objects o1,o2,o3 and morphisms m12:o1 → o2 and m23:o2 → o3, there is a morphism m13:o1 → o3.

As Adámek, Herrlich and Strecker remark in their popular textbook [3], replacing every object with one of its identity morphisms (they are not in general unique) allows any category-theoretic statement to be made just in terms of morphisms. The Yoneda Lemma and the notion of object-free categories formalize this fact. Hence mathematics, in a deep sense, does not need objects. It just needs morphisms, some of which preserve an abstraction called "identity" and others of which do not.

Similar moves are made ubiquitously in physics. Quantum theory replaces "properties" of objects – position, momentum, spin, etc. – with operators that yield values of those properties when deployed by an observer [4]. Quantum field theory replaces "particles" with creation, destruction, and number operators acting on fields that permeate spacetime. Feynman's diagrams famously replace "particles" with sums of processes that yield equivalent results when interrupted by observations. Even "quantum systems" can be replaced with the operational procedures required to identify them [5]. An operational procedure is a process that may or may not preserve the identity of either the system that it acts on or the system – conventionally called an "observer” or “perceiver" – that implements it.

[PAGE 2] The idea that objects are constructed by perceivers is foundational in 2nd-order cybernetics [6,7]. This idea was introduced into evolutionary and developmental biology by Maturana and Varela [8], where it became the notion of the joint "enaction" of an object by an organism and its environment. It was introduced into perceptual psychology by Gibson [9] as the idea that objects and their properties are "affordances" of an environment for a specific, behaving organism. In all of these formulations, perceived "objects" are emergent from the joint activity of a perceiver and its environment, not fundamental "furniture of the world" as sought by traditional ontologists [10].

All of these process-oriented ways of thinking, however, employ object-like concepts in their formulation. One can think of a morphism m12: o1 → o2 simply as an ordered pair (o1,o2); the ordered-pair notation suggests an object, not a process. Quantum-theoretic measurement operators must be deployed by observers to yield results [11]; observers are themselves quantum systems, i.e. objects. When such systems are replaced by identification criteria, these criteria must be implemented, again by a system – an object - acting as an observer. Enaction requires two systems, or objects, an organism and its environment, which interact in some well-defined way as discussed below. Affordances are *for* something, an organism, hence an object, and are *offered by* something, the organism's environment, another object. Objects, in other words, seem to be required after all.

We will argue here that "processes v/s objects" is not a useful dichotomy in either physics or the life sciences. There is, instead, substantial theoretical utility in viewing “objects" and "processes" as complementary ways of describing persistence through time, and hence the possibility of observation and manipulation. This way of thinking highlights the roles of memory as an essential resource for observation and of time as an essential resource for action, and makes it clear that "memory" and "time" are also mutually inter-defined, complementary concepts that both rest on the underlying notion of an "identity" that persists. We formulate our approach in terms of the Free Energy Principle (FEP) of Friston and colleagues [12-15,17,18], noting the close relationship between the FEP and the Holographic Principle (HP) of physics [19-22] and that the fundamental idea of both of these principles – that information passing through a surface can be represented as being encoded on that surface – is implicit in the Divergence Theorem, one of the fundamental theorems of vector calculus, first formulated in the early 19th century [23]. The FEP characterizes all time-persistent physical systems as implementing a process, active inference [24,25], which is a combination of reacting to (learning from) and acting on (manipulating) their environments. As a generic description of persistent physical systems as information-processing systems, the FEP provides a uniform way of thinking about information and memory across biological scales, from molecular networks in morphogenesis to evolutionary dynamics.

Following a brief review of the FEP and its conceptual underpinnings in §2, we apply this view of objects and processes as complementary to evolution and development in §3, asking "what evolves?" and "what develops?" We show how thinking of evolution and development as processes acting on processes enables a fully scale-free biology, i.e. a biology in which processes at every scale can be described using a single set of concepts and methods. We then consider the observer as a developmental system in §4, asking what is required for an observer to model itself as a persistent system, and hence to model its observations as a historical sequence.  We re-examine in §5 the distinction between stigmergic memories – particularly those encoded as permanent, public records, such as this paper - and implicit or procedural memories such as those stored by machine learning (ML) systems, cellular networks [26,27], or biochemical pathways [28,29]. Following Levin [30], we emphasize that memory is, first and foremost, an interpretative function, from which the idea of memory as a record, at some level of accuracy, of past events is derivative. We conclude in §6 that the distinction between objects and processes is always contrived, and always misleading, and that science would be better served by abandoning it entirely. We then look forward to how doing so enables a bioscience of agentive materials that integrates biomedicine with bioengineering, a bioscience that is difficult to conceptualize from the traditional viewpoint that considers organisms – with the possible exception of humans – as biological machines implementing programs that are largely, if not completely, fixed by their genomes.

[PAGE 2] 2. Systems, distinguishability, and persistence

The FEP is a mathematical framework for talking about persistence through time; indeed, the FEP can be stated, in obviously tautological form, as the claim that any system that persists through time must behave in a way that allows its persistence through time [14]. From a formal perspective, the FEP is an interpretation of dynamical systems that can be decomposed into components that maintain conditionally statistically independent states over some period of time. Note that the FEP characterizes a system only *while* it persists – maintains a state that is conditionally statistically independent from that of its environment – through time, and says nothing about how long any system will be successful in behaving in a way that allows its continued persistence. Understanding the FEP as a statement of principle requires understanding what "persistence", "time", and "behavior" are. The formal theory of the FEP is, effectively, an answer to these three questions. This formal theory has evolved significantly since it was first proposed; see [14,17,18] for current statements of the theory using classical statistical physics, [15] for a restatement in the language of quantum information theory, and [16] for a detailed comparison of the classical and quantum formulations. We will not replicate these formal presentations here, but will rather examine, in reverse order, how the theory addresses the fundamental questions of persistence, time, and behavior.


[PAGE 3] 2.1. Behavior and time in the FEP

First, what is behavior? The FEP follows physics in general in identifying behavior with physical interaction, and then follows statistical physics - equivalently, information theory, cybernetics, or even computer science - in identifying physical interaction with information exchange. The "space" in which this behavior takes place is the joint state space of the system of interest S and its environment E, i.e. the state space of U in Fig. 1. The roots of the identification of behavior with information exchange again go back to the 19th century, to Clausius' original definition of entropy, ΔS = ΔE/T, where S is entropy, E is energy, and T is temperature, and Boltzmann's interpretation of entropy in terms of uncertainty, S = *k*B lnΩ, where *k*B is Boltzmann's constant and Ω is the number of observationally indistinguishable states of the system of interest. Combining these two in the case of a binary system (Ω = 2) yields ΔE = ln2*k*BT, which is Landauer's Principle [31,32], now recognized as the fundamental connection between energy and information [33,34].

Physical interactions can be of two types: interactions within a system (internal or “self” interactions) and interactions between systems. In order to describe these interactions, we make a fundamental assumption: that "standard" or "textbook" quantum theory, with the Dirac-von Neumann axioms as given in, e.g. [35], is correct. With this assumption, the state spaces of physical systems can be represented as Hilbert spaces, which for convenience we take to be finite dimensional, and interactions between physical systems can be represented as (again finite dimensional) linear operators, specifically Hamiltonians or total energy operators, operating on Hilbert spaces. This assumption rules out, in particular, any nonlinear interactions between quantum systems, and hence any objective, observer-independent "collapse of the wavefunction." While such nonlinear interactions have never been observed, they have not yet been conclusively ruled out by experiments [36,37].

Given the assumption that standard quantum theory is correct, we can write the total energy of any isolated system U as an operator, the Hamiltonian HU = HS + HE + HSE, where HU, HS, and HE are the internal interactions of U, S, and E respectively and HSE is the interaction between S and E. Fig. 1 illustrates this equation. The distinction between interactions within or between systems is, therefore, dependent on, and relative to, a decomposition of whole systems such as U into components such as S and E. Decomposing U in a different way, e.g. as U = A⊕B with A ≠ S, would define different interactions: HU = HA + HB + HAB. This dependence of the distinction between self-interactions and other-interactions – interactions between systems – on the decomposition of U tells us that the distinction cannot be fundamental. Nothing about the physics of U favors any one decomposition – any one placement of an inter-component boundary – over any other.

[PAGE 3] Assuming standard quantum theory also provides a strict criterion for determining whether the components S and E specified by a decomposition of a system U have independently-specifiable, and hence conditionally statistically independent states, as is required if the FEP is to apply. The components S and E have independently-specifiable states if and only if their joint state factors, i.e. if and only if |SE) = |S)|E) in Dirac's notation. If the joint state does not factor, it is, by definition, entangled. Interactions between physical systems generically induce entanglement; an assumption that two systems are unentangled is, therefore, an assumption that they interact only weakly, and that even this weak interaction has not been observed for an asymptotically long time. Determining experimentally whether two systems are entangled is very difficult; the 2022 Nobel Prize in Physics was awarded to Aspect, Clauser, and Zeilinger for showing how this could be done. In practical settings, therefore, interacting systems that appear to be mutually distinguishable are assumed to be unentangled, i.e. assumed to have joint states that factor. The FEP applies only to systems that can each be assumed to have its own, independently-specifiable, conditionally statistically independent internal states. We will refer to systems as "distinct” or “distinguishable" if they can, in practice, be assumed to satisfy this condition.

[PAGE 4] The above discussion refers to the "system" U and its components, the distinct "systems" S and E. A system is naturally thought of as an object: a collection of "degrees of freedom" each of which can be in some state or other. As seen in Fig. 1, however, each system is also characterized by its self-interaction. This self-interaction is a process; in the quantum formalism, the propagator PU(t) = exp((-i/ħ)HU(t)), where ħ is the reduced Planck's constant, is the unitary operator that evolves the state of U forward in an assumed "background" [38] time t. Each degree of freedom x of U can, moreover, be replaced with a collection of binary degrees of freedom, one for each of the discernable values of x. We can, therefore, think of U simply as a collection of physically-implemented bits, or in the quantum case, qubits. We can, in other words, think of any state of U as an encoding of data, and think of the action of PU on any such state as a computation on those data; as shown in [39], doing so only requires finding a mapping – a semantic interpretation – from states of U to some data structure, and from finite samples of the action of PU to the action of some function on that data structure. Many such mappings are, moreover, always possible [40]. Provided that their joint state remains separable, the same goes for S and E.

If HU, HS, and HE can be interpreted as implementing computations, how are we to interpret the interaction HSE? Let us suppose that information in some measurable form is carried by discrete, detectable physical systems, such as individual photons, with each photon encoding one bit of information. The Divergence Theorem tells us that the number of photons, and hence the amount, in bits, of information emitted by any source within a system can be computed by computing the number of photons crossing any closed boundary around the system. The HP extends this result, postulating that the only information that can be obtained about a system by an external observer is the information that is carried across its boundary by physical systems capable of traversing the boundary. It states, moreover, that for any finite system, the maximal number of physical information carriers – again, e.g. photons – that can traverse its boundary is finite, and indeed proportional to the area of the boundary (formally, the maximum classical entropy S(B) of the boundary B is A/4, where A is the area of B in Planck units). The boundary B separating S from E can, therefore, also be thought of as a collection of discrete, physically-implemented bits, or in the quantum case, physically-implemented qubits (see [16] or [22] for mathematical details and [41] for a less formal discussion). The interaction HSE can, therefore, be regarded as finite bit exchange between S and E, i.e. as communication between S and E.

[PAGE 4] With this understanding of behavior, we are in a position to address time, the background time t employed above to construct the propagator PU(t). The FEP is about persistence through time, but where does "time" come from, and what role is it playing in the theory? To answer this question, it is useful to ask another: what happens to the information that flows, via the action of PU(t), into S from E? If HS is viewed as implementing a computation, the answer is clear: information from E is transferred across B, after which it serves as an input to whatever computation HS implements. In FEP language, information arriving across B is "sensation." Information flow from S back to E is output from HS - "action" in FEP language. Sending E a bit string involves writing these bits on B; this takes energy, at least ln2*k*BT per bit. It also takes time: formally, action is the product, ΔEΔt, of energy and time. Hence formally, time is built into the concept of interaction: it is what separates input from output. Any system S able to distinguish or record successive inputs from, or outputs to, another system E must implement, as one function of its internal interaction HS, a time counter or clock that "ticks" between each input and the following output, and thus provides an available timestamp for recording inputs or outputs [15]. The internal time ts counted by S is discrete, as it counts particular events, and is by definition coarse-grained with respect to the background time t. This internal time is strictly system-relative, as stressed by Di Biagio, Donà and Rovelli [42] among others.

Let us now describe S in the language of the FEP, considering S to both observe and manipulate E by reading from and writing on B; this alternation between reading and writing is implemented by HS and is, in FEP language, the process of *active inference*. Observation can be informative only if most of what is observed is not changing (or changing only slowly) in the internal time ts; if everything is observed to be changing, all that is "seen" is noise. Manipulation similarly makes sense only if there is something to manipulate, and an effectively fixed background against which to manipulate it. Formally, these are conditions of "sparse coupling" or "weak interaction" required by the FEP [16]. Processes in E that act on B, in other words, can be observed and manipulated only against a background of stasis. Time enables stasis: time provides a redundancy resource for – a "place to put" – the multiple "copies" of the observed background that enable it to be seen as unchanging, and hence allow other things to be seen as changing. Time is not just Nature's way of assuring that everything does not happen at once [43]; it is also Nature's way of allowing what happens to be noticed by observers.

[PAGE 4] Time has, therefore, two roles in the FEP: it is something that agents count by acting, and it is a resource made available to observers by their weak interactions with their environments. We can view these two roles at the level of fundamental physics, where weak interaction is required for distinctness from - absence of entanglement with - the environment as noted above. Hence weak interaction is required for HS to be locally well-defined, i.e. defined on S alone, which is in turn required for HS to implement an S-specific clock with which to count time relative to S. We can also view these roles at the level of problem solving, where distinctness from the environment is required for active inference, and active inference requires a distinction between input and output - sensation and action – that is provided by counting time. Active inference agents can manipulate their coupling to their environments by coarse-graining their inputs, or by employing selective attention – manipulating Bayesian precision – to ignore some inputs altogether. Change blindness [44], from this perspective, is not a cognitive deficit, but rather a way of maintaining the fixed background that renders attended-to changes salient. Maintaining a conversation in a crowded, noisy room, for example, would be impossible without the ability to *not* notice – be blind to – other conversations in the background. Manipulating coupling to the environment can be achieved by manipulating (agent-relative) time - processing fewer inputs - in order to minimize the energy expended to keep track of what is happening. Agents accomplish this by using specific reference frames to selectively observe and act on their environments.

[PAGE 5] 2.2. Examples: detecting changes in signals and objects

Let us first consider the detection of a signal in some medium. Analog radios that employ either amplitude modulation (AM) or frequency modulation (FM) provide a particularly clean example, and one with historical importance for theories of perception. Analog radio transmitters encode signals in the ambient photon field, the same medium that encodes visually-accessible information. We access these signals with appropriate detectors, i.e. analog radios.

Suppose now that you are listening to a song on the radio. How does the joint system comprising you and your radio extract the information of interest – the song - from the ambient photon field? Extracting the song is a two-step process that mirrors the two-step process of encoding it. Your radio is a tunable resonator that detects the "carrier wave" (CW) of the transmitter. This CW is a constant signal at some designated frequency. It encodes, effectively, the name of the transmitter. Once the radio has found this signal, it amplifies small, very slow (tens of Hz to kHz as opposed to GHz) AM or FM perturbations of the CW, using filters and feedback (i.e. measures of the differences between current and expected states) to reduce noise. It is these small, slow perturbations that encode the song.

The ability of radios to extract songs from the ambient photon field is of interest historically because Gibson [45] used it as a model of visual perception, and used it in particular to argue that perception by "resonance" required no memory for prior information on the part of the perceiver. This argument has since been used extensively to argue that organisms neither store nor process information [46,47]. The brief description above highlights the stored information that this argument misses: radios store information about CWs - that, for example, rocks of similar size and weight do not - and also store extensive information about how CWs are modulated by radio transmitters. This latter information allows them to process perturbations of the CW of the right kind, and at the right timescale, to extract the encoded information, e.g. a song.

A human using a radio to find songs is faced with the same problem facing the radio, and solves this problem is a similar way. In order to use the radio to find songs, a human must first find the radio. This is usually done by interacting with the ambient photon field, i.e. by visual perception. The human must scan the available components of this field to locate an expected, constant signal, what we ordinarily call the "visual appearance" of the radio. It is only after doing this that the human can detect or manipulate time-varying properties of the radio, such as the station (i.e. CW) to which it is tuned. Fig. 2 illustrates this difference between constant (or "reference") and variable (or "pointer") properties. The specificity of the reference state |R) (using Dirac's notation) used to identify a system – a radio, or even a CW in the radio-frequency spectrum - determines the specificity with which a particular system can be detected. If one wants a specific radio station, or a specific radio, one needs to detect an |R) specific to, and known to be specific to, that individual system and no other.

[PAGE 5] The above characterization of detecting changes in signals and objects is completely general, and in fact characterizes any physical system capable of responding in some specific way to some specific property of its environment [5]. In the language of physics, detectors of specific kinds of signals or systems are quantum (because physically implemented) reference frames (QRFs [49,50]). A rhodopsin molecule responsive to photons in a specific frequency range, for example, is, in virtue of its physical implementation, a QRF for photons in that frequency range.  A tick responsive to butyric acid is a QRF for butyric acid.  A human responsive to the visual appearance of a radio is a QRF for radios.  All of these QRFs, moreover, implement memory: they "remember" what they are QRFs *for*.  This memory is implemented physically, by the structure and dynamics of the QRF.  It is this physically-implemented memory that allows QRFs to detect specific signals or systems in their environments.

[PAGE 5]
Fig. 4. Mutual coherency condition for MCA models of a system S. Provided that both M₁ and M₂ are coherent models of S [39], and that the map I between
models remains constant over time, the diagram commutes and mutual coherence between the models M₁ and M₂ is guaranteed.

Systems and their internal dynamics defined within standard quantum theory do not impose any mereological descriptions a priori; all such descriptions are model-dependent, and hence dependent on how the system and its behavior are measured [62,63]. No particular decomposition of any system into "parts" can be assumed to be given, a priori, by the "physics" of the system. Hence no description of a system as a Multiscale Competency Architecture (MCA) can be assumed to be given, or implied, by its physical dynamics.

We can, therefore, describe any MCA in multiple ways, and ask how these descriptions relate [40]. Let M₁ and M₂ be descriptions of S as an MCA. The descriptions M₁ and M₂ can be stated in any language(s), formal or informal, and can divide S into "parts" and characterize the states of the parts at any scale and in any way. To "make sense" or be coherent as models of the behavior of S, each of M₁ and M₂ must, individually, satisfy the simple criterion given by [39]: the map from states |S(t)) to their descriptions in the model must commute with both the physical dynamics Hs and the postulated model dynamics. Describing some state |S₁) in the model and then running, or inferring, the model dynamics on the model's description of |S₁) for a simulated time dt must, in other words, yield the same result as describing the state |S(t+dt)) in the model. How precise the criterion of “sameness” is depends on the scale, precision, and typically, level of formality of the model. If the model is stated in a programming language, the notion of "the same result" means: as computed using an implementation of that language. If the model is stated in ordinary English, e.g. with diagrams such as typically used in biochemistry, what “the same result" means may be considerably more vague.

Assuming that M₁ and M₂ are each individually coherent models of the behavior of S, we can ask what is required for them to be mutually coherent, or inter-translatable. Suppose that the descriptions M₁(t) and M₂(t) of |S(t)) can be related by some translation T(t). If M₁ and M₂ are formal models, this T will be a mapping from the symbols used by M₁ to the symbols used by M₂. If M₁ and M₂ are informal models, T may just be an explanation, in some natural language, of how the models relate; nearly all models of genetic interactions, cellular biochemistry, or organismal physiology or behavior are of this informal kind. We can now ask: does the same translation between the descriptions of S given by M₁ and M₂ work at subsequent times, e.g. for the descriptions M₁(t + dt) and M₂(t + dt) of |S(t + dt)), as illustrated in Fig. 4? If so, the translation T “works” or “makes sense,” at least for the limited collection of descriptions generated by observations of S between t and t + dt. If T works for descriptions of large numbers of systems, over long observation times, that are generated in the languages of M₁ and M₂, we can say, at least tentatively, that T is a coherent translation between these two languages.

The assumption of linearity that allows us to write Eq. (1) provides an alternative approach to checking whether two models of a system S are mutually coherent. Suppose that M₁ describes S in terms of a mereological decomposition into subsystems Si, while M₂ describes S in terms of a mereological decomposition into subsystems Sk. If the two models can each fully reconstruct a representation of Hs, i.e. if each model can fully describe both the self-interactions, Hsi and Hsk respectively, and the pairwise interactions, Hsij and Hskl respectively, of the subsystems that they postulate, then the translation T : M₁ → M₂ between the two models is just a change in decomposition, a redrawing of component boundaries within S that, by satisfying Eq. (1), leaves the dynamics Hs and hence the state transition |S(t)) → |S(t + dt)) unchanged. The map T (t) → T (t + dt) is, in this case, the Identity, which guarantees that the diagram shown in Fig. 4 commutes, i.e. that the models M₁ and M₂ are mutually coherent as MCA models of S.

It is important to emphasize that the coherence of models of physical systems is an empirical question, and that whether any model of any system S is coherent, or accurate, in the above sense cannot be proven a priori. The Conway-Kochen theorem [96], which rules out local determinism for any physical system if quantum theory is true, guarantees that even models that are highly accurate for ensembles cannot precisely predict outcomes of experiments on individual systems. In biological systems, where even clonal populations cannot be expected to be uniform in all characteristics potentially relevant to their dynamics or interactions, determining the accuracy of the decompositions of Hs proposed by competing models is correspondingly more difficult.

If two models M₁ and M₂ are decompositions of S into active inference agents, and of Hs into interactions between such agents, at two different scales - that is, if M₁ and M₂ are both MCA models - then T describes how the active inference agents

[PAGE 6]
at one scale relate to the active inference agents at the other scale. Fig. 5 illustrates this for the case in which M₁ describes S as a single active inference agent, while M₂ describes S as an MCA comprising multiple active inference agents. In this case, the translation T describes how the single active inference agent of M₁ implements its generative model using the multiple active inference agents of M₂.

[PAGE 7]
Fig. 5. Multiscale competency architectures (MCAs) as hierarchies of active inference agents. a) Any system S can be described as a single active inference agent interacting with its environment E. b) The same system S can be described as an MCA comprising multiple active inference agents S₁, each interacting with its own environment E₁. The translation T between these two descriptions specifies how the single active inference agent of panel a) implements its generative model using the multiple active inference agents of panel b).

3.3. Development as multiscale model selection
As discussed above, any system S can be described as an MCA in multiple ways. Which of these descriptions is most useful depends on what one wants to do with it. If one wants to predict the behavior of S in some particular situation, the most useful description will be the one that most accurately predicts S’s behavior in that situation. If one wants to control the behavior of S, the most useful description will be the one that most effectively enables such control.  Different MCA models of a system S will, in general, make different predictions about S’s behavior. The FEP, however, provides a principled way to select among competing models: the model that minimizes VFE is the most accurate model, and hence the model that should be used to guide action. This principle of model selection applies at all scales, from the selection of individual actions to the selection of developmental trajectories to the selection of evolutionary strategies.

[PAGE 8]
As discussed above, the FEP describes agents as minimizing a variational free energy (VFE) that is, effectively, a measure of prediction error. They do this by active inference, a combination of learning environmental regularities and acting on the environment in order to obtain both thermodynamic resources and new information. The latter activity is essential for any systems not living in an unrealistically ideal environment [64] and its motivation - whether termed epistemic hunger, infotaxis, or curiosity - is a fundamental drive [65].

Whether for food or knowledge, foraging involves risk; indeed [14] shows how risk and (predicted) information gain are just two ways of describing the same statistical measure. Hence active inference is intrinsically risky, and systems that engage in it can suffer irreversible damage or destruction, i.e. boundary collapse [66].

As discussed above, to persist as an entity is to maintain statistical separability from one's environment by maintaining a collection of internal states that are causally "far from one's boundary" and hence available for implementing computations under one's own control. How many such states there are depends on the size of one's boundary – effectively, on one's surface to volume ratio. The size of the boundary, in turn, depends via the HP on the strength of the system-environment interaction. Hence persistent systems must maintain small boundaries to avoid being penetrated or incinerated by their environments. Boundary size, however, also determines both the amount of information and the amount of thermodynamic resources that can be obtained from the environment [67,68]. Both are needed, and they trade off against each other when boundary size is fixed. Hence all active inference agents face coupled tradeoffs related to boundary size or, equivalently, total strength of interaction with their environments [41]. For any active inference agent, different choices of environmental exposure and different tradeoffs between feeding, learning, and exploration can be expected to yield different lifestyles, and hence pre-adaption to different niches.

[PAGE 9]
What evolves, then, in an MCA? The answer is: the self-interactions Hi and the pairwise interactions Hij of the components at every scale. These interactions are what determine the dynamics of the system as a whole, and hence what determine how the system responds to perturbations, whether internal or external. The interactions Hi and Hij are, moreover, what determine the boundary of the system, and hence what determine what counts as "inside" and "outside" the system. The boundary of a system is, as discussed above, not a fixed, observer-independent entity, but rather depends on how the system is described. The FEP provides a principled way of describing systems, and hence of defining their boundaries: the boundary of a system is the set of states that mediate its interactions with its environment. The interactions Hi and Hij are what determine these mediating states, and hence what determine the boundary of the system.

In the context of development, model selection can be viewed as a process of exploring the space of possible MCA models of an organism. As an organism develops, it interacts with its environment and gathers information about the consequences of its actions. This information is used to update the organism’s generative model, which in turn is used to select the MCA model that best predicts the organism’s future interactions with its environment. This process of model selection can be viewed as a form of Bayesian inference, in which the organism’s prior beliefs about its environment are updated based on its experiences.  The process of model selection during development is not simply a matter of choosing the MCA model that best fits the organism’s current environment. It is also a matter of choosing the MCA model that best prepares the organism for future environmental changes. This requires the organism to anticipate the kinds of changes that are likely to occur in its environment and to choose the MCA model that is most robust to these changes.

[PAGE 9] 
Fig. 4. Mutual coherency condition for MCA models of a system S. Provided that both M₁ and M₂ are coherent models of S [39], and that the map I between
models remains constant over time, the diagram commutes and mutual coherence between the models M₁ and M₂ is guaranteed.

Systems and their internal dynamics defined within standard quantum theory do not impose any mereological descriptions a priori; all such descriptions are model-dependent, and hence dependent on how the system and its behavior are measured [62,63]. No particular decomposition of any system into "parts" can be assumed to be given, a priori, by the "physics" of the system.